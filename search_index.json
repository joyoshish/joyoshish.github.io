[{"title":"Probability \u0026 Statistics for Data Science - Bayesian Thinking, MLE, MAP \u0026 Inference","url":"/blog/2022/mathforml-probstat3/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, probability-statistics, math","date":"June 10, 2022","content":"The Gulf Stream – Winslow Homer (1899)In data science, where uncertainty is not an exception but the norm, reasoning under uncertainty becomes a core necessity. While traditional frequentist approaches have long provided a framework for estimating parameters and testing hypotheses, the Bayesian paradigm brings an alternative—and in many ways, more intuitive—framework to model beliefs, incorporate prior knowledge, and update our understanding as new data arrives.Bayesian inference treats unknown parameters as random variables and uses probability distributions to express uncertainty. This philosophical shift opens the door to a rich array of techniques and tools that power everything from spam filters to hyperparameter tuning in deep learning.Bayes’ Theorem and Conditional ProbabilityBayes’ Theorem is a foundational result in probability theory and statistics that relates conditional probabilities. Given two events \\(A\\) and \\(B\\) with \\(P(B) \u0026gt; 0\\), Bayes’ Theorem states:\\[P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\\]In this formulation: \\(P(A)\\) is the prior probability of \\(A\\), reflecting our initial belief before observing \\(B\\). \\(P(B \\mid A)\\) is the likelihood of observing \\(B\\) given \\(A\\). \\(P(B)\\) is the marginal probability of \\(B\\), integrated over all possibilities. \\(P(A \\mid B)\\) is the posterior probability: our updated belief about \\(A\\) after observing \\(B\\).This theorem is derived directly from the definition of conditional probability:\\[P(A \\cap B) = P(A \\mid B) P(B) = P(B \\mid A) P(A)\\]Rearranging gives:\\[P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\\]In Bayesian statistics, this result is used to update beliefs about unknown parameters in light of new data. For continuous parameters, the theorem generalizes to:\\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\\]Where: \\(\\theta\\) is a parameter. \\(D\\) is observed data. \\(P(\\theta)\\) is the prior distribution. \\(P(D \\mid \\theta)\\) is the likelihood. \\(P(\\theta \\mid D)\\) is the posterior distribution. \\(P(D) = \\int P(D \\mid \\theta) P(\\theta) d\\theta\\) is the evidence or marginal likelihood.Example: Diagnostic TestingSuppose a rare disease affects 1% of the population. A diagnostic test has: Sensitivity (true positive rate): 99% Specificity (true negative rate): 95%We want to compute the probability that a person has the disease given a positive test result.Let \\(D\\) denote having the disease, and \\(T\\) denote a positive test. Then:\\[P(D \\mid T) = \\frac{P(T \\mid D) P(D)}{P(T)} = \\frac{0.99 \\cdot 0.01}{0.99 \\cdot 0.01 + 0.05 \\cdot 0.99} \\approx 0.167\\]So despite a highly accurate test, the probability of truly having the disease given a positive test result is only about 16.7%. This demonstrates the importance of the prior (base rate) in interpreting diagnostic results.Python Code for the Example# Bayesian disease diagnosisP_disease = 0.01P_pos_given_disease = 0.99P_pos_given_no_disease = 0.05P_no_disease = 1 - P_diseaseP_pos = P_pos_given_disease * P_disease + P_pos_given_no_disease * P_no_diseaseP_disease_given_pos = (P_pos_given_disease * P_disease) / P_posprint(f\"Posterior probability: {P_disease_given_pos:.4f}\")VisualizationThe following visualization generalizes the example by showing how the posterior probability changes as the prior (disease prevalence) varies.import numpy as npimport matplotlib.pyplot as pltprior_probs = np.linspace(0.001, 0.1, 100)sensitivity = 0.99specificity = 0.95false_positive = 1 - specificityposterior_probs = (sensitivity * prior_probs) / ( sensitivity * prior_probs + false_positive * (1 - prior_probs))plt.plot(prior_probs, posterior_probs, label=\"P(Disease | Positive Test)\")plt.xlabel(\"Prior Probability of Disease\")plt.ylabel(\"Posterior Probability\")plt.title(\"Bayesian Update: Disease Diagnosis\")plt.grid(True)plt.legend()plt.show() Figure: Posterior Probability vs Prior for Varying Specificity This plot illustrates how the posterior probability of having a disease—after receiving a positive test result—varies based on two factors: The prior probability of the disease (x-axis), which corresponds to its prevalence in the population, And the specificity of the diagnostic test, or how well the test avoids false positives (multiple curves).All curves assume a fixed sensitivity of 99% (i.e., the test correctly identifies almost all diseased cases). When the disease is rare (e.g., prior probability below 1%), even a highly accurate test may produce a low posterior. This is because false positives dominate the marginal probability of a positive result at low prevalence. As the prior probability increases (e.g., testing a high-risk group), the posterior probability increases sharply. This reflects how more confidence in the disease’s presence in the population strengthens the update from a positive result. The higher the test specificity, the steeper the curve—and the stronger the belief update. With 99% specificity, a positive test leads to much higher posterior probabilities compared to 90% specificity, especially when the prior is low. Applications in Data ScienceBayes’ Theorem is a pillar in many areas of applied machine learning and data science: Naive Bayes Classifiers: Used in email spam detection, text classification, and sentiment analysis. Medical Diagnostic Systems: Estimate disease probabilities as symptoms and test results accumulate. Bayesian A/B Testing: Provides posterior distributions over conversion rates instead of binary conclusions. Credit Scoring and Fraud Detection: Updates risk estimates in real-time based on user behavior.It provides a mathematically sound, interpretable, and adaptive approach to reasoning and decision-making under uncertainty.Prior, Likelihood, and PosteriorIn the Bayesian framework, statistical inference is built on the principle of updating beliefs about parameters based on observed data. This belief is represented mathematically using probability distributions and updated using Bayes’ Theorem. The process relies on three central components: The prior: our belief about the parameter before seeing data, The likelihood: the probability of observing the data given a specific value of the parameter, The posterior: the revised belief after combining the prior and the likelihood.Let us denote the unknown parameter by \\(\\theta\\), and the observed data by \\(D = \\{x_1, x_2, \\dots, x_n\\}\\). Then:\\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{P(D)}\\]This is Bayes’ Theorem applied to parameter estimation, where: \\(P(\\theta)\\) is the prior distribution, \\(P(D \\mid \\theta)\\) is the likelihood, \\(P(D)\\) is the marginal likelihood or evidence, \\(P(\\theta \\mid D)\\) is the posterior.Prior DistributionThe prior distribution \\(P(\\theta)\\) expresses our belief or uncertainty about the parameter \\(\\theta\\) before observing any data. Mathematically, the prior is a probability density function (pdf) over the domain of \\(\\theta\\):\\[\\int P(\\theta) \\, d\\theta = 1\\]This prior may be: Informative: When strong domain knowledge is available. Uninformative or weakly informative: To allow data to dominate inference. Subjective: Based on expert intuition or empirical insight.The specific form of the prior depends on the nature of the parameter:▸ Beta Prior (for probabilities \\(\\theta \\in [0, 1]\\)):\\[P(\\theta) = \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\\]Where \\(\\alpha, \\beta \u0026gt; 0\\) are shape parameters, and \\(B(\\alpha, \\beta)\\) is the Beta function:\\[B(\\alpha, \\beta) = \\int_0^1 \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1} \\, d\\theta\\]This distribution is commonly used as a prior for binary outcomes and proportions. Its mean and variance are:\\[\\mathbb{E}[\\theta] = \\frac{\\alpha}{\\alpha + \\beta}, \\quad\\text{Var}(\\theta) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\]▸ Gaussian Prior (for real-valued \\(\\theta\\)):\\[P(\\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp\\left( -\\frac{(\\theta - \\mu)^2}{2\\sigma^2} \\right)\\]Used in models like Bayesian linear regression or Gaussian processes, this prior expresses belief that \\(\\theta\\) is centered around \\(\\mu\\) with spread controlled by variance \\(\\sigma^2\\).▸ Uniform Prior (non-informative):If nothing is known about \\(\\theta\\) within an interval \\([a, b]\\):\\[P(\\theta) = \\frac{1}{b - a}, \\quad \\text{for } \\theta \\in [a, b]\\]This flat prior assumes all values in \\([a, b]\\) are equally likely.Likelihood FunctionThe likelihood function \\(P(D \\mid \\theta)\\) represents how plausible the observed data is for different values of the parameter \\(\\theta\\). While the prior is independent of the data and reflects belief, the likelihood is derived from a data-generating model—a statistical assumption about how the data arises conditional on \\(\\theta\\).Formally, if the data consists of independent and identically distributed (i.i.d.) samples:\\[D = \\{x_1, x_2, \\dots, x_n\\}, \\quad x_i \\sim P(x \\mid \\theta)\\]Then the likelihood becomes:\\[P(D \\mid \\theta) = \\prod_{i=1}^n P(x_i \\mid \\theta)\\]This is not a probability distribution over \\(\\theta\\), but a function of \\(\\theta\\) with data held fixed.▸ Example: Bernoulli/Binomial LikelihoodSuppose each observation is a binary outcome (success/failure), modeled as a Bernoulli trial:\\[x_i \\sim \\text{Bernoulli}(\\theta), \\quad \\text{so } P(x_i \\mid \\theta) = \\theta^{x_i} (1 - \\theta)^{1 - x_i}\\]If we observe \\(k\\) successes in \\(n\\) trials, then:\\[P(D \\mid \\theta) = \\prod_{i=1}^n \\theta^{x_i} (1 - \\theta)^{1 - x_i}= \\theta^k (1 - \\theta)^{n - k}\\]This expression is the likelihood function, which evaluates how consistent various \\(\\theta\\) values are with the observed success/failure count.Posterior DistributionThe posterior distribution \\(P(\\theta \\mid D)\\) combines the prior and likelihood using Bayes’ Theorem:\\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{P(D)}\\]Where the denominator is the marginal likelihood or evidence, ensuring that the posterior integrates to 1:\\[P(D) = \\int P(D \\mid \\theta) \\cdot P(\\theta) \\, d\\theta\\]▸ Beta Prior + Binomial LikelihoodLet’s assume: Prior: \\(P(\\theta) = \\text{Beta}(\\alpha, \\beta)\\) Likelihood: \\(P(D \\mid \\theta) = \\theta^k (1 - \\theta)^{n - k}\\)Then:Unnormalized posterior:\\[P(\\theta \\mid D) \\propto P(D \\mid \\theta) \\cdot P(\\theta) \\\\\\propto \\theta^k (1 - \\theta)^{n - k} \\cdot \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1} \\\\= \\theta^{k + \\alpha - 1}(1 - \\theta)^{n - k + \\beta - 1}\\]This is the kernel of a Beta distribution:\\[P(\\theta \\mid D) = \\text{Beta}(\\alpha + k, \\beta + n - k)\\]This result highlights the convenience of conjugate priors: the prior and posterior belong to the same family, simplifying inference.▸ Posterior Summary Statistics - Beta DistributionBased on the posterior:\\[\\theta \\mid D \\sim \\text{Beta}(\\alpha + k, \\beta + n - k)\\]where: \\(\\alpha\\) and \\(\\beta\\) are prior parameters, \\(k\\) is the number of observed successes, \\(n\\) is the total number of observations.We derive:1. Posterior MeanIf:\\[\\theta \\sim \\text{Beta}(a, b)\\]then the mean is given by:\\[\\mathbb{E}[\\theta] = \\frac{a}{a + b}\\]In our case, the posterior parameters are: \\[a = \\alpha + k\\] \\[b = \\beta + n - k\\] So the posterior mean is:\\[\\mathbb{E}[\\theta \\mid D] = \\frac{\\alpha + k}{\\alpha + \\beta + n}\\]This is a convex combination of the prior mean and the observed frequency: Prior mean: \\(\\frac{\\alpha}{\\alpha + \\beta}\\) Observed frequency: \\(\\frac{k}{n}\\)As \\(n\\) increases, the posterior mean converges toward the sample mean \\(k/n\\), and the influence of the prior diminishes.2. Posterior VarianceThe variance of a Beta distribution \\(\\text{Beta}(a, b)\\) is:\\[\\text{Var}[\\theta] = \\frac{ab}{(a + b)^2 (a + b + 1)}\\]Apply this to the posterior: \\[a = \\alpha + k\\] \\[b = \\beta + n - k\\] Then:\\[\\text{Var}[\\theta \\mid D] = \\frac{(\\alpha + k)(\\beta + n - k)}{(\\alpha + \\beta + n)^2 (\\alpha + \\beta + n + 1)}\\]This variance shrinks as \\(n\\) increases — reflecting increased confidence in our estimate of \\(\\theta\\) after observing more data.3. MAP Estimate (Posterior Mode)The mode (maximum a posteriori estimate) of a Beta distribution \\(\\text{Beta}(a, b)\\) is given by:\\[\\theta_{\\text{MAP}} = \\frac{a - 1}{a + b - 2}, \\quad \\text{for } a \u0026gt; 1 \\text{ and } b \u0026gt; 1\\]Apply to the posterior: \\[a = \\alpha + k\\] \\[b = \\beta + n - k\\] So:\\[\\hat{\\theta}_{\\text{MAP}} = \\frac{\\alpha + k - 1}{\\alpha + \\beta + n - 2}\\]This estimate corresponds to the mode of the posterior distribution, and will differ from the mean unless the distribution is symmetric.These quantities are critical in: Computing expected outcomes and uncertainty, Constructing Bayesian credible intervals, Making point predictions (e.g., MAP for classification), Visualizing posterior summaries.▸ Posterior Derivation: Gaussian Likelihood and Gaussian PriorIn many data science tasks, we assume that the observed data is generated from a continuous process with Gaussian noise, and that our prior belief about the parameter is also normally distributed. This leads to one of the most well-known conjugate pairs: Gaussian-Gaussian inference (both the likelihood and prior are Gaussian — one of the most important and elegant conjugate models in Bayesian inference).Let us assume: The parameter of interest is a real-valued scalar \\(\\theta\\). The observed data \\(D = \\{x_1, x_2, \\dots, x_n\\}\\) are i.i.d. samples from:\\[x_i \\mid \\theta \\sim \\mathcal{N}(\\theta, \\sigma^2)\\] where \\(\\sigma^2\\) is known (observation noise variance). The prior belief about \\(\\theta\\) is:\\[\\theta \\sim \\mathcal{N}(\\mu_0, \\tau^2)\\] where \\(\\mu_0\\) is the prior mean, and \\(\\tau^2\\) is the prior variance. Step 1: Likelihood FunctionGiven \\(n\\) i.i.d. observations \\(x_1, ..., x_n\\), the likelihood of the data given \\(\\theta\\) is:\\[P(D \\mid \\theta) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i - \\theta)^2}{2\\sigma^2} \\right)\\]This is the product of Gaussians with the same mean \\(\\theta\\) and fixed variance \\(\\sigma^2\\).Taking the log-likelihood:\\[\\log P(D \\mid \\theta) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\theta)^2\\]Let \\(\\bar{x} = \\frac{1}{n} \\sum x_i\\). Then:\\[\\sum (x_i - \\theta)^2 = \\sum (x_i - \\bar{x} + \\bar{x} - \\theta)^2 = \\sum (x_i - \\bar{x})^2 + n(\\theta - \\bar{x})^2\\]So the likelihood becomes:\\[P(D \\mid \\theta) \\propto \\exp\\left( -\\frac{n}{2\\sigma^2} (\\theta - \\bar{x})^2 \\right)\\]Which shows that the likelihood (up to normalization) is itself Gaussian:\\[\\theta \\mid D \\propto \\mathcal{N}(\\bar{x}, \\sigma^2 / n)\\]Step 2: PriorThe prior is:\\[P(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau^2}} \\exp\\left( -\\frac{(\\theta - \\mu_0)^2}{2\\tau^2} \\right)\\]Step 3: Posterior DerivationBayes’ Theorem gives:\\[P(\\theta \\mid D) \\propto P(D \\mid \\theta) \\cdot P(\\theta)\\]Since both terms are exponentials of quadratics in \\(\\theta\\), their product is proportional to another Gaussian:Let’s expand both exponentials:\\[\\log P(\\theta \\mid D) \\propto -\\frac{n}{2\\sigma^2} (\\theta - \\bar{x})^2 - \\frac{1}{2\\tau^2} (\\theta - \\mu_0)^2\\]Combine terms:\\[\\log P(\\theta \\mid D) \\propto -\\frac{1}{2} \\left[ \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2} \\right) \\theta^2 - 2 \\left( \\frac{n\\bar{x}}{\\sigma^2} + \\frac{\\mu_0}{\\tau^2} \\right) \\theta \\right]\\]This is the kernel of a Gaussian distribution with:▸ Posterior Mean:\\[\\mu_n = \\frac{\\frac{n}{\\sigma^2} \\bar{x} + \\frac{1}{\\tau^2} \\mu_0}{\\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}}\\]▸ Posterior Variance:\\[\\sigma_n^2 = \\left( \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2} \\right)^{-1}\\]Interpretation The posterior mean \\(\\mu_n\\) is a weighted average of the prior mean and sample mean, where the weights are proportional to their respective precisions (inverse variances). The posterior variance \\(\\sigma_n^2\\) is always smaller than either the prior or the sample variance alone, reflecting increased certainty after combining information.This Gaussian-Gaussian model forms the mathematical foundation of many applications: Bayesian Linear Regression: Each weight in a regression model has a Gaussian prior and is updated analytically with Gaussian likelihoods. This allows regularization and closed-form uncertainty quantification. Bayesian Updating for Streaming Data: In online learning, new data incrementally shifts the posterior, making it a new prior — enabling scalable, memory-efficient learning. Sensor Fusion: In robotics and control systems, Bayesian Gaussian updates allow combining noisy measurements from multiple sensors to produce more confident estimates. Kalman Filters: A specialized form of recursive Bayesian estimation based on Gaussian distributions used in tracking and forecasting.Stepping back from this specific case, the same pattern—prior, likelihood, and posterior—runs through the entire Bayesian approach. The prior captures what we know (or assume) before seeing any data. The likelihood tells us how compatible the data is with different parameter values. The posterior combines both to give us a data-informed belief about the parameter.In data science, this allows: Incorporating prior knowledge (from previous experiments, expert belief, or regulatory constraints), Updating beliefs incrementally as more data becomes available, Quantifying uncertainty through full distributions instead of point estimates.This foundation underlies a wide range of Bayesian models — from Naive Bayes classifiers and probabilistic graphical models to Gaussian Processes and Bayesian neural networks.Example: Inferring Coin BiasLet us assume we want to estimate the bias \\(\\theta\\) (probability of heads) of a coin. Suppose we observe 10 tosses and get 6 heads and 4 tails. This is a Binomial process. Prior: Assume \\(\\theta \\sim \\text{Beta}(2, 2)\\) — a weakly informative prior reflecting fairness. Likelihood: For 6 heads out of 10 tosses,\\[P(D \\mid \\theta) = \\binom{10}{6} \\theta^6 (1 - \\theta)^4\\] Posterior: Since the Beta prior is conjugate to the Binomial likelihood, the posterior is:\\[\\theta \\mid D \\sim \\text{Beta}(2 + 6, 2 + 4) = \\text{Beta}(8, 6)\\] This posterior reflects our updated belief about the coin’s bias after observing the data.Visualizationimport numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import beta# Grid of theta valuestheta = np.linspace(0, 1, 200)# Prior: Beta(2, 2)prior = beta.pdf(theta, 2, 2)# Likelihood (up to proportionality): theta^6 * (1 - theta)^4likelihood = theta**6 * (1 - theta)**4likelihood /= np.trapz(likelihood, theta) # Normalize for plotting# Posterior: Beta(8, 6)posterior = beta.pdf(theta, 8, 6)# Plottingplt.figure(figsize=(8, 5))plt.plot(theta, prior, label=\"Prior: Beta(2, 2)\", linestyle=\"--\")plt.plot(theta, likelihood, label=\"Likelihood (scaled)\", linestyle=\":\")plt.plot(theta, posterior, label=\"Posterior: Beta(8, 6)\", linewidth=2)plt.title(\"Bayesian Update: Estimating Coin Bias\")plt.xlabel(\"θ (Probability of Heads)\")plt.ylabel(\"Density\")plt.legend()plt.grid(True)plt.tight_layout()plt.show() Figure: Bayesian Update: Estimating Coin Bias This visualization shows how the prior belief and the observed data interact to form a posterior that reflects both — centered slightly above 0.5 due to the 6 observed heads.Applications in Data ScienceThis prior-likelihood-posterior triad is a universal framework used across many data science workflows:Bayesian A/B Testing Prior: Encodes historical conversion rates for variants. Likelihood: Comes from observed clicks or conversions (e.g., Binomial model). Posterior: Used to make probabilistic comparisons like \\(P(\\theta_A \u0026gt; \\theta_B \\mid \\text{data})\\).This leads to more robust and interpretable decisions compared to traditional p-values.Bayesian Regression Prior: Places distributions (e.g., Normal) over regression coefficients. Likelihood: Based on residuals from training data. Posterior: Yields not just point estimates, but full predictive intervals — crucial for risk-aware applications like pricing, forecasting, or credit scoring.Fraud Detection Prior: Reflects expected fraud rate (e.g., from industry benchmarks). Likelihood: Comes from behavioral or transactional data. Posterior: Quantifies the probability of fraud for new transactions in real time.Recommender Systems Prior: Reflects assumed user preferences or item popularity. Likelihood: Derived from user-item interaction data (ratings, clicks). Posterior: Enables personalized predictions with uncertainty quantification, improving exploration in recommendation.The combination of prior beliefs and observed evidence, culminating in a posterior, provides a powerful and flexible inference engine. This Bayesian updating mechanism equips data scientists to not only make predictions but also understand their confidence in those predictions — a critical capability in domains where decisions have consequences.Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori Estimation (MAP)One of the central challenges in statistical inference is the estimation of model parameters from observed data. Two important frameworks for parameter estimation are Maximum Likelihood Estimation (MLE) and Maximum A Posteriori Estimation (MAP). While both aim to select parameter values that explain the data well, they differ in how they incorporate prior knowledge.MLE derives purely from the likelihood function, whereas MAP is based on the full Bayesian posterior distribution. Their distinction becomes particularly meaningful in the presence of prior information, limited data, or regularization constraints.MLE: Derivation and ExplanationLet \\(D = \\{x_1, x_2, \\dots, x_n\\}\\) be a set of i.i.d. observations drawn from a distribution parameterized by \\(\\theta\\).The likelihood function is:\\[L(\\theta \\mid D) = \\prod_{i=1}^{n} P(x_i \\mid \\theta)\\]Taking logs, the log-likelihood becomes:\\[\\ell(\\theta) = \\log L(\\theta \\mid D) = \\sum_{i=1}^{n} \\log P(x_i \\mid \\theta)\\]The Maximum Likelihood Estimator is the value of \\(\\theta\\) that maximizes the log-likelihood:\\[\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\, \\ell(\\theta)\\]MLE Derivation: Bernoulli CaseSuppose each \\(x_i\\) is a Bernoulli trial with success probability \\(\\theta\\). Then:\\[P(x_i \\mid \\theta) = \\theta^{x_i} (1 - \\theta)^{1 - x_i}\\]So the log-likelihood becomes:\\[\\ell(\\theta) = \\sum_{i=1}^n \\left[ x_i \\log \\theta + (1 - x_i) \\log(1 - \\theta) \\right]\\]Let: \\(k = \\sum_{i=1}^n x_i\\) be the number of successes, \\(n - k\\) be the number of failures.Then:\\[\\ell(\\theta) = k \\log \\theta + (n - k) \\log(1 - \\theta)\\]To maximize, differentiate with respect to \\(\\theta\\) and set the derivative to zero:\\[\\frac{d\\ell}{d\\theta} = \\frac{k}{\\theta} - \\frac{n - k}{1 - \\theta} = 0\\]Solving:\\[\\frac{k}{\\theta} = \\frac{n - k}{1 - \\theta} \\Rightarrow k (1 - \\theta) = (n - k) \\theta\\]Expanding:\\[k - k\\theta = n\\theta - k\\theta \\Rightarrow k = n\\theta \\Rightarrow \\hat{\\theta}_{\\text{MLE}} = \\frac{k}{n}\\]MAP: Derivation and ExplanationIn the Bayesian framework, we update our belief about \\(\\theta\\) after observing \\(D\\) using Bayes’ Theorem:\\[P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) P(\\theta)}{P(D)}\\]The MAP estimator is:\\[\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\, P(\\theta \\mid D)= \\arg\\max_\\theta \\, P(D \\mid \\theta) P(\\theta)\\]Taking logarithms:\\[\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta \\left[ \\log P(D \\mid \\theta) + \\log P(\\theta) \\right]\\]This formulation shows that MAP estimation is equivalent to MLE with a regularization term derived from the prior. If \\(P(\\theta)\\) is uniform (uninformative), then MAP reduces to MLE. If \\(P(\\theta)\\) is Gaussian, the log-prior is quadratic and acts like L2 regularization.MAP Derivation: Bernoulli Likelihood and Beta PriorLet: Likelihood: \\(P(D \\mid \\theta) \\propto \\theta^k (1 - \\theta)^{n - k}\\) Prior: \\(P(\\theta) = \\text{Beta}(\\alpha, \\beta) \\propto \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\\)Then:\\[P(\\theta \\mid D) \\propto \\theta^{k + \\alpha - 1}(1 - \\theta)^{n - k + \\beta - 1}\\]This is a Beta posterior: \\(\\text{Beta}(k + \\alpha, n - k + \\beta)\\).To find the MAP estimate (mode of the Beta distribution), we differentiate the log-posterior:\\[\\log P(\\theta \\mid D) = (k + \\alpha - 1) \\log \\theta + (n - k + \\beta - 1) \\log(1 - \\theta)\\]Take derivative and set to zero:\\[\\frac{d}{d\\theta} \\log P(\\theta \\mid D) =\\frac{k + \\alpha - 1}{\\theta} - \\frac{n - k + \\beta - 1}{1 - \\theta} = 0\\]Solving:\\[\\frac{k + \\alpha - 1}{\\theta} = \\frac{n - k + \\beta - 1}{1 - \\theta}\\Rightarrow (k + \\alpha - 1)(1 - \\theta) = (n - k + \\beta - 1)\\theta\\]Expanding:\\[k + \\alpha - 1 - (k + \\alpha - 1)\\theta = (n - k + \\beta - 1)\\theta\\]Move terms:\\[k + \\alpha - 1 = \\theta \\left[(n - k + \\beta - 1) + (k + \\alpha - 1)\\right]= \\theta (n + \\alpha + \\beta - 2)\\]Thus, the MAP estimate is:\\[\\hat{\\theta}_{\\text{MAP}} = \\frac{k + \\alpha - 1}{n + \\alpha + \\beta - 2}\\]When \\(\\alpha = \\beta = 1\\) (uniform prior), MAP reduces to MLE:\\[\\hat{\\theta}_{\\text{MAP}} = \\frac{k}{n}\\]Bernoulli Example with Beta PriorSuppose we toss a coin \\(n = 5\\) times and observe \\(k = 3\\) heads. Let \\(\\theta\\) be the probability of heads. The likelihood is:\\[P(k \\mid \\theta) = \\binom{5}{3} \\theta^3 (1 - \\theta)^2 \\propto \\theta^3 (1 - \\theta)^2\\] The prior is a Beta distribution: \\(P(\\theta) \\propto \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}\\). Let’s use \\(\\text{Beta}(2, 2)\\), so:\\[P(\\theta) \\propto \\theta (1 - \\theta)\\] The posterior is then:\\[P(\\theta \\mid D) \\propto \\theta^3 (1 - \\theta)^2 \\cdot \\theta (1 - \\theta) = \\theta^4 (1 - \\theta)^3\\] This corresponds to a Beta(5, 4) posterior. The MLE is:\\[\\hat{\\theta}_{\\text{MLE}} = \\frac{k}{n} = \\frac{3}{5} = 0.6\\] The MAP estimate (mode of Beta(5,4)) is:\\[\\hat{\\theta}_{\\text{MAP}} = \\frac{5 - 1}{5 + 4 - 2} = \\frac{4}{7} \\approx 0.571\\] Visualizationimport numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import betatheta_vals = np.linspace(0, 1, 200)# Likelihood (up to constant)k, n = 3, 5likelihood = theta_vals**k * (1 - theta_vals)**(n - k)likelihood /= np.trapz(likelihood, theta_vals)# Prior: Beta(2, 2)prior = beta.pdf(theta_vals, 2, 2)# Posterior: Beta(5, 4)posterior = beta.pdf(theta_vals, 5, 4)# Estimatestheta_mle = k / ntheta_map = (5 - 1) / (5 + 4 - 2)plt.figure(figsize=(8, 5))plt.plot(theta_vals, likelihood, label=\"Likelihood (MLE)\", linestyle=\"--\")plt.plot(theta_vals, prior, label=\"Prior: Beta(2,2)\", linestyle=\":\")plt.plot(theta_vals, posterior, label=\"Posterior: Beta(5,4)\", linewidth=2)plt.axvline(theta_mle, color=\"gray\", linestyle=\"--\", label=f\"MLE: {theta_mle:.2f}\")plt.axvline(theta_map, color=\"black\", linestyle=\":\", label=f\"MAP: {theta_map:.2f}\")plt.title(\"MAP vs MLE Estimation for Coin Bias\")plt.xlabel(\"θ (Probability of Heads)\")plt.ylabel(\"Density\")plt.legend()plt.grid(True)plt.tight_layout()plt.show()The plot below demonstrates how Maximum Likelihood Estimation (MLE) and Maximum A Posteriori (MAP) estimation differ when estimating the bias \\(\\theta\\) of a coin (i.e., the probability of getting heads).The goal is to estimate the most likely value of \\(\\theta\\) based on: A prior belief about the coin’s fairness (a Beta distribution), A small sample of observed data (3 heads out of 5 tosses). Figure: MAP vs MLE Estimation for Coin Bias Prior: A \\(\\text{Beta}(2,2)\\) distribution, centered at 0.5, representing a mild belief that the coin is fair. Observed Data: 5 tosses, with 3 heads and 2 tails. Likelihood: Based on the Binomial model:\\[P(D \\mid \\theta) \\propto \\theta^3 (1 - \\theta)^2\\] Posterior: With a conjugate Beta prior and Binomial likelihood, the posterior becomes:\\[\\text{Beta}(\\alpha + k, \\beta + n - k) = \\text{Beta}(5, 4)\\] Inferences we can make: MLE Ignores Prior Knowledge:The likelihood peaks at \\(\\theta = 0.6\\)—which is simply the empirical ratio \\(k/n = 3/5\\). This is the MLE and represents the estimate purely from data. MAP Blends Prior and Data:The posterior peaks at around \\(\\theta = 0.571\\). The MAP estimate is slightly pulled toward the prior mean (0.5) compared to MLE. This reflects the influence of the prior, which assumed the coin is more likely to be fair. Priors Act as Regularizers:The MAP estimator essentially acts like a regularized version of MLE—biasing the estimate toward prior beliefs, especially when sample size is small. With more data, the MAP and MLE would converge. Posterior Reflects Uncertainty More Holistically:Compared to the sharper likelihood, the posterior incorporates both data and prior uncertainty, making it slightly wider and smoother—especially relevant in low-data settings. MAP ≠ MLE When Priors Are Informative:This visualization is a concrete demonstration of how MAP ≠ MLE when the prior is not flat. It’s a critical concept when explaining regularization, Bayesian learning, or when modeling with limited data. To summarize, this plot provides a visual comparison of two common estimation strategies: MLE: Trusts only the data. MAP: Trusts both the data and a prior belief.When data is scarce (as it often is in real-world applications), the regularization effect of the prior becomes particularly useful. Bayesian methods provide a principled way to implement this regularization through posterior inference, and this example makes that visible and intuitive.Applications in Data ScienceLogistic RegressionMLE finds weights that minimize the negative log-likelihood. However, MAP estimation adds a prior over the weights (usually Gaussian), which leads to regularized logistic regression:\\[\\hat{w}_{\\text{MAP}} = \\arg\\min_w \\left[ \\sum_i \\log(1 + e^{-y_i x_i^T w}) + \\frac{\\lambda}{2} \\|w\\|^2 \\right]\\]This helps control overfitting, especially in high-dimensional spaces.Bayesian Linear RegressionIn linear regression, MAP estimation with a Gaussian prior yields Ridge regression:\\[\\hat{\\beta}_{\\text{MAP}} = \\arg\\min_{\\beta} \\left[ \\|y - X\\beta\\|^2 + \\lambda \\|\\beta\\|^2 \\right]\\]This provides stability when data is sparse or multicollinearity is present.Cold Start and Sparse Data ProblemsMAP estimators are essential when: Data is limited (e.g., few clicks or ratings per user). You want to encode prior beliefs (e.g., users prefer popular items). You want robust, regularized predictions rather than overfitting.MAP allows us to “pull” parameter estimates towards prior expectations when data is insufficient, making it highly useful in recommender systems and early-stage modeling.Conjugate PriorsOne of the elegant outcomes in Bayesian inference is that certain choices of priors lead to mathematically convenient posteriors. When a prior and its corresponding posterior distribution belong to the same family, the prior is said to be a conjugate prior for the likelihood function.This property is not just algebraic elegance — it enables closed-form updates, analytical tractability, and efficient implementation in sequential or real-time inference systems.Formal DefinitionA prior distribution \\(P(\\theta)\\) is said to be conjugate to a likelihood function \\(P(D \\mid \\theta)\\) if the posterior \\(P(\\theta \\mid D)\\) is in the same family of distributions as the prior.That is:\\[\\text{If } P(\\theta) \\in \\mathcal{F} \\text{ and } P(\\theta \\mid D) \\in \\mathcal{F} \\text{ as well, then } P(\\theta) \\text{ is conjugate.}\\]Some classic conjugate prior–likelihood pairs include: Likelihood Conjugate Prior Posterior Bernoulli/Binomial Beta Beta Poisson Gamma Gamma Normal (known variance) Normal Normal Multinomial Dirichlet Dirichlet Why It MattersConjugate priors greatly simplify Bayesian analysis. When using a conjugate prior: Posterior distributions can be derived analytically. Bayesian updating can be done incrementally and efficiently. The form of the prior helps encode domain knowledge (e.g., belief in fairness, expected rates, etc.)This is particularly useful in low-latency systems like online learning, A/B testing pipelines, and probabilistic graphical models where recomputation must be fast.Example: Beta Prior for a Bernoulli/Binomial LikelihoodSuppose we are modeling a binary outcome — say, a coin flip. The outcome is modeled as a Bernoulli process with unknown probability of success \\(\\theta\\).LikelihoodLet \\(x_1, \\dots, x_n\\) be i.i.d. Bernoulli trials with parameter \\(\\theta\\). The likelihood is:\\[P(D \\mid \\theta) = \\prod_{i=1}^{n} \\theta^{x_i}(1 - \\theta)^{1 - x_i}\\]Let \\(k = \\sum x_i\\) (number of successes), so:\\[P(D \\mid \\theta) = \\theta^k (1 - \\theta)^{n - k}\\]PriorWe choose a Beta prior:\\[P(\\theta) = \\frac{1}{B(\\alpha, \\beta)} \\theta^{\\alpha - 1}(1 - \\theta)^{\\beta - 1}\\]Where \\(\\alpha, \\beta \u0026gt; 0\\), and \\(B(\\alpha, \\beta)\\) is the Beta function:\\[B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha - 1} (1 - t)^{\\beta - 1} dt\\]PosteriorUsing Bayes’ theorem:\\[P(\\theta \\mid D) \\propto P(D \\mid \\theta) \\cdot P(\\theta)= \\theta^k (1 - \\theta)^{n - k} \\cdot \\theta^{\\alpha - 1} (1 - \\theta)^{\\beta - 1}= \\theta^{\\alpha + k - 1} (1 - \\theta)^{\\beta + n - k - 1}\\]Thus, the posterior is:\\[P(\\theta \\mid D) = \\text{Beta}(\\alpha + k, \\beta + n - k)\\]This clean and efficient update rule makes the Beta distribution the conjugate prior for the Bernoulli and Binomial likelihood.InterpretationEach time we observe new data in the form of successes and failures (e.g., from a Bernoulli or Binomial process), we can update the parameters of the Beta prior in a simple, additive way. This is one of the most intuitive benefits of conjugate priors.If the prior is \\(\\text{Beta}(\\alpha, \\beta)\\), and we observe: \\(k\\) successes, \\(n - k\\) failures,then the posterior becomes \\(\\text{Beta}(\\alpha', \\beta')\\), where:\\[\\alpha' = \\alpha + k\\]\\[\\beta' = \\beta + (n - k)\\]This rule makes it easy to perform sequential updates — as new observations come in, we can incrementally revise our posterior without needing to recompute from scratch. It’s especially valuable in streaming, online learning, and real-time probabilistic systems.VisualizationTo better understand how conjugate priors simplify Bayesian updating, let’s visualize how a Beta prior gets updated after observing data from a Binomial process. In this example, we assume a weakly informative prior belief about a coin’s fairness (Beta(2, 2)) and then observe 10 coin tosses with 7 heads. Because the Beta distribution is conjugate to the Binomial likelihood, we can compute the posterior analytically—resulting in another Beta distribution with updated parameters. This makes it easy to see how the prior and data interact to form the posterior.import numpy as npimport matplotlib.pyplot as pltfrom scipy.stats import beta# Observed data: 7 successes in 10 trialsk, n = 7, 10failures = n - k# Prior: Beta(2, 2)alpha_prior = 2beta_prior = 2# Posterior: Beta(2 + 7, 2 + 3) = Beta(9, 5)alpha_post = alpha_prior + kbeta_post = beta_prior + failurestheta = np.linspace(0, 1, 200)prior_pdf = beta.pdf(theta, alpha_prior, beta_prior)posterior_pdf = beta.pdf(theta, alpha_post, beta_post)plt.figure(figsize=(8, 5))plt.plot(theta, prior_pdf, label=\"Prior: Beta(2, 2)\", linestyle=\"--\")plt.plot(theta, posterior_pdf, label=\"Posterior: Beta(9, 5)\", linewidth=2)plt.title(\"Posterior Update with Conjugate Beta Prior\")plt.xlabel(\"θ (Probability of Success)\")plt.ylabel(\"Density\")plt.legend()plt.grid(True)plt.tight_layout()plt.show()This visualization shows how the prior belief (centered at 0.5) gets updated based on data favoring higher success probability. Figure: Posterior Update with Conjugate Beta Prior This plot compares two distributions: The prior: \\(\\text{Beta}(2, 2)\\) — symmetric around 0.5, representing mild uncertainty about the coin being fair. The posterior: \\(\\text{Beta}(9, 5)\\) — updated belief after observing 7 heads out of 10 flips.What’s Happening Under the Hood: The prior parameters \\(\\alpha = 2, \\beta = 2\\) represent 1 prior success and 1 prior failure (Beta counts start from one). After observing the data, we apply the conjugate update:\\[\\alpha_{\\text{posterior}} = \\alpha_{\\text{prior}} + k = 2 + 7 = 9\\]\\[\\beta_{\\text{posterior}} = \\beta_{\\text{prior}} + (n - k) = 2 + 3 = 5\\] The posterior is thus \\(\\text{Beta}(9, 5)\\) — a distribution that favors \\(\\theta \u0026gt; 0.5\\), aligning with the observed data, but still shaped by the prior.The Plot Shows: The prior curve is flat-ish and centered at 0.5, reflecting openness to a range of \\(\\theta\\) values. The posterior curve is steeper and shifted right, peaking around \\(\\theta \\approx 0.64\\). The update is not overconfident — the posterior still acknowledges uncertainty but now leans toward higher \\(\\theta\\) based on evidence.Inferences to make: Bayesian updating is additive and intuitive:With conjugate priors, updating is just a matter of adjusting counts. This keeps the math clean and interpretability high. The prior influences the posterior more when data is limited:In this example, with only 10 trials, the prior still has a noticeable effect. Had we used Beta(1, 1), the posterior would shift even further toward the empirical proportion (0.7). Posterior reflects a refined belief:The posterior balances prior belief and observed data, yielding a distribution that’s sharper than the prior but not as sharp as the maximum likelihood would suggest. This approach scales well:The same logic applies whether you’re flipping a coin, testing an email subject line, or modeling click-through rates. That’s why Beta-Binomial updates are widely used in A/B testing, online learning, and Bayesian filtering. Applications in Data ScienceBayesian A/B TestingIn online experimentation: Each version has a Beta prior over its conversion rate. New data updates the posterior in real-time. This allows comparing \\(P(\\theta_A \u0026gt; \\theta_B \\mid \\text{data})\\) directly, unlike frequentist p-values.Conjugate priors enable this with fast, closed-form updates and interpretable distributions.Real-Time User ModelingClick-through rates, open rates, fraud likelihoods — all modeled as binary outcomes. Beta priors can be updated on-the-fly as new data arrives, powering systems like: Dynamic personalization Spam filtering Risk scoring in transactionsBayesian Filtering and Probabilistic RoboticsIn robotics and control systems, conjugate priors are used for recursive Bayesian filters (e.g., Kalman filters, where Gaussians are conjugate to themselves) to update beliefs about position, velocity, or sensor noise.Conjugate priors marry theory and practice in Bayesian modeling. They offer a principled way to integrate domain knowledge, perform fast posterior updates, and maintain mathematical elegance — making them an indispensable tool in the probabilistic data scientist’s toolkit.Gaussian Processes (Intro)Gaussian Processes (GPs) offer a powerful and flexible framework for non-parametric Bayesian modeling. Unlike traditional models that learn a finite set of parameters (like coefficients in linear regression), GPs treat inference as learning a distribution over functions.This makes them ideal when you not only want to predict a value but also quantify uncertainty about that prediction — a crucial requirement in safety-critical applications such as medical diagnostics, robotics, and autonomous systems.What is a Gaussian Process?A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by: A mean function \\(m(x)\\), and A covariance function or kernel \\(k(x, x')\\).Formally, we write:\\[f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\\]Where: \\[m(x) = \\mathbb{E}[f(x)]\\] \\[k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\] This means that for any finite set of inputs \\(x_1, \\dots, x_n\\), the function values \\(f(x_1), \\dots, f(x_n)\\) follow a multivariate normal distribution:\\[[f(x_1), \\dots, f(x_n)]^\\top \\sim \\mathcal{N}(\\mu, K)\\]Where: \\[\\mu_i = m(x_i)\\] \\[K_{ij} = k(x_i, x_j)\\] Gaussian Process Regression: IntuitionLet us consider a regression problem where we are given a dataset of \\(n\\) observed input-output pairs:\\[D = \\{(x_i, y_i)\\}_{i=1}^n\\]We assume the outputs are generated from an unknown latent function \\(f(x)\\) with added Gaussian noise:\\[y_i = f(x_i) + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\\]Our goal is to learn about \\(f(x)\\) — not as a fixed parametric model, but as a distribution over possible functions. This is where Gaussian Processes come into play.A Gaussian Process (GP) is a prior over functions such that any finite collection of function values follows a multivariate Gaussian distribution:\\[f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\\]Here: \\(m(x) = \\mathbb{E}[f(x)]\\) is the mean function, often set to zero for simplicity. \\(k(x, x') = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\\) is the kernel or covariance function.Given training inputs \\(X = [x_1, ..., x_n]\\) and outputs \\(\\mathbf{y} = [y_1, ..., y_n]^T\\), and test inputs \\(X_*\\), the GP framework models the joint distribution of training and test outputs as:\\[\\begin{bmatrix}\\mathbf{y} \\\\\\mathbf{f}_*\\end{bmatrix}\\sim \\mathcal{N} \\left(\\begin{bmatrix}\\mathbf{0} \\\\\\mathbf{0}\\end{bmatrix},\\begin{bmatrix}K(X, X) + \\sigma_n^2 I \u0026amp; K(X, X_*) \\\\K(X_*, X) \u0026amp; K(X_*, X_*)\\end{bmatrix}\\right)\\]Where: \\(K(X, X)\\) is the \\(n \\times n\\) covariance matrix for training inputs. \\(K(X, X_*)\\) is the \\(n \\times m\\) cross-covariance between training and test inputs. \\(K(X_*, X_*)\\) is the \\(m \\times m\\) covariance of the test inputs. \\(\\sigma_n^2 I\\) adds noise to the diagonal (due to the assumed observational noise).The posterior predictive distribution for the function values at test points is then given by:\\[\\mathbf{f}_* \\mid X, \\mathbf{y}, X_* \\sim \\mathcal{N}(\\mu_*, \\Sigma_*)\\]Where: Posterior mean:\\[\\mu_* = K(X_*, X)[K(X, X) + \\sigma_n^2 I]^{-1} \\mathbf{y}\\] Posterior covariance:\\[\\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \\sigma_n^2 I]^{-1} K(X, X_*)\\] This formulation gives us both predictions (mean) and uncertainty (variance) at any set of new inputs.The Role of the Kernel FunctionThe kernel function \\(k(x, x')\\) defines the covariance structure between the function values at different inputs. It encodes prior assumptions about the function’s properties — smoothness, periodicity, linearity, etc. The choice of kernel is crucial as it determines the shape of the functions the GP considers likely.Here are some commonly used kernels:1. Radial Basis Function (RBF) or Squared Exponential KernelThis is the most widely used kernel due to its universal approximation properties and smoothness:\\[k(x, x') = \\exp\\left(-\\frac{(x - x')^2}{2\\ell^2}\\right)\\] \\(\\ell\\) is the length scale, controlling how quickly the function varies. Encourages infinitely differentiable, smooth functions. Implies that points closer in input space have highly correlated function values.2. Matern KernelA generalization of the RBF kernel that allows for less smoothness:\\[k_\\nu(r) = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\frac{\\sqrt{2\\nu}r}{\\ell} \\right)^\\nu K_\\nu \\left( \\frac{\\sqrt{2\\nu}r}{\\ell} \\right)\\] \\[r = |x - x'|\\] \\(\\nu\\) controls smoothness: e.g., \\(\\nu = 1/2\\) gives exponential kernel, \\(\\nu \\to \\infty\\) recovers RBF. Suitable for modeling rougher, more realistic functions in real-world applications.3. Dot Product (Linear) KernelUsed when the function is expected to be linear:\\[k(x, x') = x^T x'\\] Equivalent to Bayesian linear regression. Doesn’t model nonlinearity unless combined with other kernels.4. Periodic KernelModels functions with known repeating structure:\\[k(x, x') = \\exp\\left(-\\frac{2 \\sin^2(\\pi |x - x'| / p)}{\\ell^2}\\right)\\] \\(p\\) controls the period, \\(\\ell\\) controls smoothness. Ideal for seasonal data, time series, and cyclic behaviors.Why Kernels Matter in PracticeThe kernel acts as a prior over function space, shaping not only the kinds of functions the model will favor but also how information propagates across the input domain. Inference in a GP is guided entirely by the covariance implied by the kernel.Choosing a kernel is a modeling decision — like choosing a neural network architecture or a basis function family — but in GPs, it’s probabilistically grounded. And importantly, it’s differentiable and tunable: you can learn kernel parameters (like \\(\\ell\\)) by maximizing the marginal likelihood.VisualizationTo illustrate how Gaussian Processes model distributions over functions, let’s walk through a simple 1D regression example using scikit-learn. We’ll fit a GP to a small set of noisy training points, allowing it to not only predict the mean function but also provide a confidence interval that reflects its uncertainty. The GP is equipped with an RBF (Radial Basis Function) kernel, which assumes smoothness in the underlying function.This example visually demonstrates one of the GP’s most powerful features: it can interpolate sparse data while expressing its uncertainty about regions it has not seen.import numpy as npimport matplotlib.pyplot as pltfrom sklearn.gaussian_process import GaussianProcessRegressorfrom sklearn.gaussian_process.kernels import RBF# Training data (sparse)X_train = np.array([[1], [3], [5], [6]])y_train = np.sin(X_train).ravel()# Define GP with RBF kernelkernel = RBF(length_scale=1.0)gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-2)gp.fit(X_train, y_train)# Predictive distribution at test pointsX_test = np.linspace(0, 10, 100).reshape(-1, 1)y_pred, sigma = gp.predict(X_test, return_std=True)# Plotplt.figure(figsize=(8, 5))plt.plot(X_train, y_train, 'ro', label='Training Data')plt.plot(X_test, y_pred, 'b-', label='Mean Prediction')plt.fill_between(X_test.ravel(), y_pred - 1.96 * sigma, y_pred + 1.96 * sigma, alpha=0.3, label='95% Confidence Interval')plt.title(\"Gaussian Process Regression\")plt.xlabel(\"x\")plt.ylabel(\"f(x)\")plt.legend()plt.grid(True)plt.tight_layout()plt.show() Figure: Gaussian Process Regression The Plot Shows: Red dots represent the observed training data points. The blue line is the mean prediction of the Gaussian Process at every point in the input space. The shaded region corresponds to a 95% confidence interval, calculated as:\\[\\hat{f}(x) \\pm 1.96 \\cdot \\sigma(x)\\] where \\(\\hat{f}(x)\\) is the predicted mean and \\(\\sigma(x)\\) is the standard deviation from the posterior. Insights we can infer from this are Probabilistic Predictions, Not Just Point EstimatesUnlike traditional regressors (like polynomial or linear regression), the GP predicts a distribution over functions, not a single best fit. For each input \\(x\\), it returns a mean prediction and a measure of uncertainty. Uncertainty Reflects Data CoverageThe model is most confident (i.e., narrowest uncertainty band) near the observed data points, and increasingly uncertain as we move away from them. This is especially visible at the edges (near \\(x = 0\\) and \\(x = 10\\)), where the GP hasn’t seen any training data. The Role of the RBF KernelThe RBF kernel assumes that points closer in input space produce similar outputs. It’s what gives the GP its smooth, wavy behavior. If a different kernel were used (e.g., linear or periodic), the shape of the mean and uncertainty band would change. Handling Small Datasets GracefullyDespite having only four training points, the GP constructs a smooth function that captures the structure of the underlying sine wave—without overfitting. This makes it particularly useful in low-data regimes like: Experimental design, Bayesian optimization, Medical modeling, where samples are expensive. Interpretable UncertaintyThe shaded band gives us a principled way to quantify model confidence. Unlike confidence intervals in frequentist regression, which apply to the parameter estimate, the GP’s uncertainty is pointwise and interpretable: “Here’s how unsure the model is at this input.”This showcases the core strength of Gaussian Processes: flexible, non-parametric regression with uncertainty quantification. By treating the prediction as a distribution over functions, the GP provides not only a best guess but also a principled expression of how much we trust that guess at each point. This property is crucial in settings where uncertainty is as important as accuracy.This visualization illustrates: How GPs interpolate observed points with smooth curves, How prediction uncertainty grows away from the training data, And how the kernel controls the shape of the functions we consider probable.Applications in Data ScienceBayesian OptimizationGPs are often used as surrogate models in Bayesian Optimization, where the goal is to optimize a black-box function that is expensive to evaluate. The GP captures both the function estimate and its uncertainty, enabling exploration vs. exploitation strategies.Uncertainty-Aware RegressionGPs naturally model predictive uncertainty. This is vital in: Medical diagnosis (model confidence matters), Sensor fusion in robotics (merging noisy measurements), Scientific modeling (where understanding the confidence of predictions is key).Small-Data RegimesIn cases where data is expensive or scarce (e.g., drug discovery, experimental physics), GPs shine because they can learn complex patterns without overfitting and provide principled uncertainty.Active LearningBecause GPs quantify uncertainty, they are ideal for active learning — selecting new data points where the model is uncertain to improve learning efficiency.Gaussian Processes offer an elegant solution to the problem of modeling unknown functions with uncertainty. Their interpretability, flexibility, and strong theoretical foundation make them a powerful tool for data scientists working in probabilistic modeling, especially in safety-critical and small-data domains.Bayesian Foundations in Modern Data ScienceThe concepts explored throughout this discussion—Bayes’ theorem, priors and posteriors, likelihoods, conjugate priors, MLE, MAP, and Gaussian Processes—collectively form a foundational perspective on statistical inference. These ideas extend beyond isolated techniques; they represent a systematic approach to learning from data in uncertain settings. Bayesian methods offer a coherent framework for incorporating prior knowledge, updating beliefs based on observed evidence, and reasoning in a probabilistic manner.Among the earliest and most accessible examples is the Naive Bayes classifier, which applies Bayes’ theorem to supervised classification tasks. Despite its assumption that features are conditionally independent given the class label, the model often performs well in practice, especially in high-dimensional problems like spam detection or document categorization. It constructs a posterior distribution over class labels:\\[P(C \\mid x_1, x_2, \\dots, x_n) \\propto P(C) \\prod_{i=1}^n P(x_i \\mid C)\\]In this formulation, the prior class probabilities \\(P(C)\\) and the conditional likelihoods \\(P(x_i \\mid C)\\) can be estimated using MLE or MAP techniques. If needed, conjugate priors can be used to incorporate prior knowledge and simplify inference. While simple, the model’s efficiency, transparency, and probabilistic interpretation make it a useful baseline in a variety of real-world systems.More expressive probabilistic modeling is possible when one relaxes the independence assumptions and considers the dependency structure among variables. This leads to the framework of Bayesian networks, which use directed acyclic graphs to represent conditional dependencies. The joint distribution is factored as:\\[P(X_1, X_2, \\dots, X_n) = \\prod_{i=1}^n P(X_i \\mid \\text{Parents}(X_i))\\]Each node in the graph corresponds to a random variable, and each edge indicates a dependency. This decomposition generalizes the ideas seen in Naive Bayes by allowing richer structures that capture causality, interaction effects, and shared influences. Parameters in Bayesian networks can be estimated using MLE or MAP, and conjugate priors are often employed to simplify learning, especially in the presence of missing or noisy data. These models are well-suited for complex reasoning tasks, such as medical diagnosis, credit risk modeling, and user behavior analysis.While the models discussed so far rely on explicit parameterizations, Gaussian Processes offer a non-parametric alternative by placing a prior over the space of functions themselves. Rather than defining a fixed number of parameters, a Gaussian Process assumes that any finite set of function values follows a multivariate normal distribution. This is particularly valuable in regression tasks where predictive uncertainty is as important as the predicted value.A Gaussian Process model, once conditioned on observed data, yields a posterior distribution over possible functions, complete with mean predictions and confidence intervals. The choice of kernel function encodes assumptions about smoothness, periodicity, or other properties of the underlying function. This makes Gaussian Processes especially useful when working with small datasets, where flexibility and uncertainty quantification are critical.A prominent use case for Gaussian Processes is in Bayesian optimization, a strategy for optimizing functions that are expensive or difficult to evaluate. Here, a GP acts as a surrogate model that approximates the true objective, guiding the selection of future evaluations based on both predicted values and uncertainty. This methodology has become a standard tool in hyperparameter tuning, experimental design, and materials discovery.The broader value of Bayesian modeling lies in its capacity to quantify and propagate uncertainty. In domains such as healthcare, autonomous systems, and finance, uncertainty-aware predictions support better decision-making under risk. For instance, posterior class probabilities from a Naive Bayes model can inform risk thresholds in a classifier; MAP estimates provide regularization by incorporating prior constraints; and Gaussian Processes yield confidence bounds that reflect the limits of available information. These capabilities are not peripheral—they are central to deploying models that must make informed decisions under real-world conditions.As we close our discussion on Bayesian inference, the natural next step in our learning journey is to explore how statistical inference—particularly in its frequentist form—shapes decision-making in data science. In the upcoming part of this series, we will dive into the core principles behind A/B testing, sampling techniques, confidence intervals, and hypothesis testing frameworks such as the Z-test, T-test, and ANOVA. We’ll also unpack concepts like p-values, statistical power, and Type I/II errors to understand how to validate results under uncertainty. These tools are critical when measuring the effect of product changes, analyzing experiment results, or quantifying the impact of features in machine learning workflows. If Bayesian inference teaches us how to update beliefs, classical inference equips us to rigorously evaluate them—and together, they form a powerful toolkit for modern data science."},{"title":"Probability \u0026 Statistics for Data Science - Probability Distributions","url":"/blog/2022/mathforml-probstat2/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, probability-statistics, math","date":"June 05, 2022","content":"Probability distributions are the foundation of machine learning. They shape how we simulate data, quantify uncertainty, and reason about model behavior. Whether you are generating synthetic samples, fitting probabilistic models, or understanding errors, distributions provide the structure for everything that follows.In this article, we will explore: Important discrete and continuous distributions Concepts around multivariate distributions Key applications in machine learning and data science Demonstrations using Python and Plotly visualizationsDiscrete DistributionsDiscrete probability distributions represent outcomes that are countable — such as binary labels, event counts, or integer results of repeated trials. These are particularly relevant in classification tasks, anomaly detection, and simulations of real-world processes.1. Bernoulli DistributionThe Bernoulli distribution is used to model a single trial with two possible outcomes: success (1) or failure (0). It is the building block of binary classification problems.Probability Mass Function (PMF):\\(P(X = x) = p^x (1 - p)^{1 - x}, \\quad x \\in \\{0,1\\}\\) Mean: \\(\\mathbb{E}[X] = p\\) Variance: \\(\\text{Var}(X) = p(1 - p)\\)Applications: Binary classification (e.g., logistic regression target) Simulating binary labels for synthetic datasets The plot below illustrates a Bernoulli distribution where the probability of success (1) is 0.7 and failure (0) is 0.3. This type of distribution is ideal for binary classification tasks where outcomes are either “yes” or “no”, such as predicting if a customer will churn. The interactive bar chart helps visualize how the probability is distributed between the two outcomes. 2. Binomial DistributionThe Binomial distribution generalizes the Bernoulli trial to \\(n\\) repeated trials. It models the number of successes in a fixed number of independent experiments.PMF:\\(P(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\\) Mean: \\(\\mathbb{E}[X] = np\\) Variance: \\(\\text{Var}(X) = np(1 - p)\\)Applications: Ensemble model success probability Simulating repeated trials This interactive chart represents the probability of getting a certain number of successes in 10 independent trials, each with a 50% success rate. The binomial distribution models many real-world situations in machine learning, such as predicting how many models in an ensemble will make correct predictions. Notice how the distribution is symmetric when \\(p = 0.5\\) and peaks around \\(n \\cdot p = 5\\). 3. Poisson DistributionThe Poisson distribution models the number of events that occur in a fixed interval of time or space, assuming the events occur independently and at a constant rate \\(\\lambda\\).PMF:\\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\) Mean: \\(\\mathbb{E}[X] = \\lambda\\) Variance: \\(\\text{Var}(X) = \\lambda\\)Applications: Anomaly detection (e.g., fraud or system failures) Modeling event frequency (web traffic, queueing)from scipy.stats import poissonlmbda = 4x = np.arange(0, 15)pmf = poisson.pmf(x, lmbda)plt.bar(x, pmf)plt.title(f\"Poisson Distribution (λ={lmbda})\")plt.xlabel(\"k (events)\")plt.ylabel(\"Probability\")plt.show() The Poisson distribution is commonly used for modeling the number of times an event occurs in a fixed interval. In the plot below, we use \\(\\lambda = 4\\) to show the probability distribution of event counts. This distribution is especially useful in anomaly detection — for example, identifying if the number of failed login attempts is abnormally high.Continuous DistributionsContinuous distributions represent variables that can take on any value within a range. They are essential in feature modeling, regression analysis, generative modeling, and more.1. Uniform DistributionThis distribution assigns equal probability to all values in an interval $[a, b]$.PDF:\\(f(x) = \\frac{1}{b - a}, \\quad a \\leq x \\leq b\\)Applications: Random initialization (e.g., neural network weights) Data simulation and control baselinessamples = np.random.uniform(0, 1, 10000)sns.histplot(samples, bins=30, kde=True)plt.title(\"Uniform Distribution [0,1]\")plt.show()2. Normal (Gaussian) DistributionThe Normal distribution is the most widely used distribution in statistics and machine learning, due to the Central Limit Theorem.PDF:\\(f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{- \\frac{(x - \\mu)^2}{2\\sigma^2}}\\) Mean: \\(\\mu\\) Variance: \\(\\sigma^2\\)Applications: Linear regression assumptions PCA and feature decorrelation Modeling errors and noisemu, sigma = 0, 1samples = np.random.normal(mu, sigma, 10000)sns.histplot(samples, bins=30, kde=True)plt.title(\"Normal Distribution (μ=0, σ=1)\")plt.show() The bell-shaped curve shown below represents the standard normal distribution. This is foundational to many algorithms in statistics and machine learning. It models noise, errors, and is used in methods like Principal Component Analysis (PCA). Thanks to the Central Limit Theorem, many sample-based statistics tend to follow this distribution even when the original data is not normal. 3. Exponential DistributionThe exponential distribution describes the time between events in a Poisson process.PDF:\\(f(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\\)Applications: Survival and reliability analysis Modeling waiting timeslmbda = 1samples = np.random.exponential(1/lmbda, 10000)sns.histplot(samples, bins=30, kde=True)plt.title(\"Exponential Distribution\")plt.show() This visualization shows the exponential distribution, which is often used to model the time between events in a Poisson process. It’s especially useful in survival analysis and reliability engineering. The curve declines rapidly, showing that events are most likely to happen shortly after the last one, and become less likely over time. 4. Beta DistributionA flexible distribution on the interval [0,1], defined by two shape parameters \\(\\alpha\\) and \\(\\beta\\).PDF:\\(f(x; \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\\)Applications: Bayesian modeling of probabilities Thompson sampling in reinforcement learningfrom scipy.stats import betax = np.linspace(0, 1, 1000)for a, b in [(2, 5), (5, 2), (2, 2)]: plt.plot(x, beta.pdf(x, a, b), label=f\"α={a}, β={b}\")plt.title(\"Beta Distributions\")plt.legend()plt.show() The Beta distribution is defined over the interval [0,1] and is commonly used to model probabilities themselves. In the visualization below, we use shape parameters \\(\\alpha = 2\\) and \\(\\beta = 5\\). This creates a distribution skewed toward 0, reflecting a belief that lower probability values are more likely. Beta distributions are essential in Bayesian statistics and exploration-exploitation algorithms like Thompson Sampling. 5. Gamma DistributionA two-parameter generalization of the exponential distribution.PDF:\\(f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\)Applications: Waiting time models Priors in Bayesian inferencefrom scipy.stats import gammax = np.linspace(0, 20, 1000)for shape in [1, 2, 5]: plt.plot(x, gamma.pdf(x, shape), label=f\"Shape={shape}\")plt.title(\"Gamma Distributions\")plt.legend()plt.show()Multivariate DistributionsMultivariate distributions model joint behavior of multiple variables, especially when they are correlated.Joint, Marginal, and Conditional Distributions Joint distribution: \\(P(X, Y)\\) gives the probability of two variables occurring together. Marginal distribution: \\(P(X) = \\sum_Y P(X, Y)\\), projecting one variable. Conditional distribution: \\(P(Y \\mid X) = \\frac{P(X, Y)}{P(X)}\\)These are critical for reasoning about dependencies, causality, and generative models.Multivariate Normal DistributionDefined by a mean vector \\(\\mu\\) and a covariance matrix \\(\\Sigma\\).\\[X \\sim \\mathcal{N}(\\mu, \\Sigma)\\]Applications: PCA (eigen decomposition of \\(\\Sigma\\)) Gaussian Mixture Models (GMMs) Modeling correlated featuresimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsmean = [0, 0]cov = [[1, 0.8], [0.8, 1]]samples = np.random.multivariate_normal(mean, cov, size=1000)sns.scatterplot(x=samples[:, 0], y=samples[:, 1])plt.title(\"Samples from Bivariate Normal Distribution\")plt.xlabel(\"X1\")plt.ylabel(\"X2\")plt.axis(\"equal\")plt.show()Simulating Datasets in PracticeSimulating data allows us to prototype models, test ideas, and understand algorithms.Example: Binary Classification Datasetfrom sklearn.datasets import make_classificationimport pandas as pdX, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_classes=2)df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(X.shape[1])])df[\"Target\"] = yprint(df.head())Example: Two-Class Gaussian Clustersclass0 = np.random.normal(loc=-2, scale=1, size=(500, 2))class1 = np.random.normal(loc=2, scale=1, size=(500, 2))labels = np.array([0]*500 + [1]*500)X = np.vstack([class0, class1])Summary Table: Choosing the Right Distribution Distribution Common Application Bernoulli Binary classification Binomial Ensemble voting, success trials Poisson Anomaly detection, event counts Normal Error modeling, PCA, regression Exponential Time-to-event, survival modeling Beta Probability modeling, Bayesian inference Gamma Duration modeling, Bayesian priors Multivariate Normal Feature correlation, PCA, GMM Probability distributions are at the core of every model you build in machine learning. They guide how you generate, structure, and analyze data. Whether you are simulating features, modeling uncertainty, or decomposing variance in PCA, the right distribution makes all the difference.In the next post, we’ll move into the world of Bayesian inference and learn how concepts like MLE, MAP, and priors shape our understanding of parameter estimation."},{"title":"Probability \u0026 Statistics for Data Science - Foundations of Probability","url":"/blog/2022/mathforml-probstat1/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, probability-statistics, math","date":"June 01, 2022","content":"When we build machine learning models, we’re not just dealing with data — we’re dealing with uncertainty. Whether we’re classifying emails, predicting stock prices, or detecting fraud, our models are built on probabilistic foundations that allow them to reason under uncertainty.This post covers the core probability theory concepts that underpin machine learning, deep learning, and AI. We’ll explore how concepts like sample space, events, random variables, expectation, variance, the Law of Large Numbers, and the Central Limit Theorem connect directly to real-world ML applications.Sample Space, Events, and Conditional ProbabilityTo understand how probability plays a role in ML, we must first start with the basics. Sample Space ( \\(\\Omega\\) ): The set of all possible outcomes of an experiment. Event: A subset of the sample space. It’s a collection of outcomes we’re interested in. Probability Function: Assigns a number between 0 and 1 to each event, satisfying the axioms of probability.Example: Suppose we build a binary classifier to detect spam. \\[\\Omega = \\{\\text{spam}, \\text{not spam}\\}\\] \\(P(\\text{spam}) = 0.4\\), \\(P(\\text{not spam}) = 0.6\\)IndependenceTwo events \\(A\\) and \\(B\\) are independent if the occurrence of one does not affect the probability of the other:\\[P(A \\cap B) = P(A) \\cdot P(B)\\]In ML, this concept appears in Naive Bayes classifiers, where we assume that features are conditionally independent given the class label — a simplification that works surprisingly well in practice.Conditional ProbabilityConditional probability tells us the probability of an event \\(A\\) given that event \\(B\\) has occurred:\\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\]This is the foundation of Bayes’ Theorem, which is used to update predictions as new information becomes available — exactly what happens when your email spam filter learns from new messages.Random Variables (Discrete and Continuous)A random variable maps outcomes from the sample space to numerical values.Discrete Random VariablesThese take countable values (like integers). Examples include: Number of clicks on an ad Whether a transaction is fraudulent (0 or 1)A discrete random variable \\(X\\) has a probability mass function (PMF):\\[P(X = x) = p(x)\\]Continuous Random VariablesThese take uncountably infinite values (e.g., any real number). Examples: The exact temperature in a room Probability of customer spendingA continuous random variable has a probability density function (PDF) such that:\\[P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\\]In ML, we use random variables to model data distributions. For example, we assume weights in Bayesian models come from a Gaussian prior — a continuous random variable.Expectation, Variance, and Standard DeviationThese are the building blocks of descriptive statistics and are critical to understanding model behavior.ExpectationAlso called the mean or expected value: For discrete \\(X\\):\\(\\mathbb{E}[X] = \\sum_x x \\cdot P(X = x)\\) For continuous \\(X\\):\\(\\mathbb{E}[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\\) Think of it as the center of mass of the distribution.VarianceMeasures the spread of the random variable around the mean:\\[\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\\]This gives us insight into how uncertain a prediction might be. A model with low variance makes consistent predictions.Standard DeviationThe square root of variance:\\[\\sigma = \\sqrt{\\text{Var}(X)}\\]Used commonly in ML to normalize features and analyze error distributions.Law of Large Numbers (LLN)The Law of Large Numbers states that as the sample size \\(n\\) increases, the sample mean of random variables converges to the true mean:\\[\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i = \\mathbb{E}[X]\\]This justifies why averaging predictions across many models (ensembles) improves performance — individual errors average out.Let’s simulate this idea by drawing samples from a uniform distribution \\(\\mathcal{U}(0, 1)\\) and watching how the sample mean stabilizes:import numpy as npimport matplotlib.pyplot as plt# Reproducibilitynp.random.seed(42)# Generate 10,000 samples from a Uniform[0,1] distributionsamples = np.random.uniform(0, 1, 10000)# Compute sample means as we increase sample sizesample_means = [np.mean(samples[:i]) for i in range(1, len(samples) + 1)]# Plottingplt.figure(figsize=(10, 5))plt.plot(sample_means, label='Sample Mean')plt.axhline(y=0.5, color='red', linestyle='--', label='True Mean = 0.5')plt.xlabel(\"Number of Samples\")plt.ylabel(\"Sample Mean\")plt.title(\"Law of Large Numbers (LLN)\")plt.legend()plt.show() As shown in the plot, the sample mean starts off noisy but converges toward 0.5, which is the true expected value of a uniform distribution over [0, 1].ML Insight:In bagging methods (e.g., Random Forests), we train many models on bootstrapped samples and average their predictions. The LLN guarantees that as the number of trees increases, the aggregate prediction becomes more stable and closer to the true signal.Central Limit Theorem (CLT)The CLT is one of the most powerful ideas in all of statistics. Theorem: The sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables approaches a Normal distribution, regardless of the original distribution.Formally, for i.i.d. variables \\(X_1, X_2, \\dots, X_n\\):\\[\\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\quad \\text{and} \\quad \\frac{\\bar{X}_n - \\mu}{\\sigma / \\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1)\\]This explains why the normal distribution is so prevalent in ML — it naturally arises when averaging data, sampling errors, or even when computing model parameter estimates.Let’s visualize this with a non-Gaussian distribution — the exponential distribution:import numpy as npimport matplotlib.pyplot as pltimport seaborn as sns# Draw means from 1000 samples, each of size 50, from an Exponential distributionmeans = []for _ in range(1000): sample = np.random.exponential(scale=1.0, size=50) means.append(np.mean(sample))# Plottingplt.figure(figsize=(10, 5))sns.histplot(means, bins=30, kde=True)plt.title(\"Central Limit Theorem (CLT): Means from Exponential Distribution\")plt.xlabel(\"Sample Mean\")plt.ylabel(\"Frequency\")plt.show() Despite the exponential distribution being highly skewed, the histogram of sample means is bell-shaped, showing the CLT in action.Applications in Machine LearningNow that we’ve built up the mathematical foundations, let’s see how they directly impact practical machine learning workflows.Probabilistic ModelingProbabilistic models like: Naive Bayes Gaussian Mixture Models Bayesian Linear Regression…are all rooted in these probability concepts. They define likelihoods, priors, and posteriors, which use conditional probability, distributions, and expectation.Even advanced models like Variational Autoencoders (VAEs) rely on these basics — using expectations, KL divergence, and Normal distributions.Uncertainty QuantificationModels that output probabilities (like softmax classifiers or probabilistic regressors) provide a distribution over outputs, allowing you to quantify uncertainty. Knowing the variance of a model’s prediction helps determine confidence. Using the CLT, you can estimate confidence intervals around predictions.This is critical in high-stakes applications like medicine, finance, and autonomous driving.Why Empirical Means Work (Ensemble Models)Ensemble techniques like: Bagging (Random Forests) Boosting (XGBoost, LightGBM) Model Averaging (e.g., Stacking)…all rely on the Law of Large Numbers and Central Limit Theorem.By aggregating multiple noisy models, the final prediction has: Lower variance Greater stability Better generalizationThe magic lies in averaging — and the math tells us why that works.Wrapping UpProbability theory is more than a theoretical curiosity — it’s the engine behind machine learning. From modeling uncertainties, understanding feature distributions, building generative models, and even constructing deep learning layers — these foundational ideas are everywhere.Next Up: In the next post, we’ll dive into Probability Distributions — understanding Bernoulli, Binomial, Gaussian, and how they shape the models we build."},{"title":"Linear Algebra Basics for ML - Advanced Topics","url":"/blog/2022/mathforml-linalg6/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"February 07, 2022","content":"Advanced Matrix Factorizations in Machine LearningMatrix factorizations are a foundational tool in linear algebra and play a critical role in modern machine learning. They simplify matrix computations, enable numerical stability, and reveal latent structures in data. This post explores several advanced matrix factorizations used in machine learning and deep learning, with complete mathematical derivations and practical insights.This section covers: LU Decomposition Cholesky Decomposition QR Decomposition Non-negative Matrix Factorization (NMF)1. LU DecompositionDefinitionLU decomposition factors a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) into the product of two matrices:\\[A = LU\\]Where: \\(L\\) is a lower triangular matrix with ones on its diagonal (\\(L_{ii} = 1\\)), \\(U\\) is an upper triangular matrix.If row pivoting is required, the decomposition is written as:\\[PA = LU\\]Where \\(P\\) is a permutation matrix.AlgorithmGiven:\\[A = \\begin{bmatrix}a_{11} \u0026amp; a_{12} \u0026amp; \\cdots \u0026amp; a_{1n} \\\\a_{21} \u0026amp; a_{22} \u0026amp; \\cdots \u0026amp; a_{2n} \\\\\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\a_{n1} \u0026amp; a_{n2} \u0026amp; \\cdots \u0026amp; a_{nn}\\end{bmatrix},\\]we use Gaussian elimination to eliminate entries below the diagonal. The multipliers used are stored in \\(L\\), and the remaining upper triangular form becomes \\(U\\).For each row \\(j \u0026gt; i\\), compute the multiplier:\\[l_{ji} = \\frac{a_{ji}}{a_{ii}}\\]Then update row \\(j\\):\\[a_{j\\cdot} = a_{j\\cdot} - l_{ji} \\cdot a_{i\\cdot}\\]After completing all iterations, the matrix \\(A\\) is factorized into \\(L\\) and \\(U\\).Applications in Machine Learning Efficiently solving systems of equations \\(Ax = b\\) by solving \\(Ly = b\\) then \\(Ux = y\\), Matrix inversion, Numerical optimization routines in regression and convex problems.2. Cholesky DecompositionDefinitionCholesky decomposition applies to symmetric, positive-definite matrices. For a matrix \\(A \\in \\mathbb{R}^{n \\times n}\\) satisfying: \\(A = A^T\\) (symmetry), \\(x^T A x \u0026gt; 0\\) for all non-zero \\(x \\in \\mathbb{R}^n\\) (positive-definiteness),there exists a unique lower triangular matrix \\(L\\) such that:\\[A = LL^T\\]ConstructionWe compute each entry of \\(L\\) as follows:For diagonal entries:\\[L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\\]For off-diagonal entries where \\(i \u0026gt; j\\):\\[L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=1}^{j-1} L_{ik} L_{jk} \\right)\\]All entries above the diagonal are zero.Applications in Machine Learning Efficient sampling from multivariate Gaussian distributions, Gaussian Processes (for inverting the kernel matrix), Kalman filters and Bayesian updates.3. QR DecompositionDefinitionFor any matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) where \\(m \\geq n\\), the QR decomposition expresses \\(A\\) as:\\[A = QR\\]Where: \\(Q \\in \\mathbb{R}^{m \\times m}\\) is orthogonal (\\(Q^T Q = I\\)), \\(R \\in \\mathbb{R}^{m \\times n}\\) is upper triangular.For economy-size decomposition (when only \\(n\\) orthogonal vectors are needed), we use:\\[A = Q_{\\text{red}} R, \\quad Q_{\\text{red}} \\in \\mathbb{R}^{m \\times n}\\]Gram-Schmidt OrthogonalizationLet \\(a_1, a_2, \\ldots, a_n\\) be the columns of \\(A\\). We generate orthonormal vectors \\(q_1, q_2, \\ldots, q_n\\) using: Initialization:\\[u_1 = a_1, \\quad q_1 = \\frac{u_1}{\\|u_1\\|}\\] For \\(k = 2, \\ldots, n\\):\\[u_k = a_k - \\sum_{j=1}^{k-1} \\langle a_k, q_j \\rangle q_j\\]\\[q_k = \\frac{u_k}{\\|u_k\\|}\\]Then define:\\[Q = [q_1, q_2, \\ldots, q_n], \\quad R = Q^T A\\]Least Squares via QRGiven an overdetermined system \\(Ax = b\\), we solve:\\[A = QR \\Rightarrow QRx = b\\]Then:\\[Rx = Q^T b\\]Since \\(R\\) is upper triangular, this is solved efficiently via back substitution.Applications in Machine Learning Numerically stable solution for linear regression, Eigenvalue computations using the QR algorithm, Orthonormal basis construction.4. Non-negative Matrix Factorization (NMF)DefinitionGiven a non-negative matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) (i.e., \\(A_{ij} \\geq 0\\)), NMF seeks matrices \\(W \\in \\mathbb{R}^{m \\times k}\\) and \\(H \\in \\mathbb{R}^{k \\times n}\\) such that:\\[A \\approx WH\\]subject to:\\[W \\geq 0, \\quad H \\geq 0\\]Optimization ProblemThe factorization is found by solving:\\[\\min_{W, H \\geq 0} \\|A - WH\\|_F^2\\]where \\(\\|\\cdot\\|_F\\) denotes the Frobenius norm.Multiplicative Update RulesA common approach (Lee \u0026amp; Seung, 2001) involves the following update rules: Update \\(H\\):\\[H_{ij} \\leftarrow H_{ij} \\cdot \\frac{(W^T A)_{ij}}{(W^T W H)_{ij}}\\] Update \\(W\\):\\[W_{ij} \\leftarrow W_{ij} \\cdot \\frac{(A H^T)_{ij}}{(W H H^T)_{ij}}\\]These updates are applied iteratively until convergence.Applications in Machine Learning Topic modeling from document-term matrices, Collaborative filtering in recommendation systems, Image compression and decomposition, Clustering with parts-based representation.Orthogonalization Techniques in Machine Learning and Deep LearningOrthogonalization plays a central role in linear algebra and is extensively used in various machine learning and deep learning tasks. Whether it’s constructing orthonormal bases, decorrelating features, or stabilizing neural network training, orthogonal structures are powerful due to their numerical and geometric properties.Now we shall cover: The Gram-Schmidt orthogonalization process for constructing orthonormal bases, Orthogonal initialization in neural networks for improved stability and convergence.1. Orthogonalization and Orthonormal BasesMotivationGiven a set of linearly independent vectors \\(\\{v_1, v_2, \\ldots, v_n\\}\\) in \\(\\mathbb{R}^n\\), it is often desirable to construct an orthonormal basis \\(\\{q_1, q_2, \\ldots, q_n\\}\\) that spans the same subspace, where: Vectors are orthogonal: \\(\\langle q_i, q_j \\rangle = 0\\) for \\(i \\ne j\\), Vectors are normalized: \\(\\|q_i\\| = 1\\).Orthonormal bases are easier to work with: Projections are straightforward, Matrix representations (e.g., \\(Q^T Q = I\\)) are numerically stable, Useful for dimensionality reduction and decorrelation (e.g., PCA).2. The Gram-Schmidt ProcessDefinitionThe Gram-Schmidt process transforms a set of linearly independent vectors \\(\\{v_1, v_2, \\ldots, v_n\\}\\) into an orthonormal set \\(\\{q_1, q_2, \\ldots, q_n\\}\\) spanning the same subspace.This is achieved by iteratively subtracting projections onto previously computed vectors and normalizing.Step-by-step AlgorithmLet the input be vectors \\(v_1, v_2, \\ldots, v_n \\in \\mathbb{R}^d\\). Initialize the first orthonormal vector:\\[q_1 = \\frac{v_1}{\\|v_1\\|}\\] Iterate for \\(k = 2, \\ldots, n\\): Project \\(v_k\\) onto each previous \\(q_j\\): \\[\\text{proj}_{q_j}(v_k) = \\langle v_k, q_j \\rangle q_j\\] Subtract the projections: \\[u_k = v_k - \\sum_{j=1}^{k-1} \\langle v_k, q_j \\rangle q_j\\] Normalize to get the next orthonormal vector: \\[q_k = \\frac{u_k}{\\|u_k\\|}\\] After the process, the vectors \\(\\{q_1, \\ldots, q_n\\}\\) form an orthonormal basis of the span of \\(\\{v_1, \\ldots, v_n\\}\\).ExampleLet:\\[v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad v_2 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\]Compute:\\[q_1 = \\frac{v_1}{\\|v_1\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\]Project \\(v_2\\) onto \\(q_1\\):\\[\\langle v_2, q_1 \\rangle = \\frac{1}{\\sqrt{2}}(1 \\cdot 1 + 0 \\cdot 1) = \\frac{1}{\\sqrt{2}}\\]Then:\\[u_2 = v_2 - \\langle v_2, q_1 \\rangle q_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{\\sqrt{2}} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{2} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{bmatrix}\\]Normalize:\\[q_2 = \\frac{u_2}{\\|u_2\\|} = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\]So the orthonormal basis is:\\[q_1 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad q_2 = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\]Applications QR Decomposition: Gram-Schmidt is used to compute orthogonal matrix \\(Q\\) in QR. PCA and SVD: Constructs orthonormal eigenvectors or singular vectors. Feature Embeddings: Ensures orthogonality between embedding dimensions for decorrelation.3. Orthogonal Initialization in Neural NetworksMotivationDeep networks are sensitive to weight initialization. Poorly chosen initializations can lead to: Vanishing or exploding gradients, Poor convergence, Suboptimal generalization.Orthogonal initialization addresses these issues by ensuring that: The weight matrix \\(W\\) preserves the norm of the signal during forward and backward passes, Gradients are not distorted across layers.Mathematical PrincipleLet \\(W \\in \\mathbb{R}^{n \\times n}\\) be an orthogonal matrix, so that:\\[W^T W = WW^T = I\\]If input \\(x \\in \\mathbb{R}^n\\) is passed through such a weight matrix:\\[y = Wx \\Rightarrow \\|y\\|_2 = \\|x\\|_2\\]Hence, the norm is preserved, avoiding scaling issues layer by layer.In backpropagation, the gradient flow also remains stable:\\[\\frac{\\partial L}{\\partial x} = W^T \\frac{\\partial L}{\\partial y}\\]If \\(W\\) is orthogonal, the gradient is rotated but not scaled, avoiding vanishing/exploding gradients.How to Initialize Orthogonal Matrices Generate a random matrix \\(A\\), Perform QR decomposition: \\(A = QR\\), Use \\(Q\\) as the initialization matrix (optionally scale by a factor \\(\\sigma\\)):\\[W = \\sigma Q\\]Implementing in Deep Learning LibrariesIn PyTorch:import torch.nn as nnnn.init.orthogonal_(tensor, gain=1.0)In TensorFlow:initializer = tf.keras.initializers.Orthogonal(gain=1.0)Applications Recurrent Neural Networks (RNNs): Orthogonal or unitary weight matrices are crucial for long-term memory retention. Deep Fully Connected Networks: Improves training dynamics for very deep MLPs. Transformer Layers: Can help in initializing dense layers to preserve signal variance.Orthogonalization is not only a theoretical concept but also a practical tool that: Enhances numerical stability, Helps decorrelate features, Enables better gradient flow in deep neural networks.Whether through Gram-Schmidt orthogonalization for structured bases or orthogonal initialization for training deep networks, mastering these tools improves both understanding and implementation of modern ML models.Kernel Methods and Hilbert Spaces in Machine LearningKernel methods are powerful tools that enable learning algorithms to operate in high-dimensional feature spaces without explicitly computing those spaces. They are foundational to algorithms like Support Vector Machines (SVMs), Gaussian Processes (GPs), and Kernel PCA. This is achieved through the kernel trick and formalized via the theory of Hilbert spaces and Mercer’s theorem.In this section, we will cover: Inner products in high-dimensional (possibly infinite-dimensional) spaces, The concept of Reproducing Kernel Hilbert Spaces (RKHS), The kernel trick and its use in ML algorithms, Mercer’s theorem and the mathematical foundation of kernels.1. Inner Products in High-Dimensional SpacesMotivationIn many ML algorithms, we compute inner products between feature vectors:\\[\\langle x, x' \\rangle = x^\\top x'\\]However, linear models using this inner product are limited to linear decision boundaries.To handle non-linear patterns, we can map the inputs into a higher-dimensional space:\\[\\phi: \\mathcal{X} \\rightarrow \\mathcal{H}, \\quad x \\mapsto \\phi(x)\\]where \\(\\mathcal{H}\\) is a Hilbert space (a complete inner product space). Then, inner products become:\\[\\langle \\phi(x), \\phi(x') \\rangle\\]If \\(\\phi\\) maps into a very high-dimensional or infinite-dimensional space, computation becomes infeasible. However, we can avoid this cost using kernels.2. Kernel Functions and the Kernel TrickDefinitionA kernel function \\(k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}\\) is defined as:\\[k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle_{\\mathcal{H}}\\]This allows computing the inner product without explicitly computing \\(\\phi(x)\\) or \\(\\phi(x')\\).This is known as the kernel trick.Examples of Kernel Functions Linear kernel:\\[k(x, x') = x^\\top x'\\] Polynomial kernel (degree \\(d\\)):\\[k(x, x') = (x^\\top x' + c)^d\\] Radial Basis Function (RBF) / Gaussian kernel:\\[k(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\sigma^2}\\right)\\] Sigmoid kernel:\\[k(x, x') = \\tanh(\\alpha x^\\top x' + c)\\]Why It WorksMany algorithms (like SVMs) rely on computing dot products between data points: Training: \\(x_i^\\top x_j\\) Prediction: \\(x^\\top x_i\\)By replacing these with \\(k(x_i, x_j)\\) and \\(k(x, x_i)\\), we effectively operate in the high-dimensional space without ever computing it explicitly.This is computationally and memory-efficient and allows fitting complex, non-linear decision boundaries.3. Reproducing Kernel Hilbert Space (RKHS)DefinitionA Reproducing Kernel Hilbert Space is a Hilbert space \\(\\mathcal{H}\\) of functions from \\(\\mathcal{X} \\rightarrow \\mathbb{R}\\) such that: For all \\(x \\in \\mathcal{X}\\), the evaluation functional \\(f \\mapsto f(x)\\) is continuous. There exists a kernel function \\(k(x, \\cdot) \\in \\mathcal{H}\\) such that for all \\(f \\in \\mathcal{H}\\):\\[f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}}\\]This is called the reproducing property, and it guarantees that the kernel uniquely defines the Hilbert space.IntuitionRKHS is the space induced by the kernel. Every function in the RKHS can be written in terms of kernels evaluated at training points. This leads to kernel representer theorems and simplifies optimization.4. Mercer’s TheoremStatementMercer’s Theorem provides a condition under which a function is a valid kernel (i.e., corresponds to an inner product in some Hilbert space).Let \\(k(x, x')\\) be a continuous, symmetric, and positive semi-definite kernel on a compact domain \\(\\mathcal{X} \\subset \\mathbb{R}^n\\). Then: There exists a sequence of orthonormal eigenfunctions \\(\\{\\phi_i\\}\\) and non-negative eigenvalues \\(\\{\\lambda_i\\}\\) such that:\\[k(x, x') = \\sum_{i=1}^{\\infty} \\lambda_i \\phi_i(x) \\phi_i(x')\\]This shows that a kernel corresponds to an inner product in a (possibly infinite-dimensional) feature space.Practical UseMercer’s Theorem justifies that functions like the RBF or polynomial kernel are valid kernels and thus induce a real Hilbert space with meaningful inner products.5. Applications in Machine Learning5.1 Support Vector Machines (SVMs)The dual form of the SVM optimization problem involves only dot products. With a kernel function:\\[\\text{maximize}_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j k(x_i, x_j)\\]Predictions are made using:\\[f(x) = \\sum_i \\alpha_i y_i k(x, x_i) + b\\]This enables non-linear classification by implicitly mapping into a high-dimensional space.5.2 Gaussian Processes (GPs)A Gaussian Process is a distribution over functions:\\[f(x) \\sim \\mathcal{GP}(0, k(x, x'))\\]The kernel function \\(k\\) defines the covariance between function values. This allows modeling smoothness, periodicity, and other function properties.Prediction involves computing:\\[\\mathbb{E}[f(x_*)] = k(x_*, X) [K(X, X) + \\sigma^2 I]^{-1} y\\]Where: \\(K(X, X)\\) is the Gram matrix over the training data using kernel \\(k\\), \\(x_*\\) is the test input.5.3 Kernel PCAStandard PCA uses the covariance matrix:\\[C = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T\\]Kernel PCA generalizes this using the kernel trick by computing the kernel Gram matrix:\\[K_{ij} = k(x_i, x_j)\\]Then perform eigen-decomposition on the centered kernel matrix:\\[K_c = K - \\mathbf{1}K - K\\mathbf{1} + \\mathbf{1}K\\mathbf{1}\\]Where \\(\\mathbf{1}\\) is the centering matrix.The result is non-linear dimensionality reduction using kernel-defined similarities.Summary Concept Mathematical Foundation Role in Machine Learning Feature Mapping \\(\\phi: x \\mapsto \\mathcal{H}\\) Transforms data into high-dimensional space Kernel Function \\(k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle\\) Computes inner products without explicit mapping RKHS Function space induced by a valid kernel Guarantees expressiveness and optimization theory Mercer’s Theorem \\(k(x, x') = \\sum \\lambda_i \\phi_i(x) \\phi_i(x')\\) Validates kernel as inner product in Hilbert space Kernel Trick Replace \\(\\langle x, x' \\rangle\\) with \\(k(x, x')\\) Enables non-linear learning with linear algorithms Kernel methods let us apply powerful linear algorithms in high-dimensional non-linear spaces—without computing those spaces directly. The kernel trick, rooted in Hilbert space theory and Mercer’s theorem, forms the backbone of some of the most elegant and effective machine learning algorithms.Sparse Matrices and Sparsity in Machine LearningModern machine learning applications often involve high-dimensional data, where most entries are zero. In such settings, sparsity becomes a powerful property to exploit—both for computational efficiency and for improving generalization.This section explores: Sparse matrix representation and storage, The role of sparsity in large-scale ML problems, Compressed sensing and sparse coding, Applications in recommender systems, NLP, computer vision, and feature selection.1. Sparse RepresentationsDefinitionA matrix \\(A \\in \\mathbb{R}^{m \\times n}\\) is called sparse if most of its entries are zero:\\[\\text{Sparsity}(A) = \\frac{\\text{Number of zero entries}}{mn} \\gg 0\\]Equivalently, it has only a few non-zero entries compared to its total size.Storage and EfficiencyInstead of storing all \\(mn\\) entries, we store only the non-zero elements and their indices. Common sparse matrix formats include: Compressed Sparse Row (CSR):Stores non-zero values, column indices, and row pointer. Compressed Sparse Column (CSC):Similar to CSR, but column-wise. Coordinate (COO):Stores each non-zero entry as a tuple \\((i, j, A_{ij})\\). These formats enable: O(1) access to non-zero elements, Efficient matrix-vector multiplication in \\(O(\\text{nnz})\\) time, Reduced memory usage.Use Cases Recommender Systems:User-item interaction matrices are sparse; most users rate very few items. NLP:Bag-of-Words, TF-IDF, one-hot encoding—all result in sparse representations. Graph ML:Adjacency matrices of large graphs (social networks, web graphs) are sparse. 2. Compressed SensingMotivationCompressed sensing addresses the question:Can we recover high-dimensional signals from few linear measurements if the signal is sparse?The answer is yes—under specific conditions.Problem SetupLet: \\(x \\in \\mathbb{R}^n\\) be the original signal, sparse in some basis. \\(y \\in \\mathbb{R}^m\\) be the observed measurements, where \\(m \\ll n\\). \\(A \\in \\mathbb{R}^{m \\times n}\\) be a measurement matrix.We want to solve:\\[y = A x \\quad \\text{with} \\quad x \\text{ sparse}\\]Since \\(m \u0026lt; n\\), this is underdetermined. But if \\(x\\) is sparse (i.e., \\(\\|x\\|_0 \\ll n\\)), recovery is possible.Recovery via OptimizationThe sparse recovery problem is posed as:\\[\\min_x \\|x\\|_0 \\quad \\text{subject to} \\quad y = Ax\\]But this is NP-hard. Instead, we solve:\\[\\min_x \\|x\\|_1 \\quad \\text{subject to} \\quad y = Ax\\]This is known as Basis Pursuit, and under the Restricted Isometry Property (RIP), it provably recovers the sparse signal.Lasso (Relaxed version)If measurements are noisy:\\[y = Ax + \\varepsilon\\]Then we solve:\\[\\min_x \\|y - Ax\\|_2^2 + \\lambda \\|x\\|_1\\]This is the Lasso (Least Absolute Shrinkage and Selection Operator) formulation.Applications Medical imaging: MRI and CT scan reconstruction from fewer samples. Signal processing: Denoising, compression. NLP and CV: Learning compact word/image representations.3. Sparse CodingDefinitionGiven an input signal \\(x \\in \\mathbb{R}^d\\), sparse coding assumes:\\[x \\approx D h\\]Where: \\(D \\in \\mathbb{R}^{d \\times k}\\) is a dictionary of basis vectors (atoms), \\(h \\in \\mathbb{R}^k\\) is a sparse code (i.e., few non-zero entries).The goal is to learn \\(D\\) and \\(h\\) such that \\(x\\) is well-represented with a sparse \\(h\\).Optimization ObjectiveGiven training data \\(X = [x^{(1)}, \\ldots, x^{(n)}]\\), we solve:\\[\\min_{D, H} \\sum_{i=1}^n \\left( \\|x^{(i)} - D h^{(i)}\\|_2^2 + \\lambda \\|h^{(i)}\\|_1 \\right)\\]Subject to normalization constraints on \\(D\\) (e.g., \\(\\|d_j\\|_2 \\leq 1\\)).This problem is bi-convex and typically solved via alternating minimization: Fix \\(D\\), update sparse codes \\(H\\). Fix \\(H\\), update dictionary \\(D\\).Interpretation Sparse coding learns overcomplete dictionaries that can represent inputs efficiently. The sparse codes \\(h\\) capture salient features with fewer active components.Applications Image processing: Denoising, super-resolution, texture synthesis. NLP: Sparse embedding representations. Feature learning: Unsupervised pretraining for deep networks. Compression: Reduces storage and computation for large models. Summary Concept Mathematical Description Applications Sparse Matrix Mostly zero entries, stored in CSR/COO format Recommender systems, NLP, graph ML Compressed Sensing \\(y = Ax, |x|_0 \\ll n\\), recover \\(x\\) from \\(y\\) Imaging, signal processing, low-data regimes Lasso \\(\\min_x |y - Ax|_2^2 + \\lambda |x|_1\\) Feature selection, regularization Sparse Coding \\(x \\approx D h\\) with sparse \\(h\\) Feature learning, representation compression Sparsity is a crucial structural assumption in many ML settings. Whether it’s handling massive sparse data structures, recovering signals from limited observations, or learning compressed representations, sparsity enables scalable and interpretable learning.The underlying mathematical principles—\\(\\ell_0\\) and \\(\\ell_1\\) norms, underdetermined systems, and structured optimization—form the basis of compressed sensing and sparse coding.Numerical Stability and Conditioning in Machine LearningIn large-scale machine learning models and numerical computations, numerical stability plays a vital role in ensuring accuracy, convergence, and generalization. When data is high-dimensional, features are correlated, or optimization problems are ill-posed, small numerical errors can lead to large deviations in results.This section explores: Matrix conditioning and condition numbers, Their effect on optimization and linear models, Regularization strategies like Tikhonov regularization and Ridge regression, Real-world applications after each theoretical concept.1. Matrix Conditioning and Condition NumbersDefinitionGiven a matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), its condition number in the 2-norm is:\\[\\kappa(A) = \\|A\\|_2 \\cdot \\|A^{-1}\\|_2\\]If \\(A\\) is symmetric positive definite:\\[\\kappa(A) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}\\]Where \\(\\lambda_{\\max}\\) and \\(\\lambda_{\\min}\\) are the largest and smallest eigenvalues of \\(A\\).This value measures the sensitivity of the solution \\(x\\) to small perturbations in the system \\(Ax = b\\).Application: Linear Systems and Inversion StabilityIn linear regression or least squares, solving:\\[\\hat{x} = (X^T X)^{-1} X^T y\\]can be unstable if \\(X^T X\\) is ill-conditioned (e.g., due to multicollinearity).Real-World Scenarios: High-dimensional regression models, Polynomial regression (where powers of features become highly correlated), PCA: covariance matrix conditioning affects eigenvalue computation.2. Instability in OptimizationGradient Descent SensitivityConsider optimizing a quadratic loss:\\[f(x) = \\frac{1}{2} x^T A x\\]Gradient descent update:\\[x_{k+1} = x_k - \\eta A x_k\\]If \\(A\\) is ill-conditioned (i.e., eigenvalues vary widely), the optimization will: Converge slowly, Zigzag across the cost surface, Require small learning rates to remain stable.Application: Neural Network TrainingIn deep networks, layers may learn at drastically different rates due to ill-conditioning. Common symptoms include: Exploding or vanishing gradients, Slow convergence even on simple tasks, Difficulty tuning learning rates.Example: In early training of deep MLPs or RNNs, poor weight scaling leads to ill-conditioned Jacobians and Hessians.3. Tikhonov RegularizationTheoryTo solve an ill-posed least squares problem:\\[\\min_x \\|Ax - b\\|_2^2\\]we add a regularization term:\\[\\min_x \\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2\\]This is Tikhonov regularization. The solution becomes:\\[\\hat{x}_\\lambda = (A^T A + \\lambda I)^{-1} A^T b\\]This improves conditioning by ensuring the matrix being inverted is better-behaved.Application: Ill-posed Inverse ProblemsTikhonov regularization is used in: Image deblurring and denoising, Medical imaging (MRI, CT), Physics-based simulations with uncertain measurements.In ML, it improves: Matrix inversion stability in linear models, Numerical robustness in batch/mini-batch computations.4. Ridge RegressionTheoryRidge regression is a specific case of Tikhonov regularization applied to linear regression. Given \\(X \\in \\mathbb{R}^{n \\times d}\\) and \\(y \\in \\mathbb{R}^n\\):\\[\\min_w \\|Xw - y\\|_2^2 + \\lambda \\|w\\|_2^2\\]Solution:\\[w = (X^T X + \\lambda I)^{-1} X^T y\\]Benefits: Stabilizes matrix inversion, Reduces overfitting in high-dimensional settings.Application: High-Dimensional Linear ModelsRidge regression is essential when: The number of features exceeds the number of samples, Features are highly correlated (multicollinearity), Predictors are noisy or redundant.Example:from sklearn.linear_model import Ridgemodel = Ridge(alpha=1.0)model.fit(X, y)Common in: Genomics (p » n), Text regression models with bag-of-words features, Financial models with redundant indicators.5. Conditioning in Deep LearningCommon ProblemsIn deep neural networks: Weight matrices may become poorly conditioned, Gradients may vanish or explode during backpropagation, Activations may saturate, leading to optimization stalls.Solutions and Their Mathematical Roles Orthogonal Initialization: Weight matrices initialized to be orthogonal preserve input norm and maintain conditioning.\\[W^T W = I \\Rightarrow \\|Wx\\|_2 = \\|x\\|_2\\] Code: torch.nn.init.orthogonal_(tensor) Weight Decay (L2 regularization): Equivalent to Ridge on weights. Controls weight growth, stabilizes learning.\\[\\min_w \\mathcal{L}(w) + \\lambda \\|w\\|_2^2\\] Gradient Clipping: Prevents gradient explosion by clipping:\\[\\nabla \\mathcal{L} \\leftarrow \\frac{\\nabla \\mathcal{L}}{\\max(1, \\|\\nabla \\mathcal{L}\\| / \\tau)}\\] Code: torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0) Application: Training Stability in Deep NetworksThese techniques are used in: RNNs: To avoid exploding gradients through time. Transformers: LayerNorm and initialization stabilize long-depth training. CNNs: Weight decay improves generalization and convergence.Summary Concept Mathematical Description Role in ML / DL Condition Number \\(\\kappa(A) = |A| \\cdot |A^{-1}|\\) Measures sensitivity to noise Ill-conditioning \\(\\kappa(A) \\gg 1\\) Leads to instability in training and optimization Tikhonov Regularization \\(\\min_x |Ax - b|^2 + \\lambda |x|^2\\) Improves matrix invertibility Ridge Regression \\(w = (X^T X + \\lambda I)^{-1} X^T y\\) Stabilizes regression with correlated features Orthogonal Init. \\(W^T W = I\\) Preserves norm in forward/backward pass Weight Decay \\(\\min_w \\mathcal{L} + \\lambda |w|^2\\) Regularizes weights and enhances generalization Gradient Clipping \\(\\nabla \\mathcal{L} \\rightarrow \\frac{\\nabla \\mathcal{L}}{\\max(\\cdot)}\\) Prevents exploding gradients during training Understanding numerical stability and matrix conditioning helps you: Design models that train efficiently, Use optimization methods that converge reliably, Avoid silent failures due to ill-conditioning.By incorporating regularization, better initialization, and gradient control, you can ensure your machine learning models are not only performant—but also numerically robust.Rotation, Reflection, and Markov Matrices in Machine LearningGeometric and probabilistic transformations form the backbone of many machine learning systems—especially in computer vision, robotics, and probabilistic reasoning. This section explores two foundational categories of matrices: Rotation and Reflection matrices (used in spatial transformations), Markov (Stochastic) matrices (used in probabilistic models and temporal systems).Rotation and Reflection MatricesGeometric transformations such as rotation and reflection are represented by orthogonal matrices in linear algebra. These are crucial for tasks in computer vision, robotics, 3D graphics, and data augmentation.Rotation MatricesA rotation matrix in \\(\\mathbb{R}^n\\) rotates a vector about the origin while preserving its norm. A matrix \\(R\\) is a rotation matrix if:\\[R^T R = RR^T = I \\quad \\text{and} \\quad \\det(R) = 1\\]2D RotationIn two dimensions, rotation by an angle \\(\\theta\\) counterclockwise is given by:\\[R(\\theta) = \\begin{bmatrix}\\cos\\theta \u0026amp; -\\sin\\theta \\\\\\sin\\theta \u0026amp; \\cos\\theta\\end{bmatrix}\\]For any vector \\(x = \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\\), the rotated vector is:\\[x' = R(\\theta) x\\]3D Rotation and Euler AnglesIn 3D, rotations can be performed around each axis. The elementary rotations are: Around \\(x\\)-axis:\\(R_x(\\theta) = \\begin{bmatrix}1 \u0026amp; 0 \u0026amp; 0 \\\\0 \u0026amp; \\cos\\theta \u0026amp; -\\sin\\theta \\\\0 \u0026amp; \\sin\\theta \u0026amp; \\cos\\theta\\end{bmatrix}\\) Around \\(y\\)-axis:\\(R_y(\\theta) = \\begin{bmatrix}\\cos\\theta \u0026amp; 0 \u0026amp; \\sin\\theta \\\\0 \u0026amp; 1 \u0026amp; 0 \\\\-\\sin\\theta \u0026amp; 0 \u0026amp; \\cos\\theta\\end{bmatrix}\\) Around \\(z\\)-axis:\\(R_z(\\theta) = \\begin{bmatrix}\\cos\\theta \u0026amp; -\\sin\\theta \u0026amp; 0 \\\\\\sin\\theta \u0026amp; \\cos\\theta \u0026amp; 0 \\\\0 \u0026amp; 0 \u0026amp; 1\\end{bmatrix}\\) Any 3D rotation can be represented by a combination of these, often using Euler angles.Application: Computer Vision and Robotics Image Augmentation: Rotating images during training increases robustness to orientation. Pose Estimation: Estimating camera or robot orientation using rotation matrices. 3D Reconstruction: Applying transformations to point clouds and mesh data. Robotics Control: Planning and executing movement using rotation matrices in kinematics.Example:import torchvision.transforms as TT.RandomRotation(degrees=15)Reflection MatricesA reflection matrix flips a vector across a subspace (hyperplane). It is also orthogonal, but unlike a rotation matrix:\\[\\det(R) = -1\\]Reflection in 2DReflection across the \\(x\\)-axis:\\[R = \\begin{bmatrix}1 \u0026amp; 0 \\\\0 \u0026amp; -1\\end{bmatrix}\\]Reflection across an arbitrary line through the origin with unit normal vector \\(n\\):\\[R = I - 2nn^T\\]Where \\(n \\in \\mathbb{R}^d\\) and \\(\\|n\\| = 1\\).Application: Data Augmentation in CVReflections are used to simulate different perspectives of the same object. Horizontal Flip: T.RandomHorizontalFlip(p=0.5) Vertical Flip: T.RandomVerticalFlip(p=0.5) Symmetry-based Learning: Useful in object detection and scene understanding.Markov Matrices and Stochastic ProcessesMarkov models describe systems that evolve probabilistically over time, where the future depends only on the current state. Their underlying structure is captured by stochastic (Markov) matrices.Stochastic MatricesA Markov matrix (or row-stochastic matrix) is a square matrix \\(P \\in \\mathbb{R}^{n \\times n}\\) where: \\(P_{ij} \\geq 0\\) for all \\(i, j\\), \\(\\sum_{j=1}^n P_{ij} = 1\\) for all \\(i\\) (each row sums to 1).This means \\(P_{ij}\\) is the probability of transitioning from state \\(i\\) to state \\(j\\).Let \\(x_t\\) be a probability distribution vector at time \\(t\\). Then:\\[x_{t+1} = x_t P\\]This recurrence describes the evolution of a Markov chain over time.Stationary DistributionA distribution \\(\\pi\\) is stationary if:\\[\\pi = \\pi P\\]This represents the long-term distribution of states. Under mild conditions (irreducibility, aperiodicity), every Markov chain has a unique stationary distribution.Application: PageRank and Random WalksPageRank Algorithm: Models the web as a Markov chain. Pages are states, links are transitions. Uses a stochastic matrix with damping:\\[P' = \\alpha P + (1 - \\alpha) \\frac{1}{n} \\mathbf{1}\\mathbf{1}^T\\]Stationary distribution \\(\\pi\\) is computed such that:\\[\\pi = \\pi P'\\]Other Applications: Language Modeling: Character or word-level Markov chains. Hidden Markov Models (HMMs): NLP, speech, time-series. Graph algorithms: Random walk-based node ranking and clustering.Summary Concept Mathematical Description Applications Rotation Matrix \\(R^T R = I, \\ \\det(R) = 1\\) CV, robotics, 3D vision, pose estimation Reflection Matrix \\(R = I - 2nn^T, \\ \\det(R) = -1\\) Data augmentation, symmetry modeling Euler Angles Composition of axis-wise rotations Robotics, camera modeling Stochastic Matrix \\(P_{ij} \\geq 0, \\ \\sum_j P_{ij} = 1\\) Markov chains, PageRank, probabilistic models Stationary Distribution \\(\\pi = \\pi P\\) Long-term behavior modeling Rotation and reflection matrices form the mathematical backbone of spatial transformations in ML applications like vision and robotics. Meanwhile, Markov matrices provide a probabilistic framework to model temporal evolution in tasks such as language modeling, search ranking, and sequence prediction.Advanced Projections: Random Projections and the Johnson–Lindenstrauss LemmaIn modern machine learning and data science, high-dimensional datasets are common—particularly in fields like natural language processing, image processing, and information retrieval. However, high dimensionality brings computational and storage challenges, as well as the curse of dimensionality.One elegant solution to this is random projection, a method for dimensionality reduction that is fast, scalable, and surprisingly effective. Its theoretical foundation is the Johnson–Lindenstrauss Lemma, which guarantees that random projections approximately preserve distances between points.This section explains: The theory and math behind random projections, The Johnson–Lindenstrauss Lemma and its guarantees, Applications in NLP, IR, privacy, and beyond.1. The Need for Dimensionality ReductionLet \\(X \\in \\mathbb{R}^{n \\times d}\\) be a dataset with \\(n\\) samples in a high-dimensional space \\(\\mathbb{R}^d\\).Problems with large \\(d\\): Computational inefficiency: Matrix operations are expensive. Memory consumption: Storing all features is costly. Overfitting: Too many features relative to data points. Distance concentration: In high dimensions, pairwise distances become less informative.Goal: Reduce the dimensionality from \\(d\\) to \\(k \\ll d\\) such that geometric structure (e.g., pairwise distances) is preserved.2. Random ProjectionsInstead of learning an optimal projection (like PCA), random projections use a random linear map:Let \\(R \\in \\mathbb{R}^{k \\times d}\\) be a random matrix. Then:\\[z_i = \\frac{1}{\\sqrt{k}} R x_i \\in \\mathbb{R}^k\\]for each data point \\(x_i \\in \\mathbb{R}^d\\).The random matrix \\(R\\) typically has entries sampled from: Standard Gaussian: \\(R_{ij} \\sim \\mathcal{N}(0, 1)\\), Sparse sign matrices: \\(R_{ij} \\in \\{-1, 0, +1\\}\\) with controlled sparsity.3. Johnson–Lindenstrauss LemmaThe Johnson–Lindenstrauss Lemma states that a small set of points in high-dimensional space can be mapped into a lower-dimensional space such that pairwise distances are approximately preserved.Theorem (JL Lemma)For any \\(0 \u0026lt; \\epsilon \u0026lt; 1\\) and integer \\(n\\), let \\(X = \\{x_1, \\ldots, x_n\\} \\subset \\mathbb{R}^d\\) be a set of \\(n\\) points. Then for:\\[k = O\\left(\\frac{\\log n}{\\epsilon^2}\\right),\\]there exists a linear map \\(f: \\mathbb{R}^d \\rightarrow \\mathbb{R}^k\\) such that for all \\(i, j\\):\\[(1 - \\epsilon)\\|x_i - x_j\\|_2^2 \\leq \\|f(x_i) - f(x_j)\\|_2^2 \\leq (1 + \\epsilon)\\|x_i - x_j\\|_2^2\\]This means that random projections preserve pairwise distances up to small distortion with high probability.Intuition The JL Lemma shows that no information-theoretic bottleneck exists when compressing data from \\(\\mathbb{R}^d\\) to \\(\\mathbb{R}^k\\), as long as \\(k = O(\\log n)\\). This is data-independent: No need to look at the data when designing the projection.4. Construction of Projection MatrixLet \\(R \\in \\mathbb{R}^{k \\times d}\\) be the random matrix. Some common constructions:4.1 Gaussian Random ProjectionEach entry is drawn i.i.d. from:\\[R_{ij} \\sim \\mathcal{N}(0, 1)\\]Then the projection is:\\[f(x) = \\frac{1}{\\sqrt{k}} R x\\]This satisfies the JL lemma with high probability.4.2 Sparse Random ProjectionTo improve speed and memory:\\[R_{ij} = \\sqrt{s} \\cdot\\begin{cases}+1 \u0026amp; \\text{with probability } \\frac{1}{2s}, \\\\0 \u0026amp; \\text{with probability } 1 - \\frac{1}{s}, \\\\-1 \u0026amp; \\text{with probability } \\frac{1}{2s}\\end{cases}\\]For example, \\(s = 3\\) gives ~67% sparsity.5. Applications5.1 Natural Language Processing (NLP) TF-IDF vectors for documents can have tens of thousands of dimensions. Random projections reduce dimensionality for: Document classification, Similarity search, Topic modeling pre-processing. Example:from sklearn.random_projection import GaussianRandomProjectiontransformer = GaussianRandomProjection(n_components=100)X_new = transformer.fit_transform(X_tfidf)5.2 Information Retrieval and ANN Search Used to index high-dimensional vectors for approximate nearest neighbors (ANN). Efficiently reduce the dimension of image embeddings, word embeddings, etc. Compatible with LSH (Locality-Sensitive Hashing).5.3 Differential Privacy and Data Privacy Random projections are used to obscure sensitive dimensions while preserving utility. Also appear in private matrix factorization and federated learning pipelines.5.4 Kernel Approximation Random Fourier Features approximate Gaussian/RBF kernels by projecting into a low-dimensional space. Scales kernel methods to large datasets:\\(k(x, y) \\approx \\phi(x)^T \\phi(y)\\)Where \\(\\phi(x)\\) is obtained via random projections.6. Comparison with PCA Aspect PCA Random Projection Data-dependent Yes No Computational cost High (SVD-based) Low (matrix multiplication) Distance preservation Optimal for top-variance directions Approximate, but probabilistically guaranteed Scalability Less scalable for large \\(d\\) Highly scalable Interpretability High (axes = principal components) Low Summary Concept Mathematical Idea Application Domains Johnson–Lindenstrauss Lemma \\(k = O\\left(\\frac{\\log n}{\\epsilon^2}\\right)\\) Distance-preserving low-dim embedding Gaussian Projection \\(R_{ij} \\sim \\mathcal{N}(0, 1)\\) NLP, embeddings, privacy Sparse Projection \\(R_{ij} \\in \\{-1, 0, +1\\}\\) with sparsity Faster computation Random Fourier Features Approx. kernel projection via random bases Kernel methods on large datasets Distance Preservation \\(|x_i - x_j| \\approx |f(x_i) - f(x_j)|\\) ANN, clustering, manifold learning Random projections offer a principled, efficient, and theoretically sound method to compress high-dimensional data while preserving its geometric structure. Thanks to the Johnson–Lindenstrauss Lemma, we can apply these projections without worrying about distortion—making them perfect for large-scale ML systems.Whether you’re dealing with: Large vocabulary document matrices in NLP, Embedding vectors in image retrieval, Kernel methods in high dimensions,random projections are a tool you should definitely have in your toolbox."},{"title":"Linear Algebra Basics for ML - Vector Spaces and Transformations","url":"/blog/2022/mathforml-linalg5/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"February 07, 2022","content":"The Need for Vector Spaces and TransformationsWhen we work with machine learning and data science, we often deal with data in various forms, such as images, text, or numerical tables. The real challenge is to represent and manipulate this data efficiently. This is where the concept of vector spaces and transformations comes into play. In simple terms, vector spaces allow us to represent data, and transformations help us modify or map this data to different representations.In machine learning, data is typically represented as vectors in a vector space, and various algorithms manipulate these vectors to extract features, perform operations, or make predictions. Linear transformations help us understand how the data changes when we move it between different coordinate systems. Affine transformations go a step further by adding translations, which are particularly useful in fields like computer vision, where we often deal with translations and rotations of images.In this blog post, we will explore vector spaces and transformations in detail, linking them to feature engineering and model performance improvements. Let’s start by understanding vector spaces and linear transformations.Vector Spaces: The Foundation of Data RepresentationWhat is a Vector Space?A vector space (or linear space) is a collection of vectors that can be added together and multiplied by scalars, subject to certain rules. Mathematically, a vector space \\(V\\) over a field \\(F\\) is a set of objects (called vectors), along with two operations: vector addition and scalar multiplication. These operations must satisfy the following axioms: Commutativity of addition:\\(\\mathbf{v} + \\mathbf{w} = \\mathbf{w} + \\mathbf{v}\\) Associativity of addition:\\((\\mathbf{v} + \\mathbf{w}) + \\mathbf{u} = \\mathbf{v} + (\\mathbf{w} + \\mathbf{u})\\) Additive identity: There exists a zero vector \\(\\mathbf{0}\\) such that\\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\) Additive inverse: For every vector \\(\\mathbf{v}\\), there exists \\(-\\mathbf{v}\\) such that\\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\) Distributive properties of scalar multiplication:\\(a(\\mathbf{v} + \\mathbf{w}) = a\\mathbf{v} + a\\mathbf{w}\\)\\((a + b)\\mathbf{v} = a\\mathbf{v} + b\\mathbf{v}\\) Multiplicative identity of scalars:\\(1 \\cdot \\mathbf{v} = \\mathbf{v}\\) Compatibility of scalar multiplication:\\(a(b\\mathbf{v}) = (ab)\\mathbf{v}\\) In machine learning, we often think of a vector space as the space in which the data exists. For example, if you have a dataset with \\(n\\) features, each data point is a vector in \\(n\\)-dimensional space. The entire dataset is a collection of vectors that form a subspace of that \\(n\\)-dimensional space.Example: Vector Space in Feature EngineeringLet’s say we are working with a dataset containing two features, such as height and weight of individuals. These features can be represented as vectors in a 2-dimensional vector space, where each point corresponds to a person’s height and weight. The feature vectors in this space can be manipulated, transformed, and analyzed for machine learning tasks like clustering, regression, and classification.Motivation Behind Linear TransformationsA linear transformation is a function that maps a vector in one vector space to a vector in another vector space, preserving the operations of vector addition and scalar multiplication. This is extremely important in machine learning, as many algorithms involve transforming data into new spaces to extract more useful features or to make it easier for the model to learn.For example, in Principal Component Analysis (PCA), we perform a linear transformation that projects high-dimensional data onto a lower-dimensional subspace, allowing us to visualize or analyze the data more effectively. The key point is that linear transformations preserve the structure of the data in a way that simplifies operations like regression or classification.Linear Transformations and Change of BasisWhat is a Change of Basis?In machine learning, we may need to switch between different coordinate systems, depending on how we want to represent our data. This is where the concept of a change of basis comes in. When we apply a linear transformation to a vector, we can represent it in a new basis (a new set of basis vectors) that might be more convenient for our analysis.For example, imagine you have a 2D dataset with features height and weight. These features are represented in the standard basis of the 2D plane, i.e., the x-axis and y-axis. However, you may want to rotate the data such that the axes align with the directions of maximum variance. This is a change of basis, where we apply a linear transformation to rotate the data.Mathematically, if \\(A\\) is the transformation matrix and \\(\\mathbf{x}\\) is a vector in the original basis, then the transformed vector \\(\\mathbf{x'}\\) in the new basis is given by:\\(\\mathbf{x'} = A \\mathbf{x}\\)Practical Example: Change of Basis in PCAConsider PCA, where we perform an eigen-decomposition of the covariance matrix to find the principal components. These principal components form a new basis, and we can transform the data into this new basis by applying the linear transformation defined by the eigenvectors of the covariance matrix. The new dataset will have the same data points but represented in terms of the directions of maximum variance (the principal components).This transformation often helps in reducing the dimensionality of the data while retaining the most important features. In essence, PCA is a linear transformation that changes the basis from the original feature space to the space of principal components.Affine Transformations: Beyond Linear MappingsWhat is an Affine Transformation?An affine transformation is a linear transformation followed by a translation. This means that affine transformations include not just rotations, scaling, and shearing, but also shifts in space. Mathematically, an affine transformation can be represented as:\\(\\mathbf{x'} = A \\mathbf{x} + \\mathbf{b}\\)where \\(A\\) is the linear transformation matrix, \\(\\mathbf{x}\\) is the original vector, \\(\\mathbf{b}\\) is the translation vector, and \\(\\mathbf{x'}\\) is the transformed vector.Affine transformations are extremely useful in image processing and computer vision, where we often need to rotate, scale, or translate images.Example: Image Manipulation with Affine TransformationsIn computer vision, affine transformations are commonly used for tasks such as rotating or resizing images. Consider a scenario where we want to rotate an image by a certain angle or scale it up or down. These operations can be represented as affine transformations.import cv2import numpy as np# Load imageimage = cv2.imread('image.jpg')# Define affine transformation matrix (rotation)angle = 45center = (image.shape[1] // 2, image.shape[0] // 2)rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1)# Apply affine transformation (rotation)rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))# Show the resultcv2.imshow('Rotated Image', rotated_image)cv2.waitKey(0)cv2.destroyAllWindows()In this example, the image is rotated by 45 degrees around its center, and the transformation is applied using an affine matrix.Affine Transformations in Feature EngineeringIn feature engineering, affine transformations can be used to modify data features, for example, by scaling or translating them. This can help improve the performance of machine learning models, especially when the features vary widely in magnitude or range.For instance, we might apply an affine transformation to normalize or standardize data by subtracting the mean (translation) and scaling the data by its standard deviation (scaling). This is a common preprocessing step to improve the performance of algorithms like linear regression or neural networks.Application: Data Transformation in Feature EngineeringWhy Transform Data?In machine learning, feature engineering refers to the process of applying mathematical functions to features to improve model performance. Data transformations, such as scaling, normalizing, and encoding, are common operations used in feature engineering. Scaling: Many machine learning algorithms, such as k-nearest neighbors or gradient descent, perform better when features are on a similar scale. Affine transformations such as min-max scaling or standardization (subtracting the mean and dividing by the standard deviation) are commonly used to scale features. Dimensionality Reduction: Techniques like PCA, as we mentioned earlier, involve linear transformations to reduce the dimensionality of data. By transforming the data into a lower-dimensional space, we retain the most important features while discarding noise or less useful features. Non-linear Transformations: Sometimes, applying non-linear transformations to data, such as log transformations or polynomial features, can help make the data more suitable for machine learning models. Example: Data Scaling with an Affine TransformationConsider a dataset with two features, age and income, that have vastly different scales. Income may range from thousands to millions, while age ranges from 0 to 100. Applying an affine transformation like standardization can help bring both features onto a similar scale: Translate each feature by subtracting the mean. Scale each feature by dividing by its standard deviation.Mathematically, we can apply this affine transformation to each feature:\\(\\mathbf{x'} = \\frac{\\mathbf{x} - \\mu}{\\sigma}\\)where \\(\\mu\\) is the mean and \\(\\sigma\\) is the standard deviation of the feature.In Python, we can perform this transformation using sklearn:from sklearn.preprocessing import StandardScalerimport pandas as pd# Example datasetdata = {'Age': [23, 45, 34, 50, 29], 'Income': [50000, 120000, 85000, 150000, 65000]}df = pd.DataFrame(data)# Apply standardization (affine transformation)scaler = StandardScaler()df_scaled = scaler.fit_transform(df)print(df_scaled)This will standardize both features, bringing them to the same scale, which can help improve the performance of models like linear regression or k-means clustering.In this post, we explored vector spaces, linear transformations, and affine transformations, all of which are foundational concepts in linear algebra that play a critical role in machine learning and data science. From representing data in vector spaces to transforming that data in meaningful ways, these concepts help us better understand and manipulate data, improving our ability to build and refine machine learning models.Whether you’re working with feature engineering, dimensionality reduction, or image transformations, understanding vector spaces and transformations allows you to tackle complex problems and improve your models’ performance.Remember, the next time you’re faced with a complex dataset or need to manipulate features, think about how vector spaces and transformations can help simplify the problem and enhance your machine learning pipeline."},{"title":"Linear Algebra Basics for ML - Eigenvalues, Eigenvectors, and Singular Value Decomposition","url":"/blog/2022/mathforml-linalg4/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"February 03, 2022","content":"Understanding how data transforms under linear mappings is core to machine learning, and eigenvalues and eigenvectors lie at the heart of this. Whether we’re analyzing variance in high-dimensional data or compressing information with minimal loss, these concepts help us simplify and interpret complex systems. In this post, we’ll build intuition, explore the math behind eigen decomposition and diagonalization, and connect the dots to key ML applications.What Makes Eigenvectors Special?When we apply a matrix transformation to a vector, we typically expect its direction to change. But some vectors resist this change—they may get stretched or shrunk, but they stay on the same line. These are the eigenvectors of the transformation.Formally, if \\(A\\) is a square matrix, and \\(\\mathbf{v}\\) is a vector such that:\\[A \\mathbf{v} = \\lambda \\mathbf{v},\\]then \\(\\mathbf{v}\\) is called an eigenvector of \\(A\\) and \\(\\lambda\\) is the corresponding eigenvalue. The transformation scales the vector by \\(\\lambda\\) without rotating it.To visualize this, imagine applying a matrix that stretches space vertically. Vectors aligned with the vertical axis simply stretch—those are eigenvectors with eigenvalues greater than 1. Vectors aligned with the horizontal axis may stay the same length (eigenvalue = 1) or stretch differently. But vectors at diagonal angles typically change direction; they’re not eigenvectors.This geometric idea is powerful in ML: eigenvectors identify the key directions in which data varies, and eigenvalues tell us how much variation lies in each direction.Why Do Eigenvectors Matter in ML?In machine learning, eigen decomposition helps us understand structure and simplify computations. Take Principal Component Analysis (PCA) as an example—it identifies the directions (principal components) along which data varies most. These directions are eigenvectors of the covariance matrix. The corresponding eigenvalues tell us how much variance each direction captures.This insight drives a wide range of applications: Dimensionality reduction: Eliminate less informative directions to compress data. Feature selection: Identify directions with the most meaningful variation. Interpretability: Understand how transformations like matrix multiplications affect data. Stability and convergence: In optimization and iterative methods, the dominant eigenvalue often controls convergence speed.In short, eigenvalues and eigenvectors distill the essence of a transformation.Eigen-Decomposition and DiagonalizationLet’s now formalize this intuition. Given a square matrix \\(A \\in \\mathbb{R}^{n \\times n}\\), an eigen-decomposition expresses it in terms of its eigenvectors and eigenvalues. First, we solve the characteristic equation:\\[\\det(A - \\lambda I) = 0\\]The roots \\(\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\) are the eigenvalues of \\(A\\). For each eigenvalue, we solve:\\[(A - \\lambda I)\\mathbf{v} = 0\\]to find the corresponding eigenvector \\(\\mathbf{v}\\).If \\(A\\) has \\(n\\) linearly independent eigenvectors, we can form a matrix \\(Q\\) whose columns are those eigenvectors. Let \\(\\Lambda\\) be the diagonal matrix of the corresponding eigenvalues:\\[Q = [\\mathbf{v}_1\\ \\mathbf{v}_2\\ \\cdots\\ \\mathbf{v}_n], \\quad \\Lambda = \\begin{bmatrix}\\lambda_1 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\0 \u0026amp; \\lambda_2 \u0026amp; \\cdots \u0026amp; 0 \\\\\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; \\lambda_n\\end{bmatrix}\\]Then the matrix \\(A\\) can be written as:\\[A = Q \\Lambda Q^{-1}\\]This is the diagonalization of \\(A\\). It tells us that in the basis formed by eigenvectors, the transformation \\(A\\) just scales each coordinate by its eigenvalue.For symmetric matrices, the situation is even better: the eigenvectors are orthogonal and \\(Q\\) becomes an orthogonal matrix (i.e., \\(Q^{-1} = Q^T\\)), giving:\\[A = Q \\Lambda Q^T\\]This is known as the spectral theorem, and it holds for any real symmetric matrix—like covariance matrices, Laplacians, and Gram matrices in ML. The eigenvalues are always real, and the eigenvectors form an orthonormal basis.A Concrete Example: Diagonalizing a MatrixLet’s walk through an example. Suppose:\\[A = \\begin{pmatrix}3 \u0026amp; 1 \\\\0 \u0026amp; 2\\end{pmatrix}\\]To find the eigenvalues, solve:\\[\\det(A - \\lambda I) = (3 - \\lambda)(2 - \\lambda) = 0\\]The roots are \\(\\lambda_1 = 3\\) and \\(\\lambda_2 = 2\\).To find eigenvectors: For \\(\\lambda_1 = 3\\), solve \\((A - 3I)\\mathbf{v} = 0\\):\\[\\begin{pmatrix}0 \u0026amp; 1 \\\\0 \u0026amp; -1\\end{pmatrix}\\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix}= 0\\quad \\Rightarrow \\quad v_2 = 0\\]Choose \\(v_1 = 1\\). So one eigenvector is \\(\\mathbf{v}^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\). For \\(\\lambda_2 = 2\\), solve \\((A - 2I)\\mathbf{v} = 0\\):\\[\\begin{pmatrix}1 \u0026amp; 1 \\\\0 \u0026amp; 0\\end{pmatrix}\\begin{pmatrix}v_1 \\\\v_2\\end{pmatrix}= 0\\quad \\Rightarrow \\quad v_2 = -v_1\\]Choose \\(v_1 = 1\\). So \\(\\mathbf{v}^{(2)} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\).Now we build:\\[Q = \\begin{pmatrix}1 \u0026amp; 1 \\\\0 \u0026amp; -1\\end{pmatrix}, \\quad\\Lambda = \\begin{pmatrix}3 \u0026amp; 0 \\\\0 \u0026amp; 2\\end{pmatrix}\\]We can verify that:\\[A = Q \\Lambda Q^{-1}\\]In this eigenbasis, applying \\(A\\) is as simple as scaling by 3 and 2 along the new axes.Why Diagonalization Matters in Machine LearningIn ML, diagonalization is not just a theoretical convenience—it enables faster computation and deeper insight.When matrices are diagonalizable, operations like computing \\(A^k\\), solving \\(A \\mathbf{x} = \\mathbf{b}\\), or evaluating matrix exponentials become much easier. This is vital in graph analysis, dynamical systems, and deep learning, where matrix powers and iterative updates are common.In PCA, the covariance matrix is symmetric and positive semi-definite, so we can always decompose it into eigenvectors and eigenvalues. The eigenvectors give us uncorrelated directions in feature space, and eigenvalues quantify how much variance lies in each direction. Keeping only the top \\(k\\) components (largest eigenvalues) gives a compressed yet informative view of the data.Diagonalization also underlies spectral clustering, recommendation systems, and natural language processing. In these cases, the eigenvectors often correspond to latent patterns or community structures, and the eigenvalues measure their significance. For instance, in spectral clustering, the eigenvectors of a graph Laplacian can reveal cluster boundaries. In NLP, eigenvectors from co-occurrence matrices highlight semantic dimensions in word relationships.Another powerful insight comes from power iteration—a technique that leverages repeated matrix multiplication to find the dominant eigenvector. This behavior is at the core of algorithms like PageRank.Lastly, the diagonalized form \\(A = Q \\Lambda Q^{-1}\\) gives us a spectrum of the transformation: the eigenvalues summarize key properties like total variance (via the trace) and overall scaling (via the determinant). These quantities link abstract algebra to concrete geometry and help us interpret the transformation as acting independently along each eigenvector axis.Properties of Eigenvalues and EigenvectorsUnderstanding the properties of eigenvalues and eigenvectors reveals deep insights into linear transformations, variance, and the structure of data in machine learning. Here’s a concise overview: Scaling and LinearityIf \\(A \\mathbf{v} = \\lambda \\mathbf{v}\\), then for any scalar \\(c\\),\\(A (c \\mathbf{v}) = \\lambda (c \\mathbf{v})\\).So, any nonzero scalar multiple of an eigenvector is still an eigenvector associated with the same eigenvalue. The set of all such vectors (plus the zero vector) forms the eigenspace. A linear combination of two eigenvectors is generally not an eigenvector unless they share the same eigenvalue. Invariant SubspacesEigenvectors define one-dimensional invariant subspaces under transformation. The span of multiple eigenvectors (with corresponding eigenvalues \\(\\{\\lambda_1, \\lambda_2\\}\\)) also forms an invariant subspace. In PCA, the subspace spanned by the top \\(k\\) eigenvectors captures the principal variance in data. Geometric InterpretationEigenvectors are directions in which the matrix acts as a pure stretch/compression: \\(\\mid\\lambda\\mid \u0026gt; 1\\) → stretched \\(\\mid\\lambda\\mid \u0026lt; 1\\) → compressed \\(\\lambda = -1\\) → flipped (180° rotation) \\(\\lambda = 0\\) → squashed to the originIn symmetric matrices, eigenvectors form orthogonal axes—ideal for interpreting transformations. Variance Interpretation (PCA)For a covariance matrix \\(S\\): The eigenvector with the largest eigenvalue corresponds to the direction of maximum variance. Eigenvalues represent the amount of variance along their eigenvector directions. If eigenvalues are \\(\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n\\) with eigenvectors \\(e_1, e_2, \\dots, e_n\\), then: \\(e_1\\) is the direction of highest variance. \\(\\frac{\\lambda_1}{\\sum_i \\lambda_i}\\) is the proportion of total variance captured by that direction. The sum of all eigenvalues equals the trace of \\(S\\) (total variance). Orthogonality (for symmetric matrices) Eigenvectors corresponding to different eigenvalues are orthogonal:\\(e_i^T e_j = 0 \\text{ for } i \\ne j\\) We can choose an orthonormal set of eigenvectors for symmetric matrices, making projections and decompositions simpler. Sum and Product of Eigenvalues The sum of the eigenvalues of \\(A\\) is equal to its trace:\\(\\sum_{i=1}^{n} \\lambda_i = \\text{tr}(A)\\) The product of the eigenvalues equals the determinant of \\(A\\):\\(\\prod_{i=1}^{n} \\lambda_i = \\det(A)\\) If any eigenvalue is zero, \\(A\\) is singular. If all eigenvalues are positive, \\(A\\) is positive definite. Left vs Right Eigenvectors Typically, we refer to right eigenvectors satisfying \\(A \\mathbf{v} = \\lambda \\mathbf{v}\\). Left eigenvectors satisfy \\(\\mathbf{w}^T A = \\lambda \\mathbf{w}^T\\). For symmetric matrices, left and right eigenvectors are transposes of each other. In applications like PageRank, the left eigenvector of the transition matrix (eigenvector of \\(P^T\\)) represents the stationary distribution. These properties show how eigenvalues and eigenvectors help us understand invariant directions, the strength of transformation along those directions, and variance structure in data.Principal Component Analysis (PCA) Using Eigen DecompositionPrincipal Component Analysis (PCA) is a powerful technique that reduces the dimensionality of data while preserving as much variance as possible. Given an \\(m \\times n\\) data matrix \\(X\\) (with \\(m\\) samples and \\(n\\) features), the key idea is to find new axes—orthogonal directions in the feature space—that best capture the variance in the data. These axes are the principal components, which correspond to the top eigenvectors of the data’s covariance matrix.To derive PCA, we first center the data by subtracting the mean from each feature, so that each column of \\(X\\) has mean zero. Then, we compute the sample covariance matrix:\\[S = \\frac{1}{m - 1} X^T X\\]The goal is to find a unit vector \\(\\mathbf{w} \\in \\mathbb{R}^n\\) such that the projection of the data onto \\(\\mathbf{w}\\) has the maximum possible variance. The variance along \\(\\mathbf{w}\\) is given by:\\[\\mathrm{Var}(X\\mathbf{w}) = \\mathbf{w}^T S \\mathbf{w}\\]We maximize this subject to the constraint \\(\\|\\mathbf{w}\\| = 1\\). This leads us to an eigenvalue problem. Since \\(S\\) is symmetric and positive semidefinite, we can write:\\[S = Q \\Lambda Q^T\\]Here, \\(\\Lambda\\) is a diagonal matrix of eigenvalues \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n \\ge 0\\), and \\(Q\\) contains the corresponding orthonormal eigenvectors \\(\\mathbf{q}_1, \\mathbf{q}_2, \\dots, \\mathbf{q}_n\\) as columns.If we express \\(\\mathbf{w}\\) as \\(Q \\mathbf{v}\\) for some unit vector \\(\\mathbf{v}\\) (since \\(Q\\) is orthonormal), the objective becomes:\\[\\mathbf{w}^T S \\mathbf{w} = \\mathbf{v}^T \\Lambda \\mathbf{v} = \\sum_{i=1}^n \\lambda_i v_i^2\\]To maximize this weighted average of eigenvalues, we assign all weight to the largest eigenvalue by choosing \\(\\mathbf{v} = (1, 0, 0, \\dots, 0)^T\\). This corresponds to choosing \\(\\mathbf{w} = \\mathbf{q}_1\\), the eigenvector associated with \\(\\lambda_1\\). Thus, the first principal component is the direction of greatest variance in the data.Subsequent components are found by selecting eigenvectors orthogonal to the previous ones, associated with the next highest eigenvalues. This way, projecting data onto the top \\(k\\) eigenvectors captures the most variance possible among all \\(k\\)-dimensional subspaces.Another perspective is that PCA finds the best rank-\\(k\\) approximation of the data in terms of minimizing reconstruction error. This makes PCA not only a tool for understanding variance but also for efficient data compression and noise reduction.To summarize, compute the covariance matrix:\\[S = \\frac{1}{m - 1} X^T X\\]Then find eigenvectors \\(\\mathbf{e}_1, \\mathbf{e}_2, \\dots, \\mathbf{e}_n\\) and corresponding eigenvalues \\(\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n\\). The proportion of variance captured by the top \\(k\\) components is:\\[\\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{n} \\lambda_i}\\]Typically, one selects \\(k\\) so that this ratio exceeds a threshold like 95%, ensuring most of the information is retained while significantly reducing dimensionality.Once eigenvalues are computed, we often visualize them using a scree plot. This plot displays eigenvalues in descending order versus their component indices. A sharp “elbow” in the curve suggests that components beyond that point contribute little variance. For example, in the Iris dataset with four features, the scree plot shows the first eigenvalue around 4.2, the second about 0.24, and the remaining two near zero. This suggests that one or two principal components are sufficient to explain most of the variance.In practice, PCA is widely used for visualization, especially when reducing high-dimensional data to 2 or 3 components. It also serves as a preprocessing step for other algorithms, helping to mitigate the curse of dimensionality or reduce noise. Since the principal components are uncorrelated (due to orthogonality of eigenvectors), they provide cleaner inputs for downstream models.Here’s a Python example using scikit-learn on the Iris dataset:import numpy as npfrom sklearn.decomposition import PCAfrom sklearn.datasets import load_iris# Load and center the datairis = load_iris()X = iris.dataX = X - X.mean(axis=0)# Fit PCApca = PCA(n_components=4)pca.fit(X)print(\"Eigenvalues:\", pca.explained_variance_)print(\"Explained variance ratio:\", pca.explained_variance_ratio_)Typical output might be:Eigenvalues: [4.228 0.243 0.078 0.024]Explained variance ratio: [0.9246 0.0531 0.0171 0.0052]This confirms that the first component alone captures over 92% of the total variance, and the first two together over 97%. Projecting the data into these two dimensions gives a near-complete representation of the structure, making it ideal for plotting and exploratory analysis.Finally, PCA also helps denoise data. Noise tends to appear as low-variance components (low eigenvalues), and by discarding those, we remove irrelevant variation. In real-world ML pipelines, one typically performs PCA on training data, selects the number of components via a scree or cumulative variance plot, and then transforms both training and test data into the lower-dimensional space for modeling. Libraries like scikit-learn internally use efficient algorithms like Singular Value Decomposition (SVD) to compute PCA, especially when the number of features is large.Singular Value Decomposition (SVD)Eigen-decomposition we discussed is for square matrices (and especially symmetric matrices in many ML contexts). Singular Value Decomposition (SVD) is a more general matrix factorization that works for any \\(m \\times n\\) matrix (square or rectangular, symmetric or not). SVD is one of the most important algorithms in linear algebra – often described as the “Swiss Army knife” of matrix factorizations – because it has a wide range of applications from solving inverse problems to dimensionality reduction.The Singular Value Decomposition of a matrix \\(M\\) is a factorization of the form:\\[M = U\\, \\Sigma\\, V^T,\\]where:\\(M\\) is an \\(m \\times n\\) real matrix (think of m samples and n features, or an image matrix, etc.).\\(U\\) is an \\(m \\times m\\) orthogonal matrix (its columns are orthonormal vectors \\(u_1, u_2, \\dots, u_m\\), called left singular vectors).\\(V\\) is an \\(n \\times n\\) orthogonal matrix (its columns are orthonormal vectors \\(v_1, \\dots, v_n\\), called right singular vectors).\\(\\Sigma\\) is an \\(m \\times n\\) diagonal matrix (not necessarily square) with nonnegative values on the diagonal. These diagonal entries \\(\\sigma_1, \\sigma_2, \\dots\\) are called singular values, and by convention are ordered \\(\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge 0\\).Only the first \\(\\min(m,n)\\) diagonal entries of \\(\\Sigma\\) are non-zero; the rest of \\(\\Sigma\\) (if \\(m \\ne n\\)) are effectively padded with zeros. Intuitively, SVD says that any linear transformation \\(M\\) can be viewed as a rotation (by \\(V^T\\)), then a scaling (by \\(\\Sigma\\)), then another rotation (by \\(U\\)). This is analogous to eigen-decomposition but doesn’t require the matrix to be square or symmetric. In fact, if \\(M\\) is symmetric and orthogonally diagonalizable, SVD and eigen-decomposition coincide (U and V will be the same aside from dimension).Historical note: SVD was discovered in the 19th century by Eugenio Beltrami (1873) and Camille Jordan (1874) independentl (What Is the Singular Value Decomposition? – Nick Higham)】, though it wasn’t widely used until much later. It was popularized in numerical computing by Gene Golub and collaborators in the 1960s, who developed efficient algorithms (the Golub–Reinsch algorithm) to compute i (What Is the Singular Value Decomposition? – Nick Higham)】. With the advent of computers, SVD became a fundamental tool because it’s stable and reliable for solving linear systems and least squares, even when matrices are ill-conditioned or singular.Mathematical Intuition and Relation to EigenvaluesSVD can be derived or understood via the eigen-decomposition of \\(M^T M\\) (an \\(n \\times n\\) symmetric matrix) or \\(M M^T\\) (an \\(m \\times m\\) symmetric matrix). Note that \\(M^T M\\) is symmetric positive semidefinite, so it has an eigen-decomposition:\\[M^T M = V\\,D\\,V^T,\\]where \\(D\\) is diagonal with eigenvalues \\(\\mu_1, \\mu_2, \\dots, \\mu_n \\ge 0\\), and \\(V\\)’s columns are the eigenvectors (right singular vectors of \\(M\\)). It turns out the non-zero eigenvalues of \\(M^T M\\) (and \\(M M^T\\)) are the squares of the singular values (\\(\\sigma_i^2\\)). In fact: The singular values \\(\\sigma_i = \\sqrt{\\mu_i}\\) The eigenvectors of \\(M^T M\\) are the right singular vectors Similarly, eigenvectors of \\(M M^T\\) (which has the same non-zero eigenvalues) are the left singular vectors (columns of \\(U\\))\\[M M^T = U D' U^T\\]with the same non-zero eigenvalues \\(\\mu_i\\) filling \\(D'\\) (plus additional zeros if \\(m \u0026gt; n\\)).So the way to compute the SVD is: find eigenvalues and eigenvectors of \\(M^T M\\). Suppose \\(\\mu_1 \\ge \\mu_2 \\ge \\dots \\ge \\mu_r \u0026gt; 0\\) are the non-zero eigenvalues (\\(r = \\text{rank of } M\\)), with eigenvectors \\(v_1, \\dots, v_r\\). Set \\(\\sigma_i = \\sqrt{\\mu_i}\\). Let \\(v_i\\) be the i-th column of \\(V\\). Then define:\\[u_i = \\frac{1}{\\sigma_i} M v_i\\]One can show \\(u_i\\) are unit vectors and are eigenvectors of \\(M M^T\\). These form the first \\(r\\) columns of \\(U\\). The remaining columns of \\(U\\) (if any) can be any orthonormal vectors completing the basis. Similarly, if \\(n \u0026gt; r\\), the last columns of \\(V\\) can be any orthonormal completion (or correspond to the zero eigenvalues). This construction yields:\\[M = \\sum_{i=1}^r \\sigma_i u_i v_i^T,\\]which is the SVD.In summary, \\(U\\)’s columns are eigenvectors of \\(M M^T\\), \\(V\\)’s columns are eigenvectors of \\(M^T M\\), and \\(\\Sigma\\)’s diagonal entries are the square roots of those eigenvalues.Another intuition: think of \\(M\\) as transforming an n-dimensional vector (input) into an m-dimensional vector (output). The SVD tells us there is an orthonormal basis of input vectors \\(v_i\\) and an orthonormal basis of output vectors \\(u_i\\) such that:\\[M v_i = \\sigma_i u_i\\]For those familiar with eigenvectors, this looks almost like an eigen equation, except \\(v_i\\) and \\(u_i\\) live in different spaces if \\(m \\ne n\\). But:\\[M^T M v_i = \\sigma_i^2 v_i\\]does hold (so \\(v_i\\) is an eigenvector of \\(M^T M\\) with eigenvalue \\(\\sigma_i^2\\)). So SVD is an extension of eigen-decomposition to rectangular matrices.Geometric interpretationAs mentioned, any linear map can be seen as a rotation/reflection (by \\(V^T\\)), then axis-aligned stretching (by \\(\\Sigma\\)), then another rotation/reflection (by \\(U\\)). If you imagine a unit sphere in \\(\\mathbb{R}^n\\), applying \\(M\\) to it produces an m-dimensional ellipsoid. The principal semi-axes of that ellipsoid are \\(u_1, u_2, \\dots\\) (the left singular vectors), and their lengths are the singular values \\(\\sigma_1, \\sigma_2, \\dots\\). They tell us the directions in the input space that get mapped to those principal axes in the output. So singular values are basically the “strengths” of \\(M\\) along those special input/output directions.SVD in Practice: Low-Rank Approximations and ComputationOne of the most useful aspects of SVD is that it gives the best low-rank approximations of a matrix. If we have:\\[M = U \\Sigma V^T = \\sum_{i=1}^r \\sigma_i (u_i v_i^T)\\](where \\(r = \\text{rank of } M\\)), we can approximate \\(M\\) by truncating this sum to \\(k \u0026lt; r\\) terms:\\[M_k = \\sum_{i=1}^k \\sigma_i\\, u_i\\, v_i^T\\]This \\(M_k\\) is a rank-\\(k\\) matrix (only \\(k\\) singular values/vectors used). It turns out \\(M_k\\) is the best approximation of \\(M\\) among all rank-\\(k\\) matrices in terms of least-squares error (minimum Frobenius norm error) – this is known as the Eckart–Young theorem.The intuition is that since the singular values are in descending order, we are keeping the largest “components” of \\(M\\) and discarding the rest. If singular values drop off quickly, then \\(M\\) is well-approximated by a much smaller rank matrix.For example, if you have a big matrix of data but most of its structure lies in a few dimensions (e.g., a topic-document matrix where only a few underlying topics span the data), the top singular vectors capture those and the rest may be noise. In PCA terms, using SVD on the data matrix directly can give similar results to PCA on the covariance.Computing SVD: In code, computing SVD is easy with libraries. For instance, using NumPy:import numpy as npA = np.random.rand(6, 4) # a 6x4 matrixU, s, Vt = np.linalg.svd(A, full_matrices=False)This returns \\(U\\) (6×4 in this case, since full_matrices=False gives the reduced form), the singular values in array \\(s\\) (length 4), and \\(V^T\\) (4×4). We can reconstruct \\(A\\) via:U @ np.diag(s) @ Vtand it will match the original (within numerical precision). If we only want the top \\(k\\) singular values, we could take U[:,:k], s[:k], Vt[:k,:] to build the rank-\\(k\\) approximation.For very large matrices, one can use iterative methods or truncated SVD algorithms (like scipy.sparse.linalg.svds or scikit-learn’s TruncatedSVD which is useful for sparse matrices). In fact, PCA is often computed via SVD on the centered data matrix rather than eigen-decomposition of the covariance, because it’s more efficient when the number of features is large. Scikit-learn’s PCA, for example, uses an SVD under the hood. This is related to the fact that:\\[X^T X v = \\lambda v\\]is equivalent to the SVD of \\(X\\).Let’s illustrate SVD with a concrete example: image compressionWe take a grayscale image (which is basically a matrix of pixel intensities) and apply SVD to it. By keeping only a few singular values, we can reconstruct an approximation of the image.Consider the example image below. The first panel is the original image (512×512 pixels). The next panels show reconstructions using only the top 5 singular values, top 20, and top 50 (out of 512 total).As we increase the number of singular values in the reconstruction, the image quality improves. With 50 singular values (~10% of the possible components for this 512×512 image), the image is already largely recognizable.In code, the compression might look like this:import matplotlib.pyplot as pltfrom skimage import data# Load example image as a matriximg = data.camera().astype(float) # cameraman test image, shape (512,512)# Compute SVDU, s, Vt = np.linalg.svd(img, full_matrices=False)# Choose kk = 50img_approx = U[:,:k] @ np.diag(s[:k]) @ Vt[:k,:]plt.imshow(img_approx, cmap='gray')One can experiment with different \\(k\\). What SVD is doing here is essentially finding an optimal basis for the image’s pixel space such that you can truncate it.The power of SVD goes beyond images Natural language processing: the technique called Latent Semantic Analysis (LSA) uses SVD on a term-document matrix to uncover “topics” (the singular vectors) and reduce noise in text data. Recommender systems: SVD (or similar matrix factorization methods) is used to decompose the user-item ratings matrix into a product of lower-dimensional matrices. Solving least squares problems: SVD provides a robust method to compute the pseudoinverse and solve \\(Ax \\approx b\\) even if \\(A\\) is singular or ill-conditioned. To connect SVD back to eigen-decomposition vs PCA: PCA can be obtained from SVD of the data matrix. If \\(X\\) is \\(m \\times n\\) (m samples, n features), doing SVD:\\[X = U \\Sigma V^T\\]Here \\(U\\) is \\(m \\times m\\), \\(V\\) is \\(n \\times n\\). The columns of \\(V\\) (right singular vectors) are the eigenvectors of \\(X^T X\\) (the covariance matrix up to scaling), and the singular values relate to eigenvalues:\\[\\sigma_i^2 = \\lambda_i (m-1)\\]if using sample covariance. In fact, the principal components = columns of \\(V\\), and \\(X V = U \\Sigma\\) gives the principal component scores (projections) scaled by singular values.Scikit-learn’s PCA uses an SVD internally for efficiency. The difference is mostly in whether you subtract the mean and whether you divide by \\(\\sqrt{m-1}\\) in the singular values.Applications of SVD in MLWe’ve already touched on a few:Dimensionality ReductionSVD is at the heart of PCA (as explained) and more generally is used to reduce dimensionality of very large, sparse datasets (e.g., text). Sklearn’s TruncatedSVD is often used as a PCA alternative for sparse data (it doesn’t center the data, which can be fine for text frequency data). The advantage of SVD is it can work directly on the data without needing to form a covariance matrix, which is memory-intensive for large feature sets. For example, with a term-document matrix of size 100k documents, you wouldn’t explicitly form a \\(100k \\times 100k\\) covariance; you’d do truncated SVD to get, say, 100 topics.Latent Semantic Analysis (LSA) in NLPAs mentioned, by doing SVD on a term-document matrix (rows = documents, columns = terms, values = say TF-IDF scores), you get: \\(U\\) = document vectors in topic-space, \\(V\\) = term vectors in topic-space, \\(\\Sigma\\) = strengths of each “topic”. This uncovers latent concepts. After truncating, each document or term is represented in a reduced-dimensional semantic space. Queries and docs can be compared in this space to improve search (even if exact words don’t match, related concepts can be matched via the latent factors).Recommendation Systems (Collaborative Filtering)If you have a user-item rating matrix, SVD gives you matrices that represent users in a latent factor space and items in the same space. A rating is approximated by the dot product of user and item factor vectors. This reveals, for example, that a movie’s factor vector might encode how much it is comedy vs drama vs action, and a user’s factor vector encodes their preference for those genres. In practice, direct SVD on a sparse rating matrix may require filling missing entries, so often specialized alternating least squares are used, but conceptually it’s similar. The Netflix Prize solution was an ensemble of such factorization models.Image ProcessingBeyond compression, SVD (and PCA, which for images are sometimes called “eigenfaces” in face recognition) can decompose image datasets. For denoising images, one can keep only the largest singular values which tend to capture the main structures and discard smaller ones that often represent noise. SVD is also used in algorithms like image alignment and structure-from-motion in computer vision because it can find optimal linear alignments.Matrix Solvers and Model CompressionSVD is used to compute the Moore-Penrose pseudoinverse for solving linear systems. If a model has a large weight matrix (for instance, a fully connected layer in a neural network), one can compress it by approximating that weight matrix with a low-rank decomposition using SVD. Essentially, you replace a big weight matrix by two thinner matrices (which correspond to keeping only top singular vectors), which reduces the number of parameters and can speed up inference. In deep learning, after training a model, one can do SVD on weight layers to see if they have low effective rank and truncate them (with some fine-tuning to recover accuracy). SVD has also been used inside training algorithms for RNNs to enforce a well-conditioned weight matrix.Comparison Between PCA and SVDIt’s worth clarifying the relationship and differences between PCA and SVD, as they often get mentioned together:RelationshipPCA is essentially a specific application of SVD. If you take your data matrix \\(X\\) (with zero-mean for each feature) and perform SVD \\(X = U \\Sigma V^T\\), the right singular vectors \\(V\\) are the principal component directions, and the singular values \\(\\sigma_i\\) are related to the standard deviation of data along those components. In fact,\\[\\lambda_i = \\frac{\\sigma_i^2}{m-1}\\]would be the eigenvalues of the covariance. Thus, PCA results can be obtained by SVD. Conversely, if you have the covariance matrix\\[S = V \\Lambda V^T\\]and do eigen-decomposition, you could reconstruct an SVD of \\(X\\) by setting\\[U = X V \\Lambda^{-1/2}\\](for nonzero eigenvalues). So mathematically, PCA and SVD are tightly linked—PCA often just means applying SVD to a mean-centered data matrix and interpreting the results in terms of variance.Differences in FocusPCA is defined as a statistical procedure: it focuses on the covariance structure of data and identifies directions of maximal variance (making the data in the new coordinates uncorrelated). SVD is purely a matrix factorization that doesn’t inherently carry a statistical interpretation unless you connect it to something like \\(X^T X\\).SVD can be applied to matrices that are not covariance matrices (e.g., the term-document matrix, or a rectangular transformation matrix). So one difference: PCA typically involves first normalizing or standardizing data (and always centering), and it implicitly assumes we care about variance. SVD will happily factorize any matrix as is. For PCA, you must decide how to handle scaling of features (since covariance is sensitive to scale); SVD doesn’t care about that but if you gave it an unnormalized data matrix, the singular vectors would be dominated by whatever features have larger numeric scales.In practice, one usually scales features for PCA (e.g., using correlation matrix instead of covariance if units differ).Computational DifferencesPCA (eigen-decomposition) computes eigenvectors of an \\(n \\times n\\) covariance matrix (if \\(n\\) = number of features). This can be expensive when \\(n\\) is large (e.g., \\(n = 10000\\) would mean a \\(10000 \\times 10000\\) matrix to diagonalize). SVD can work directly on the \\(m \\times n\\) data matrix, which might be more feasible if \\(n\\) is large but \\(m\\) is smaller, or if the matrix is sparse.For example, if you have \\(10^6\\) samples and \\(10^5\\) features (very tall matrix), eigen-decomposition of the covariance (\\(10^5 \\times 10^5\\)) is huge, but SVD on \\(10^6 \\times 10^5\\) might be manageable with iterative methods.Similarly, SVD can handle missing data by algorithms that operate only on observed entries, whereas PCA (covariance computation) can’t directly handle missing values.Use-CasesPCA is typically used for exploratory data analysis, feature reduction, visualization, and sometimes preprocessing to decorrelate features. SVD has broader applications: beyond data variance, it’s used in matrix completion, inverse problems, etc.For example, if you have a linear system \\(Ax = b\\), SVD helps solve it (via pseudoinverse) especially if \\(A\\) is not full rank. PCA wouldn’t be referenced in that context.So PCA is a subset of what SVD can do, focusing on variance in a dataset.Output InterpretationIn PCA, we talk about “principal components”, “explained variance”, and we often care about how many components to choose, interpret the components, etc. In SVD of an arbitrary matrix, we talk about “singular vectors” and “singular values”, and their magnitudes, but not usually “variance explained” unless it’s specifically data matrix.In PCA we often drop components with small eigenvalues because they are mostly noise. In SVD, dropping small singular values gives a low-rank approximation which might be for noise reduction or compression. These are analogous ideas (and indeed the same operation), but PCA frames it as data compression, SVD frames it as matrix compression.Summary: When to Use PCA vs SVD They will give the same result if applied consistently (PCA on covariance vs SVD on centered data). If your data is very high-dimensional (many features), use SVD to compute PCA. If your data matrix is sparse (like text data or recommender systems), use truncated SVD directly. If the goal is interpretability in terms of original features and variance, PCA terminology is used. If the goal is matrix factorization or solving a linear algebra problem, SVD terminology is used. PCA uses SVD under the hood.As a fun fact, if you perform PCA on un-centered data (i.e., don’t subtract mean), that’s equivalent to doing an SVD on the raw data matrix (with an extra singular vector corresponding to the mean direction usually appearing). Generally, we center data for PCA.Not to brag, but we just wrangled some serious math today. We went from stretching vectors to compressing images — not bad for one post!We started by understanding what makes eigenvectors special: they’re the directions that remain unchanged (except for scaling) under a transformation. We saw how eigenvalues and eigenvectors reveal the internal structure of matrices and how they’re deeply tied to variance, stability, and interpretability in machine learning.We then looked at eigen-decomposition and diagonalization, and how these concepts simplify complex operations, especially when dealing with symmetric matrices like covariance matrices. This naturally led us to Principal Component Analysis (PCA), where eigenvectors define the principal directions of variance, and eigenvalues tell us how important each direction is.Then came Singular Value Decomposition (SVD) — a more general, powerful factorization that works for any matrix, not just square or symmetric ones. We explored how SVD connects back to eigenvalues, how it gives us the best low-rank approximation of data, and how it’s used in practice for tasks like image compression, noise reduction, LSA in NLP, recommender systems, and model compression.We also clarified how PCA and SVD are related: PCA can be computed via SVD, but SVD has broader applications beyond just analyzing variance.In the end, these tools — eigenvalues, eigenvectors, PCA, and SVD — give us more than math. They give us a way to see the essence of data, simplify it, compress it, and make it more interpretable. And in a field as noisy and high-dimensional as machine learning, that’s incredibly powerful."},{"title":"Linear Algebra Basics for ML - Systems of Linear Equations","url":"/blog/2022/mathforml-linalg3/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"January 20, 2022","content":"Many machine learning tasks boil down to optimizing model parameters that best fit the observed data. For instance, finding the best-fit line in linear regression is equivalent to solving a system of linear equations. In this post, we’ll dive into the mathematics behind solving these systems using multiple methods. We’ll explore row reduction, Gaussian elimination, and Cramer’s Rule—each backed by real-world ML context, intuitive math, and working code.Systems of Linear Equations in Machine LearningIn many machine learning problems—such as multiple linear regression, parameter estimation in models, and even network analysis—we often need to solve systems of equations that look like:\\[A \\mathbf{x} = \\mathbf{b}\\]where \\(A\\) is an \\(m \\times n\\) matrix of coefficients, \\(\\mathbf{x}\\) is the vector of unknowns (model parameters), and \\(\\mathbf{b}\\) is the result or outcome vector. The goal is to determine the values of \\(\\mathbf{x}\\) that satisfy this equation—an essential step in fitting many models.Solving Systems Using Row ReductionLet’s start with something familiar: multiple linear regression. When you derive the solution analytically, you arrive at the normal equations:\\[X^T X \\mathbf{\\beta} = X^T \\mathbf{y}\\]Here, \\(X\\) is the feature matrix, \\(\\mathbf{y}\\) is the target vector, and \\(\\mathbf{\\beta}\\) is the vector of regression coefficients you’re trying to find. This is a classic system of linear equations that we can solve using row reduction.To do that, we first write the augmented matrix \\([ A \\mid b ]\\). Take, for example, the system:\\[\\begin{aligned}2x + y \u0026amp;= 8 \\\\x + 3y \u0026amp;= 13\\end{aligned}\\]which becomes:\\[\\left[\\begin{array}{cc|c}2 \u0026amp; 1 \u0026amp; 8 \\\\1 \u0026amp; 3 \u0026amp; 13 \\\\\\end{array}\\right]\\]Using elementary row operations—swapping rows, scaling rows, and adding multiples of rows to one another—we aim to convert this matrix into an upper triangular or reduced row-echelon form. Once it’s simplified, back substitution helps us solve for each variable one by one.Here’s a Python implementation of row reduction in action:import numpy as npdef row_reduce(A, b): Ab = np.hstack((A.astype(float), b.reshape(-1, 1).astype(float))) n = Ab.shape[0] for i in range(n): max_row = np.argmax(np.abs(Ab[i:, i])) + i Ab[[i, max_row]] = Ab[[max_row, i]] Ab[i] = Ab[i] / Ab[i, i] for j in range(i+1, n): Ab[j] = Ab[j] - Ab[i] * Ab[j, i] x = np.zeros(n) for i in range(n-1, -1, -1): x[i] = Ab[i, -1] - np.dot(Ab[i, i+1:n], x[i+1:n]) return xA = np.array([[2, 1], [1, 3]])b = np.array([8, 13])beta = row_reduce(A, b)print(\"Solution using row reduction:\", beta)In machine learning, row reduction isn’t just a math exercise—it tells us something about our data. For instance, if a solution doesn’t exist or isn’t unique, it may be due to multicollinearity in the dataset. And while libraries use faster numerical methods, the logic here underpins those solvers.Whether you’re solving regression models, performing parameter estimation, or diagnosing issues in data, understanding row reduction equips you with both intuition and control over the structure of your solution.Gaussian Elimination for Larger SystemsFor larger systems, manually applying row operations isn’t practical. This is where Gaussian elimination shines—a structured algorithm that eliminates variables step-by-step, reducing your system to a manageable triangular form.At its core, Gaussian elimination performs forward elimination to zero out entries below the diagonal, followed by back substitution to solve for each variable from bottom to top.The method also relies on partial pivoting—swapping rows to ensure numerical stability, especially important when working with floating-point data in real-world ML scenarios.Here’s how you might code it:import numpy as npdef gaussian_elimination(A, b): n = A.shape[0] Ab = np.hstack((A.astype(float), b.reshape(-1, 1).astype(float))) for i in range(n): max_row = np.argmax(np.abs(Ab[i:, i])) + i Ab[[i, max_row]] = Ab[[max_row, i]] for j in range(i+1, n): factor = Ab[j, i] / Ab[i, i] Ab[j, i:] -= factor * Ab[i, i:] x = np.zeros(n) for i in range(n-1, -1, -1): x[i] = (Ab[i, -1] - np.dot(Ab[i, i+1:], x[i+1:])) / Ab[i, i] return xA = np.array([[3, 2, -4], [2, 3, 3], [5, -3, 1]])b = np.array([3, 15, 14])solution = gaussian_elimination(A, b)print(\"Solution using Gaussian elimination:\", solution)In machine learning, this method is commonly used when solving normal equations in regression. It’s also part of the foundation behind algorithms like LU decomposition and QR factorization. Gaussian elimination is particularly useful in data fitting, interpolation, and models that involve large-scale linear systems. While it may not always be used directly, it’s hiding beneath many optimization libraries used in practice.Cramer’s Rule: A Theoretical LensFor smaller systems—or when you’re looking for a more theoretical perspective—Cramer’s Rule offers an elegant way to find each variable explicitly using determinants.Given a system \\(A\\mathbf{x} = \\mathbf{b}\\), Cramer’s Rule gives each solution as:\\[x_i = \\frac{\\det(A_i)}{\\det(A)}\\]where \\(A_i\\) is the matrix formed by replacing the \\(i\\)-th column of \\(A\\) with the vector \\(\\mathbf{b}\\).While this approach is computationally expensive for large systems (since it requires calculating multiple determinants), it’s incredibly valuable for understanding how solutions relate to the structure of the system. For instance, a zero determinant tells you that the system doesn’t have a unique solution—critical for understanding singularities and data redundancy.Here’s an implementation in Python:import numpy as npdef cramer_rule(A, b): det_A = np.linalg.det(A) if np.isclose(det_A, 0): raise ValueError(\"The system has no unique solution (determinant is zero).\") n = A.shape[0] x = np.zeros(n) for i in range(n): A_i = A.copy() A_i[:, i] = b x[i] = np.linalg.det(A_i) / det_A return xA = np.array([[2, -1, 3], [1, 0, 2], [4, 1, 8]])b = np.array([5, 4, 12])solution_cramer = cramer_rule(A, b)print(\"Solution using Cramer's Rule:\", solution_cramer)Though not practical for large ML pipelines, Cramer’s Rule is ideal for educational and theoretical purposes. It’s often used to validate the structure of small systems and to perform sensitivity analysis—seeing how small changes in input affect the output. It’s especially insightful when exploring linear dependencies between variables in regression or in understanding how features influence predictions at a theoretical level.ConclusionSolving systems of linear equations isn’t just a math skill—it’s a fundamental tool in the machine learning toolbox. Whether you’re fitting a linear regression model or designing a network analysis algorithm, these techniques will inevitably come into play.Row reduction gives you a practical handle on small systems and helps interpret model behavior. Gaussian elimination scales that power for larger systems and forms the foundation of more advanced solvers. And Cramer’s Rule offers theoretical clarity on how and why solutions behave the way they do.Mastering these approaches deepens your understanding of both data and models—ensuring you’re not just using ML tools, but also understanding what’s happening behind the scenes."},{"title":"Linear Algebra Basics for ML - Matrices and Matrix Operations","url":"/blog/2022/mathforml-linalg2/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"January 15, 2022","content":"In machine learning, the power of matrices is undeniable. Whether you’re manipulating datasets, performing linear transformations in neural networks, or analyzing graph structures, matrices provide a compact and efficient way to represent and process information. In this post, we’ll explore the world of matrices and matrix operations through a problem-driven approach. Each section grounds the math in real ML use cases, walks through the concepts clearly, and wraps up with code and applications.Matrices and Matrix OperationsThink of a matrix as a grid—a two-dimensional array of numbers. On the surface, it may just look like a neat way to organize data, but in machine learning, it does so much more. Matrices are the building blocks for operations like transforming input features, propagating signals through neural networks, and encoding relationships in graph data. Their elegance lies in how they capture complex transformations so succinctly.Matrix Addition and MultiplicationSuppose you have two datasets or feature maps, and you want to combine them before feeding them into a model. Or maybe you need to apply a set of learned weights to an input layer in a neural network. These scenarios boil down to two essential operations: matrix addition and multiplication.Matrix addition is pretty straightforward. If two matrices \\(A\\) and \\(B\\) have the same dimensions \\(m \\times n\\), you simply add corresponding elements:\\[(A + B)_{ij} = A_{ij} + B_{ij}\\]Matrix multiplication is a little more involved—and powerful. Given a matrix \\(A\\) of size \\(m \\times n\\) and a matrix \\(B\\) of size \\(n \\times p\\), the resulting matrix \\(C = AB\\) will be of size \\(m \\times p\\), where each element is calculated as:\\[C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\\]This operation is crucial in machine learning because it represents linear combinations of inputs, weighted by learned parameters.Let’s look at how these operations play out in code:import numpy as npA = np.array([[1, 2], [3, 4]])B = np.array([[5, 6], [7, 8]])# Matrix additionsum_matrix = A + Bprint(\"Matrix Sum:\\n\", sum_matrix)# Matrix multiplicationproduct_matrix = np.dot(A, B)print(\"Matrix Product:\\n\", product_matrix)Matrix addition and multiplication are two of the most widely used operations in machine learning. Matrix addition allows us to combine information from multiple sources—whether that’s merging datasets, summing feature maps, or applying a bias term in a model. It’s a simple operation, but it appears throughout every stage of a data pipeline, especially when dealing with mini-batches of data or ensemble-style models.Matrix multiplication, on the other hand, is fundamental to how machine learning models process and learn from data. In neural networks, the input at each layer is multiplied by a weight matrix to transform it into a new representation. This transformation captures patterns and relationships between features, allowing deeper layers to learn more abstract concepts. The same operation is used to combine embeddings, transform spatial information in computer vision, and project data into new feature spaces.Outside of neural networks, matrix multiplication also shows up in dimensionality reduction techniques like PCA, where data is multiplied by a matrix of principal components to produce a compressed version of the original dataset. In graph-based learning, adjacency matrices are multiplied with feature matrices to enable message passing across nodes—allowing information to flow and be aggregated from neighboring nodes in graph neural networks.Transpose, Inverse, Determinant, Trace, and RankSometimes, understanding the structure of your data or solving an equation requires going deeper into what a matrix really does. Is it reversible? How much does it scale the space? How complex is it?The transpose of a matrix \\(A\\), written as \\(A^T\\), simply flips its rows and columns:\\[(A^T)_{ij} = A_{ji}\\]The inverse of a square matrix \\(A\\), when it exists, satisfies:\\[AA^{-1} = A^{-1}A = I\\]where \\(I\\) is the identity matrix.The determinant, \\(\\det(A)\\), gives a scalar that represents how much the transformation defined by \\(A\\) scales space. If the determinant is zero, the matrix isn’t invertible.The trace of a matrix is the sum of its diagonal entries:\\[\\text{tr}(A) = \\sum_{i=1}^{n} A_{ii}\\]And the rank tells you how many dimensions your matrix truly spans—how many linearly independent rows or columns it contains.All of these properties surface in ML. You might invert a matrix to solve a linear system in regression. The determinant and rank tell you if your data is redundant. The trace often appears in loss functions or regularization terms.Here’s how to compute them:import numpy as npA = np.array([[4, 7], [2, 6]])print(\"Transpose:\\n\", A.T)print(\"Determinant:\", np.linalg.det(A))if np.linalg.det(A) != 0: print(\"Inverse:\\n\", np.linalg.inv(A))print(\"Trace:\", np.trace(A))print(\"Rank:\", np.linalg.matrix_rank(A))These matrix operations go beyond basic arithmetic and into the territory of understanding the structure of data and transformations. The transpose operation, for example, is used when calculating gradients and aligning matrix dimensions during dot products—especially important in backpropagation in deep learning.The inverse of a matrix is essential in solving linear systems analytically. While in practice we often use approximations or decompositions, the concept of matrix inversion is still central to understanding linear regression, where the solution for the optimal weights can be written as:\\(\\mathbf{w} = (X^T X)^{-1} X^T y\\)This closed-form solution shows how matrix operations solve real ML problems when the dataset is small and the solution is tractable.The determinant tells us whether a transformation preserves volume or collapses space—if it’s zero, the transformation is not invertible. This matters in unsupervised learning, such as normalizing flows, where transformations must be invertible and differentiable. Similarly, the trace of a matrix shows up in optimization problems as a regularization penalty—for example, in matrix factorization or low-rank approximations, where trace minimization helps control complexity.Finally, matrix rank is crucial for understanding the expressive power of your dataset. If a feature matrix is rank-deficient, it means some features are linear combinations of others—signaling multicollinearity, which can break regression models or inflate variance in predictions. Knowing the rank helps us detect redundancy, reduce overfitting, and improve generalization.Special Matrices in Machine LearningSome matrices have properties that make them especially elegant—and efficient—in machine learning workflows.The identity matrix \\(I\\) acts as a neutral element in multiplication:\\[AI = IA = A\\]Diagonal matrices are square matrices with nonzero entries only on the diagonal. They scale vectors component-wise, simplifying many operations.Symmetric matrices satisfy \\(A = A^T\\). Covariance matrices, for example, are symmetric and reveal how variables vary together.Orthogonal matrices satisfy:\\[Q^T Q = QQ^T = I\\]They preserve angles and lengths, which is why they’re used in rotations, reflections, and orthonormal bases.A positive definite matrix is symmetric and satisfies:\\[x^T A x \u0026gt; 0\\]for any nonzero vector \\(x\\). These appear in optimization problems, where you want to ensure a unique minimum, and in models like ridge regression.Let’s create and check these properties:import numpy as npI = np.eye(3)print(\"Identity Matrix:\\n\", I)D = np.diag([1, 2, 3])print(\"Diagonal Matrix:\\n\", D)S = np.array([[2, -1], [-1, 2]])print(\"Symmetric Matrix:\\n\", S)Q = np.array([[0, 1], [1, 0]])print(\"Orthogonal Check (Q^T Q):\\n\", np.dot(Q.T, Q))eigenvalues = np.linalg.eigvals(S)print(\"Eigenvalues (Positive Definite if all \u0026gt; 0):\", eigenvalues)Special matrices such as identity, diagonal, symmetric, orthogonal, and positive definite matrices are not just mathematical curiosities—they are workhorses behind efficient and stable ML algorithms.The identity matrix is used to initialize parameters or serve as a “no-op” transformation. It plays a role in residual networks (ResNets), where identity shortcuts help preserve gradients in deep architectures. In regularization techniques, the identity matrix appears in terms like \\(\\lambda I\\), added to ensure numerical stability when inverting nearly singular matrices.Diagonal matrices simplify transformations by applying scaling operations—critical in feature normalization, where diagonal matrices can represent per-feature standard deviations or inverse variances. They also arise in eigenvalue decompositions, where the diagonal matrix holds the eigenvalues representing the importance of each principal component or latent feature.Symmetric matrices dominate statistics and probabilistic ML. Covariance matrices are symmetric by definition and reflect the relationships among features. In PCA, the symmetric covariance matrix is decomposed to extract the directions of maximum variance. Orthogonal matrices, which preserve inner products, form the basis for QR decomposition and SVD, enabling dimensionality reduction, whitening transformations, and stable numerical methods. And positive definite matrices guarantee convexity in optimization, which is why they’re vital in kernel methods (like in SVMs), Gaussian processes, and regularized regression.Understanding these special types helps you design models that are faster, more stable, and easier to train—and gives you the vocabulary to interpret the results geometrically.Block and Partitioned MatricesWhen working with massive datasets or large models, it often makes sense to split things up. Block matrices let us partition a large matrix into smaller, more manageable pieces:\\[A = \\begin{bmatrix}A_{11} \u0026amp; A_{12} \\\\A_{21} \u0026amp; A_{22}\\end{bmatrix}\\]This is especially helpful in distributed computing or batch processing, where each submatrix can be processed independently.You can split a matrix into blocks like this:import numpy as npA = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]])A11 = A[:2, :2]A12 = A[:2, 2:]A21 = A[2:, :2]A22 = A[2:, 2:]print(\"Block A11:\\n\", A11)print(\"Block A12:\\n\", A12)print(\"Block A21:\\n\", A21)print(\"Block A22:\\n\", A22)Block matrices offer a powerful abstraction when dealing with large-scale datasets, multi-modal learning, or distributed computing. Rather than operating on an entire matrix at once, we can break it into smaller, logically meaningful parts and process them independently or in parallel.In mini-batch training, especially in stochastic gradient descent (SGD), data is already treated as a collection of smaller blocks. Matrix operations are performed on each batch independently, allowing the model to scale to massive datasets without running out of memory.In multi-view learning or multi-task learning, where different feature sets or targets are grouped by modality or domain, block matrices naturally model these segmented structures. Each block might represent a different view of the same entity—like image features and text descriptions of the same object—and learning proceeds with interactions across blocks.In Graph ML, block matrices are used to capture the community structure within large graphs. The adjacency matrix of a graph can be partitioned into blocks, each corresponding to a subgraph or cluster, enabling more scalable and interpretable analysis. In parallel and distributed ML, partitioning matrices allows data to be distributed across nodes, with operations like block-wise matrix multiplication or block gradient descent running concurrently.Block matrices bring structure and efficiency to matrix computation, helping bridge the gap between mathematical elegance and engineering scalability.ConclusionFrom simple addition to sophisticated transformations, matrices give us a powerful framework to represent and manipulate data in machine learning. Their structure captures everything from raw inputs to learned representations and even the relationships between them. Whether you’re solving a system of equations, rotating vectors, or analyzing massive graphs, the right matrix operation unlocks the magic.In this post, we explored core matrix operations with a focus on their relevance in real ML tasks. This foundation will carry you far—as we dive deeper into eigenvectors, decompositions, and optimization in upcoming chapters of the Math for ML series."},{"title":"Linear Algebra Basics for ML - Vector Operations, Norms, and Projections","url":"/blog/2022/mathforml-linalg1/","categories":"machine-learning, math, math-for-ml","tags":"ml, ai, linear-algebra, math","date":"January 13, 2022","content":"In machine learning, every problem—whether it’s image recognition, natural language processing, or anomaly detection—begins with how we represent data. In this post, we’ll take a deep dive into vectors and vector spaces, exploring the underlying mathematics that enables ML algorithms to learn from data. We’ll follow a problem-driven approach: each section starts with a real-world ML challenge, introduces the mathematical tool needed, explains the theory from the ground up, and concludes with a detailed solution along with Python coding examples and real-world applications in NLP and Computer Vision.Vector Addition and Scalar MultiplicationImagine you’re training a neural network. At each step, your model needs to update its parameters—those weights that define how the network behaves. But how exactly are these updates performed? Behind the scenes, you’re combining current weights with gradient information and scaling them based on how much you want to change. This simple operation, which powers the heart of deep learning, relies entirely on vector addition and scalar multiplication.Let’s break it down.A vector \\(\\mathbf{v}\\) in \\(\\mathbb{R}^n\\) is an ordered list of \\(n\\) real numbers:\\[\\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\]Vectors represent points, directions, or quantities in space—and in machine learning, they can represent feature values, model parameters, or gradients.Adding two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) looks like this:\\[\\mathbf{u} + \\mathbf{v} = \\begin{bmatrix} u_1 + v_1 \\\\ u_2 + v_2 \\\\ \\vdots \\\\ u_n + v_n \\end{bmatrix}\\]This operation is performed element-wise. You can think of it as combining two data points or updating a parameter by adding the change suggested by a gradient.Now, multiplying a vector by a scalar \\(\\alpha\\) stretches or shrinks it:\\[\\alpha \\mathbf{u} = \\begin{bmatrix} \\alpha u_1 \\\\ \\alpha u_2 \\\\ \\vdots \\\\ \\alpha u_n \\end{bmatrix}\\]This changes the length of the vector (its magnitude), but not its direction—unless \\(\\alpha\\) is negative, in which case the vector flips.This brings us to the classic weight update rule in gradient descent:\\[\\mathbf{w}_{\\text{new}} = \\mathbf{w}_{\\text{old}} - \\alpha \\nabla \\mathbf{w}\\]Here, \\(\\nabla \\mathbf{w}\\) is the gradient vector, and \\(\\alpha\\) is the learning rate. You’re subtracting a scaled version of the gradient from the current weights—a simple but powerful operation that helps your model learn.Let’s see this in action:import numpy as np# Define weight vector and gradient vectorweights = np.array([0.5, -0.3, 0.8])gradients = np.array([0.1, -0.05, 0.2])# Update rule using gradient descent (learning rate = 0.1)learning_rate = 0.1new_weights = weights - learning_rate * gradientsprint(\"Updated Weights:\", new_weights)Running this snippet simulates one step of gradient descent. Each component of the weight vector is nudged slightly in the direction opposite to the gradient, scaled by how aggressively you want to learn (i.e., the learning rate).These operations form the computational backbone of most optimization routines in machine learning. For example, gradient descent, stochastic gradient descent (SGD), and their variants (like Adam or RMSProp) all rely on vector addition and scalar multiplication to iteratively update model parameters.In reinforcement learning, policy gradients are updated via vector-based gradient steps to maximize expected returns. In graph neural networks (GNNs), feature propagation across nodes often involves combining node and neighbor vectors—again using addition and scaling operations.Even in time-series forecasting, models like LSTMs and GRUs perform cell updates using vector operations that combine new and past information. These operations are not only simple but also form the atomic operations used to build and train large-scale models across supervised, unsupervised, and self-supervised learning paradigms.Linear Combinations, Span, Basis, and DimensionalityIn many machine learning applications, especially in domains like text and image processing, the data we work with lives in very high-dimensional spaces. Word embeddings might have 300 dimensions, images can have thousands of pixel values—and all of this contributes to increased computation, memory usage, and sometimes even noise. But do we really need all those dimensions?Often, we don’t. The trick lies in representing high-dimensional data more compactly—without losing the essence of what makes that data useful. To do that, we need to understand the concepts of linear combinations, span, basis, and dimensionality.Let’s start with the basics.A linear combination allows us to build a new vector using a set of existing vectors. Suppose we have vectors \\(\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k\\) in \\(\\mathbb{R}^n\\), then a linear combination is:\\[\\mathbf{v} = \\alpha_1 \\mathbf{v}_1 + \\alpha_2 \\mathbf{v}_2 + \\dots + \\alpha_k \\mathbf{v}_k\\]In other words, we scale each vector by a coefficient and add them together.The span of a set of vectors is the collection of all possible vectors you can form using linear combinations of those vectors. If your set spans \\(\\mathbb{R}^n\\), then it’s powerful enough to represent any point in that space.Now, when you want the most compact and efficient representation, you need a basis: a set of linearly independent vectors that spans the entire space. With a basis, every vector in the space can be uniquely expressed as a linear combination of these basis vectors.The number of vectors in the basis gives us the dimension of the space. And in machine learning, reducing this dimensionality—while preserving the most important structure in the data—is exactly what techniques like PCA (Principal Component Analysis) aim to do.PCA finds a new basis where each new vector (called a principal component) captures as much variance in the data as possible. These new basis vectors are orthogonal, and often, you only need the first few to explain most of your data’s structure. This simplifies your dataset, making models faster and potentially more robust.Here’s a quick example of how we can represent a vector using a standard basis:import numpy as np# Representing a point in 2D space using the standard basisbasis_vectors = np.array([[1, 0], [0, 1]])coefficients = np.array([3, 4])new_vector = np.dot(coefficients, basis_vectors)print(\"New Vector from Linear Combination:\", new_vector)This gives us the vector \\([3, 4]\\) as a combination of the basis vectors \\([1, 0]\\) and \\([0, 1]\\).The idea of expressing data using a minimal set of representative directions is ubiquitous in ML. Autoencoders, for instance, learn a compressed latent space—a learned basis—into which input data is encoded and later decoded. This compressed representation reduces dimensionality and noise while preserving the key structure of the input.In signal processing, sparse coding and dictionary learning aim to represent signals as linear combinations of a few basis elements. In finance, factor models such as PCA or ICA are used to explain asset returns through a few economic factors.Dimensionality reduction is also crucial in medical imaging, where thousands of features (pixels or voxels) are compressed into fewer latent variables for classification tasks like tumor detection. In genomics, where gene expression data is high-dimensional, basis discovery helps in clustering, feature selection, and disease classification.Orthogonality and ProjectionsImagine you’re working on a computer vision task and want to compress image data without losing important information. You could reduce the number of features, but how do you ensure you’re keeping the parts that matter?The answer lies in projections. More specifically, orthogonal projections onto lower-dimensional subspaces help us reduce dimensionality while preserving the most important aspects of the data.In math terms, two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) are said to be orthogonal if their dot product is zero:\\[\\mathbf{u} \\cdot \\mathbf{v} = 0\\]This means the vectors point in completely independent directions—think of axes in 3D space.When you project one vector onto another, you’re essentially extracting its component in that direction. The projection of \\(\\mathbf{u}\\) onto \\(\\mathbf{v}\\) is calculated as:\\[\\text{proj}_{\\mathbf{v}}(\\mathbf{u}) = \\left( \\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\|\\mathbf{v}\\|^2} \\right) \\mathbf{v}\\]This formula plays a major role in Principal Component Analysis (PCA), where data is projected onto orthogonal axes—called principal components—that maximize variance. In simpler terms, PCA finds the directions in which your data varies the most and projects it there, compressing the information without much loss.Here’s a simple example to visualize a projection:import numpy as np# Define a data point and a principal component directiondata_point = np.array([3, 4])principal_component = np.array([1, 0])# Project the data point onto the principal componentproj_scalar = np.dot(data_point, principal_component) / np.dot(principal_component, principal_component)projection = proj_scalar * principal_componentprint(\"Projection of Data Point onto Principal Component:\", projection) Orthogonality is a core concept in many areas of ML that deal with feature decorrelation. For instance, independent component analysis (ICA) extends PCA by aiming for statistically independent (not just uncorrelated) components, which is useful in blind source separation tasks like speech signal decomposition.In computer vision, projections are used in dimensionality reduction pipelines (e.g., PCA, t-SNE, UMAP) to extract visually salient features. Orthogonality also plays a role in orthogonal initialization of deep neural networks, which helps preserve variance and avoid vanishing gradients in very deep architectures.In physics-informed ML and scientific computing, projections are used to map complex nonlinear states into simpler bases for simulation or differential equation modeling. This helps compress high-dimensional simulations like fluid dynamics into learnable latent dynamics.Vector Norms and Model ComplexityNow suppose you’re training a regression model, and it starts overfitting—performing great on training data but terribly on unseen examples. One common trick to fix this is regularization, which involves penalizing large weights. But that raises a question: how do you actually measure the “size” of a vector?That’s where vector norms come in.The L1 norm (also known as the Manhattan norm) sums up the absolute values of all the vector components:\\[\\|\\mathbf{v}\\|_1 = \\sum_{i=1}^{n} |v_i|\\]The L2 norm (Euclidean norm) is the straight-line distance from the origin:\\[\\|\\mathbf{v}\\|_2 = \\sqrt{\\sum_{i=1}^{n} v_i^2}\\]And the L∞ norm captures the largest absolute value in the vector:\\[\\|\\mathbf{v}\\|_\\infty = \\max_i |v_i|\\]Each of these norms gives a different perspective on vector size, and each is used in different types of regularization. Lasso regression uses the L1 norm to encourage sparsity (pushing some weights to zero), while Ridge regression uses the L2 norm to shrink all weights evenly. L∞ isn’t as common, but it can be useful when you care about controlling the biggest contributor.Here’s a quick example showing how to calculate all three:import numpy as npv = np.array([1, -2, 3])l1_norm = np.sum(np.abs(v))l2_norm = np.sqrt(np.sum(v ** 2))linf_norm = np.max(np.abs(v))print(\"L1 Norm:\", l1_norm)print(\"L2 Norm:\", l2_norm)print(\"L∞ Norm:\", linf_norm)Regularization using norms is a widely adopted strategy to prevent overfitting and improve generalization. In logistic regression and linear classifiers, L1 and L2 regularization help constrain the coefficient space, thereby simplifying the decision boundary and increasing robustness.In deep learning, the weight decay trick (essentially L2 norm penalty) is used alongside batch normalization and dropout to stabilize training. In federated learning, client updates are sometimes clipped using norm thresholds to avoid noisy or adversarial contributions.Beyond regularization, norms are also used in anomaly detection (e.g., distance from cluster centroids), metric learning (contrastive and triplet losses depend on Euclidean or cosine distances), and adversarial robustness (where L∞, L2, and L1 norms define allowed perturbation bounds). Whether you’re compressing models for edge deployment or defending them against attacks, norms guide and constrain model behavior effectively.Inner and Outer ProductsLet’s say you’re building a recommendation engine or clustering users based on their behavior. One of the first things you’ll need to do is measure how similar two data points are. But how do you quantify “similarity” in a vector space?That’s where the inner product comes in.Given two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), the inner product (or dot product) is:\\[\\mathbf{u} \\cdot \\mathbf{v} = \\sum_{i=1}^{n} u_i v_i\\]If the result is large, it means the vectors are pointing in similar directions—i.e., they are similar. In fact, this is the basis for cosine similarity, a widely used metric in NLP for comparing word vectors.On the other hand, the outer product builds a full matrix of interactions between two vectors. For \\(\\mathbf{u} \\in \\mathbb{R}^m\\) and \\(\\mathbf{v} \\in \\mathbb{R}^n\\), the outer product looks like this:\\[\\mathbf{u} \\otimes \\mathbf{v} =\\begin{bmatrix}u_1v_1 \u0026amp; u_1v_2 \u0026amp; \\cdots \u0026amp; u_1v_n \\\\u_2v_1 \u0026amp; u_2v_2 \u0026amp; \\cdots \u0026amp; u_2v_n \\\\\\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\u_mv_1 \u0026amp; u_mv_2 \u0026amp; \\cdots \u0026amp; u_mv_n \\\\\\end{bmatrix}\\]While the inner product gives us a single similarity score, the outer product creates a full interaction map—useful when we want to understand how features influence each other.Here’s how to compute both in Python:import numpy as npu = np.array([1, 2])v = np.array([3, 4])# Inner product: similarityinner_product = np.dot(u, v)print(\"Inner Product:\", inner_product)# Outer product: interaction matrixouter_product = np.outer(u, v)print(\"Outer Product:\\n\", outer_product)The inner product is at the heart of similarity calculations across many algorithms—whether it’s comparing embeddings in a semantic space or computing attention weights in transformers. Cosine similarity, a normalized inner product, is frequently used in clustering, information retrieval, and question-answering systems.In kernel methods, such as support vector machines (SVMs), the inner product is generalized into kernel functions to measure similarity in high-dimensional (sometimes infinite-dimensional) spaces. The outer product, on the other hand, forms the basis of covariance matrices, used in PCA, Gaussian processes, and multivariate statistics.In deep learning, attention mechanisms use scaled dot-product attention, which essentially involves outer products to compute weighted combinations across key-query-value triplets. Outer products are also fundamental to tensor factorization and bilinear pooling methods used in multi-modal learning, such as combining image and text inputs in visual question answering (VQA).From updating model parameters with gradient descent to compressing high-dimensional data, measuring vector magnitudes, and analyzing feature interactions—vectors and vector spaces are the hidden framework behind much of machine learning.These mathematical ideas may seem abstract at first, but they solve incredibly concrete problems. They power dimensionality reduction in PCA, make regularization work in regression models, and drive similarity searches in recommendation engines and NLP systems.If you’ve followed along this far, you’ve just walked through the foundational math that makes many machine learning techniques possible. And we’re just getting started—these concepts will show up again and again as we explore more advanced topics in the Math for ML series."},{"title":"k-means++ অ্যালগরিদম","url":"/blog/2020/k-means/","categories":"","tags":"","date":"August 07, 2020","content":""},{"title":"মেশিন লার্নিং-এর কলকব্জা ৩ : Linear Regression ও Gradient Descent","url":"/blog/2020/linear-regression-gradient-descent/","categories":"","tags":"","date":"August 04, 2020","content":""},{"title":"মেশিন লার্নিং-এর কলকব্জা ২ : ক্ষয়ক্ষতির হিসেব","url":"/blog/2020/","categories":"","tags":"","date":"August 03, 2020","content":""},{"title":"মেশিন লার্নিং-এর কলকব্জা ১ : যন্ত্র কতভাবে শেখে","url":"/blog/2020/","categories":"","tags":"","date":"August 03, 2020","content":""},{"title":"চলো যন্ত্রকে ফুল চেনাই","url":"/blog/2020/","categories":"","tags":"","date":"August 03, 2020","content":""},{"title":"মেশিন লার্নিং : উপক্রমণিকা","url":"/blog/2020/","categories":"","tags":"","date":"August 02, 2020","content":""},{"title":"Home","url":"/","categories":"","tags":"","date":"","content":"Hey there — welcome to my little corner of the internet. This space is part portfolio, part notebook, and part late-night side project hub. I use it to share what I’m working on, what I’m learning, and sometimes just what I’m thinking out loud.I currently work as a Risk Analyst at American Express. In my current role, I design and develop Key Risk Indicators (KRIs) and data-driven risk models to automate and enhance risk control mechanisms. By applying machine learning, statistical analysis, and end-to-end data pipelines, I help ensure scalable, efficient, and proactive risk management. I hold a B.Tech. in Information Technology from IIEST Shibpur and an M.Tech. in Computer Science (Specialization in Data Science) from ISI Kolkata.You’ll find many of my blog posts here exploring topics in data science, machine learning, and computer science — written with both clarity and curiosity in mind. If you’re a fellow learner, a data nerd, or someone just starting out in this field, I hope you find something useful or thought-provoking here.In my free time, I’m usually doing things that help me slow down a bit, like experimenting in the kitchen (often with no recipe), rewatching a comfort web series (read The Big Bang Theory), or getting completely absorbed in a good non-fiction or documentary.Thanks for stopping by — feel free to explore, learn, or just say hello."},{"title":"Academic Resources","url":"/acadresrc/","categories":"","tags":"","date":"","content":"GATE CS/IT Notes: https://gatecsebyjs.github.io/IIT-JEE PCM Notes: https://plustwopcm.blogspot.com/"},{"title":"Contact","url":"/contact/","categories":"","tags":"","date":"","content":"Social MediaEmail"},{"title":"Résumé","url":"/cv/","categories":"","tags":"","date":"","content":""},{"title":"Gallery","url":"/gallery/","categories":"","tags":"","date":"","content":""},{"title":"Projects","url":"/projects/","categories":"","tags":"","date":"","content":""},{"title":"Research","url":"/reserach/","categories":"","tags":"","date":"","content":"M.Tech. Thesis Prediction of GLOF through the assessment of Glacial Lake Hazards Thesis Copy B.Tech. Major 3D Shape Analysis using Machine Learning Techniques: Predicting Viral Infections from CT scans Report Copy B.Tech. Minor 2D straight skeleton of any non-self intersecting polygon Report Copy"},{"title":"Resources","url":"/_pages/resource/","categories":"","tags":"","date":"","content":""},{"title":"Random Resources","url":"/rndmresrc/","categories":"","tags":"","date":"","content":"Random resources related to personal-dev and general interests go here!"},{"title":"ml","url":"/blog/tag/ml/","categories":"","tags":"","date":"","content":""},{"title":"ai","url":"/blog/tag/ai/","categories":"","tags":"","date":"","content":""},{"title":"linear-algebra","url":"/blog/tag/linear-algebra/","categories":"","tags":"","date":"","content":""},{"title":"math","url":"/blog/tag/math/","categories":"","tags":"","date":"","content":""},{"title":"probability-statistics","url":"/blog/tag/probability-statistics/","categories":"","tags":"","date":"","content":""},{"title":"machine-learning","url":"/blog/category/machine-learning/","categories":"","tags":"","date":"","content":""},{"title":"math","url":"/blog/category/math/","categories":"","tags":"","date":"","content":""},{"title":"math-for-ml","url":"/blog/category/math-for-ml/","categories":"","tags":"","date":"","content":""},{"title":"Blog","url":"/blogging/index.html","categories":"","tags":"","date":"","content":"{% assign blog_name_size = site.blog_name | size %}{% assign blog_description_size = site.blog_description | size %}{% if blog_name_size \u003e 0 or blog_description_size \u003e 0 %} {{ site.blog_name }} {{ site.blog_description }} {% endif %}{% if site.display_tags or site.display_categories %} {% for tag in site.display_tags %} {{ tag }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% if site.display_categories.size \u003e 0 and site.display_tags.size \u003e 0 %} \u0026bull; {% endif %} {% for category in site.display_categories %} {{ category }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% endif %}{% assign featured_posts = site.posts | where: \"featured\", \"true\" %}{% if featured_posts.size \u003e 0 %}{% assign is_even = featured_posts.size | modulo: 2 %}{% for post in featured_posts %}{{ post.title }}{{ post.description }} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ year }} {% endfor %} {% endif %} {% if page.pagination.enabled %} {% assign postlist = paginator.posts %} {% else %} {% assign postlist = site.posts %} {% endif %} {% for post in postlist %} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {% assign tags = post.tags | join: \"\" %} {% assign categories = post.categories | join: \"\" %} {% if post.thumbnail %} {% endif %} {% if post.redirect == blank %} {{ post.title }} {% elsif post.redirect contains '://' %} {{ post.title }} {% else %} {{ post.title }} {% endif %} {{ post.description }} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.date | date: '%B %d, %Y' }} {% if post.external_source %} \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.external_source }} {% endif %} {{ year }} {% if tags != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for tag in post.tags %} {{ tag }} \u0026nbsp; {% endfor %} {% endif %} {% if categories != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for category in post.categories %} {{ category }} \u0026nbsp; {% endfor %} {% endif %} {% if post.thumbnail %} {% endif %} {% endfor %} {% if page.pagination.enabled %}{% include pagination.liquid %}{% endif %} You may find my musings here"},{"title":"Blog - page 2","url":"/blogging/page/2/index.html","categories":"","tags":"","date":"","content":"{% assign blog_name_size = site.blog_name | size %}{% assign blog_description_size = site.blog_description | size %}{% if blog_name_size \u003e 0 or blog_description_size \u003e 0 %} {{ site.blog_name }} {{ site.blog_description }} {% endif %}{% if site.display_tags or site.display_categories %} {% for tag in site.display_tags %} {{ tag }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% if site.display_categories.size \u003e 0 and site.display_tags.size \u003e 0 %} \u0026bull; {% endif %} {% for category in site.display_categories %} {{ category }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% endif %}{% assign featured_posts = site.posts | where: \"featured\", \"true\" %}{% if featured_posts.size \u003e 0 %}{% assign is_even = featured_posts.size | modulo: 2 %}{% for post in featured_posts %}{{ post.title }}{{ post.description }} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ year }} {% endfor %} {% endif %} {% if page.pagination.enabled %} {% assign postlist = paginator.posts %} {% else %} {% assign postlist = site.posts %} {% endif %} {% for post in postlist %} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {% assign tags = post.tags | join: \"\" %} {% assign categories = post.categories | join: \"\" %} {% if post.thumbnail %} {% endif %} {% if post.redirect == blank %} {{ post.title }} {% elsif post.redirect contains '://' %} {{ post.title }} {% else %} {{ post.title }} {% endif %} {{ post.description }} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.date | date: '%B %d, %Y' }} {% if post.external_source %} \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.external_source }} {% endif %} {{ year }} {% if tags != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for tag in post.tags %} {{ tag }} \u0026nbsp; {% endfor %} {% endif %} {% if categories != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for category in post.categories %} {{ category }} \u0026nbsp; {% endfor %} {% endif %} {% if post.thumbnail %} {% endif %} {% endfor %} {% if page.pagination.enabled %}{% include pagination.liquid %}{% endif %} You may find my musings here"},{"title":"Blog - page 3","url":"/blogging/page/3/index.html","categories":"","tags":"","date":"","content":"{% assign blog_name_size = site.blog_name | size %}{% assign blog_description_size = site.blog_description | size %}{% if blog_name_size \u003e 0 or blog_description_size \u003e 0 %} {{ site.blog_name }} {{ site.blog_description }} {% endif %}{% if site.display_tags or site.display_categories %} {% for tag in site.display_tags %} {{ tag }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% if site.display_categories.size \u003e 0 and site.display_tags.size \u003e 0 %} \u0026bull; {% endif %} {% for category in site.display_categories %} {{ category }} {% unless forloop.last %} \u0026bull; {% endunless %} {% endfor %} {% endif %}{% assign featured_posts = site.posts | where: \"featured\", \"true\" %}{% if featured_posts.size \u003e 0 %}{% assign is_even = featured_posts.size | modulo: 2 %}{% for post in featured_posts %}{{ post.title }}{{ post.description }} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ year }} {% endfor %} {% endif %} {% if page.pagination.enabled %} {% assign postlist = paginator.posts %} {% else %} {% assign postlist = site.posts %} {% endif %} {% for post in postlist %} {% if post.external_source == blank %} {% assign read_time = post.content | number_of_words | divided_by: 180 | plus: 1 %} {% else %} {% assign read_time = post.feed_content | strip_html | number_of_words | divided_by: 180 | plus: 1 %} {% endif %} {% assign year = post.date | date: \"%Y\" %} {% assign tags = post.tags | join: \"\" %} {% assign categories = post.categories | join: \"\" %} {% if post.thumbnail %} {% endif %} {% if post.redirect == blank %} {{ post.title }} {% elsif post.redirect contains '://' %} {{ post.title }} {% else %} {{ post.title }} {% endif %} {{ post.description }} {{ read_time }} min read \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.date | date: '%B %d, %Y' }} {% if post.external_source %} \u0026nbsp; \u0026middot; \u0026nbsp; {{ post.external_source }} {% endif %} {{ year }} {% if tags != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for tag in post.tags %} {{ tag }} \u0026nbsp; {% endfor %} {% endif %} {% if categories != \"\" %} \u0026nbsp; \u0026middot; \u0026nbsp; {% for category in post.categories %} {{ category }} \u0026nbsp; {% endfor %} {% endif %} {% if post.thumbnail %} {% endif %} {% endfor %} {% if page.pagination.enabled %}{% include pagination.liquid %}{% endif %} You may find my musings here"}]