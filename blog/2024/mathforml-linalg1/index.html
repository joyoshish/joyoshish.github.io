<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra Basics for ML - Vector Operations, Norms, and Projections | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Linear Algebra 1 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2024/mathforml-linalg1/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">Computer Science and Data Science</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra Basics for ML - Vector Operations, Norms, and Projections</h1> <p class="post-meta"> May 13, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear_algebra</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#vector-addition-and-scalar-multiplication">Vector Addition and Scalar Multiplication</a></li> <li class="toc-entry toc-h2"><a href="#linear-combinations-span-basis-and-dimensionality">Linear Combinations, Span, Basis, and Dimensionality</a></li> <li class="toc-entry toc-h2"><a href="#orthogonality-and-projections">Orthogonality and Projections</a></li> <li class="toc-entry toc-h2"><a href="#vector-norms-and-model-complexity">Vector Norms and Model Complexity</a></li> <li class="toc-entry toc-h2"><a href="#inner-and-outer-products">Inner and Outer Products</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>In machine learning, every problem—whether it’s image recognition, natural language processing, or anomaly detection—begins with how we represent data. In this post, we’ll take a deep dive into vectors and vector spaces, exploring the underlying mathematics that enables ML algorithms to learn from data. We’ll follow a problem-driven approach: each section starts with a real-world ML challenge, introduces the mathematical tool needed, explains the theory from the ground up, and concludes with a detailed solution along with Python coding examples and real-world applications in NLP and Computer Vision.</p> <hr> <h2 id="vector-addition-and-scalar-multiplication">Vector Addition and Scalar Multiplication</h2> <p>Imagine you’re training a neural network. At each step, your model needs to update its parameters—those weights that define how the network behaves. But how exactly are these updates performed? Behind the scenes, you’re combining current weights with gradient information and scaling them based on how much you want to change. This simple operation, which powers the heart of deep learning, relies entirely on vector addition and scalar multiplication.</p> <p>Let’s break it down.</p> <p>A vector \(\mathbf{v}\) in \(\mathbb{R}^n\) is an ordered list of \(n\) real numbers:</p> \[\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}\] <p>Vectors represent points, directions, or quantities in space—and in machine learning, they can represent feature values, model parameters, or gradients.</p> <p>Adding two vectors \(\mathbf{u}\) and \(\mathbf{v}\) looks like this:</p> \[\mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}\] <p>This operation is performed element-wise. You can think of it as combining two data points or updating a parameter by adding the change suggested by a gradient.</p> <p>Now, multiplying a vector by a scalar \(\alpha\) stretches or shrinks it:</p> \[\alpha \mathbf{u} = \begin{bmatrix} \alpha u_1 \\ \alpha u_2 \\ \vdots \\ \alpha u_n \end{bmatrix}\] <p>This changes the length of the vector (its magnitude), but not its direction—unless \(\alpha\) is negative, in which case the vector flips.</p> <p>This brings us to the classic weight update rule in gradient descent:</p> \[\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \alpha \nabla \mathbf{w}\] <p>Here, \(\nabla \mathbf{w}\) is the gradient vector, and \(\alpha\) is the learning rate. You’re subtracting a scaled version of the gradient from the current weights—a simple but powerful operation that helps your model learn.</p> <p>Let’s see this in action:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define weight vector and gradient vector
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="c1"># Update rule using gradient descent (learning rate = 0.1)
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">new_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Updated Weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">new_weights</span><span class="p">)</span>
</code></pre></div></div> <p>Running this snippet simulates one step of gradient descent. Each component of the weight vector is nudged slightly in the direction opposite to the gradient, scaled by how aggressively you want to learn (i.e., the learning rate).</p> <p>These same operations show up everywhere. In NLP, we update word embeddings using gradients—every word vector in a model like Word2Vec gets refined through such updates. In Computer Vision, convolutional filters (which are essentially matrices or higher-dimensional tensors made up of vector-like slices) are tuned via backpropagation, relying on vector addition and scaling.</p> <p>Whether you’re fine-tuning the weights of a deep network or learning dense word representations, these basic operations—vector addition and scalar multiplication—are always in play. They may seem simple, but they’re fundamental to everything that follows in the world of machine learning.</p> <hr> <h2 id="linear-combinations-span-basis-and-dimensionality">Linear Combinations, Span, Basis, and Dimensionality</h2> <p>In many machine learning applications, especially in domains like text and image processing, the data we work with lives in very high-dimensional spaces. Word embeddings might have 300 dimensions, images can have thousands of pixel values—and all of this contributes to increased computation, memory usage, and sometimes even noise. But do we really need all those dimensions?</p> <p>Often, we don’t. The trick lies in representing high-dimensional data more compactly—without losing the essence of what makes that data useful. To do that, we need to understand the concepts of linear combinations, span, basis, and dimensionality.</p> <p>Let’s start with the basics.</p> <p>A <strong>linear combination</strong> allows us to build a new vector using a set of existing vectors. Suppose we have vectors \(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\) in \(\mathbb{R}^n\), then a linear combination is:</p> \[\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_k \mathbf{v}_k\] <p>In other words, we scale each vector by a coefficient and add them together.</p> <p>The <strong>span</strong> of a set of vectors is the collection of all possible vectors you can form using linear combinations of those vectors. If your set spans \(\mathbb{R}^n\), then it’s powerful enough to represent any point in that space.</p> <p>Now, when you want the most compact and efficient representation, you need a <strong>basis</strong>: a set of linearly independent vectors that spans the entire space. With a basis, every vector in the space can be uniquely expressed as a linear combination of these basis vectors.</p> <p>The number of vectors in the basis gives us the <strong>dimension</strong> of the space. And in machine learning, reducing this dimensionality—while preserving the most important structure in the data—is exactly what techniques like PCA (Principal Component Analysis) aim to do.</p> <p>PCA finds a new basis where each new vector (called a principal component) captures as much variance in the data as possible. These new basis vectors are orthogonal, and often, you only need the first few to explain most of your data’s structure. This simplifies your dataset, making models faster and potentially more robust.</p> <p>Here’s a quick example of how we can represent a vector using a standard basis:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Representing a point in 2D space using the standard basis
</span><span class="n">basis_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">new_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">coefficients</span><span class="p">,</span> <span class="n">basis_vectors</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">New Vector from Linear Combination:</span><span class="sh">"</span><span class="p">,</span> <span class="n">new_vector</span><span class="p">)</span>
</code></pre></div></div> <p>This gives us the vector \([3, 4]\) as a combination of the basis vectors \([1, 0]\) and \([0, 1]\).</p> <p>In practice, especially in NLP, we apply dimensionality reduction techniques like PCA to compress high-dimensional word vectors into more manageable sizes. This helps in both visualization and reducing complexity for downstream models.</p> <p>In computer vision, PCA is used for image compression—transforming large image data into a smaller set of values that still retain the key visual features. This can drastically reduce storage and computation without sacrificing much accuracy in tasks like recognition or classification.</p> <p>So whether you’re dealing with text or pixels, these foundational ideas—linear combinations, span, basis, and dimension—are what let us tame the curse of dimensionality and make sense of our data in smarter ways.</p> <hr> <h2 id="orthogonality-and-projections">Orthogonality and Projections</h2> <p>Imagine you’re working on a computer vision task and want to compress image data without losing important information. You could reduce the number of features, but how do you ensure you’re keeping the parts that matter?</p> <p>The answer lies in projections. More specifically, <em>orthogonal projections</em> onto lower-dimensional subspaces help us reduce dimensionality while preserving the most important aspects of the data.</p> <p>In math terms, two vectors \(\mathbf{u}\) and \(\mathbf{v}\) are said to be orthogonal if their dot product is zero:</p> \[\mathbf{u} \cdot \mathbf{v} = 0\] <p>This means the vectors point in completely independent directions—think of axes in 3D space.</p> <p>When you project one vector onto another, you’re essentially extracting its component in that direction. The projection of \(\mathbf{u}\) onto \(\mathbf{v}\) is calculated as:</p> \[\text{proj}_{\mathbf{v}}(\mathbf{u}) = \left( \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2} \right) \mathbf{v}\] <p>This formula plays a major role in Principal Component Analysis (PCA), where data is projected onto orthogonal axes—called principal components—that maximize variance. In simpler terms, PCA finds the directions in which your data varies the most and projects it there, compressing the information without much loss.</p> <p>Here’s a simple example to visualize a projection:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define a data point and a principal component direction
</span><span class="n">data_point</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">principal_component</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Project the data point onto the principal component
</span><span class="n">proj_scalar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">data_point</span><span class="p">,</span> <span class="n">principal_component</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">principal_component</span><span class="p">,</span> <span class="n">principal_component</span><span class="p">)</span>
<span class="n">projection</span> <span class="o">=</span> <span class="n">proj_scalar</span> <span class="o">*</span> <span class="n">principal_component</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Projection of Data Point onto Principal Component:</span><span class="sh">"</span><span class="p">,</span> <span class="n">projection</span><span class="p">)</span>
</code></pre></div></div> <p>In real-world NLP tasks, we use projections to reduce the dimensionality of word vectors, helping to clean up noisy embeddings and improve interpretability. In computer vision, orthogonal projections help extract relevant image features, enabling better classification, detection, or compression.</p> <hr> <h2 id="vector-norms-and-model-complexity">Vector Norms and Model Complexity</h2> <p>Now suppose you’re training a regression model, and it starts overfitting—performing great on training data but terribly on unseen examples. One common trick to fix this is regularization, which involves penalizing large weights. But that raises a question: how do you actually measure the “size” of a vector?</p> <p>That’s where vector norms come in.</p> <p>The <strong>L1 norm</strong> (also known as the Manhattan norm) sums up the absolute values of all the vector components:</p> \[\|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <p>The <strong>L2 norm</strong> (Euclidean norm) is the straight-line distance from the origin:</p> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}\] <p>And the <strong>L∞ norm</strong> captures the largest absolute value in the vector:</p> \[\|\mathbf{v}\|_\infty = \max_i |v_i|\] <p>Each of these norms gives a different perspective on vector size, and each is used in different types of regularization. Lasso regression uses the L1 norm to encourage sparsity (pushing some weights to zero), while Ridge regression uses the L2 norm to shrink all weights evenly. L∞ isn’t as common, but it can be useful when you care about controlling the biggest contributor.</p> <p>Here’s a quick example showing how to calculate all three:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">l1_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
<span class="n">l2_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">linf_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L1 Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">l1_norm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L2 Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">l2_norm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L∞ Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">linf_norm</span><span class="p">)</span>
</code></pre></div></div> <p>In NLP, we use norm-based regularization to keep our word embeddings generalizable. In computer vision, we regularize convolutional filters to avoid overfitting to noise in the training images. Measuring and controlling vector magnitude is key to making our models not just accurate, but robust.</p> <hr> <h2 id="inner-and-outer-products">Inner and Outer Products</h2> <p>Let’s say you’re building a recommendation engine or clustering users based on their behavior. One of the first things you’ll need to do is measure how similar two data points are. But how do you quantify “similarity” in a vector space?</p> <p>That’s where the <strong>inner product</strong> comes in.</p> <p>Given two vectors \(\mathbf{u}\) and \(\mathbf{v}\), the inner product (or dot product) is:</p> \[\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_i v_i\] <p>If the result is large, it means the vectors are pointing in similar directions—i.e., they are similar. In fact, this is the basis for cosine similarity, a widely used metric in NLP for comparing word vectors.</p> <p>On the other hand, the <strong>outer product</strong> builds a full matrix of interactions between two vectors. For \(\mathbf{u} \in \mathbb{R}^m\) and \(\mathbf{v} \in \mathbb{R}^n\), the outer product looks like this:</p> \[\mathbf{u} \otimes \mathbf{v} = \begin{bmatrix} u_1v_1 &amp; u_1v_2 &amp; \cdots &amp; u_1v_n \\ u_2v_1 &amp; u_2v_2 &amp; \cdots &amp; u_2v_n \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ u_mv_1 &amp; u_mv_2 &amp; \cdots &amp; u_mv_n \\ \end{bmatrix}\] <p>While the inner product gives us a single similarity score, the outer product creates a full interaction map—useful when we want to understand how features influence each other.</p> <p>Here’s how to compute both in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Inner product: similarity
</span><span class="n">inner_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Inner Product:</span><span class="sh">"</span><span class="p">,</span> <span class="n">inner_product</span><span class="p">)</span>

<span class="c1"># Outer product: interaction matrix
</span><span class="n">outer_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Outer Product:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">outer_product</span><span class="p">)</span>
</code></pre></div></div> <p>In NLP, we use inner products to compare embeddings and cluster similar words or documents. Outer products come in handy when building attention mechanisms or covariance matrices that model relationships between features. In computer vision, outer products are used to analyze spatial patterns in images, enabling algorithms to detect textures, faces, and more.</p> <hr> <p>From updating model parameters with gradient descent to compressing high-dimensional data, measuring vector magnitudes, and analyzing feature interactions—vectors and vector spaces are the hidden framework behind much of machine learning.</p> <p>These mathematical ideas may seem abstract at first, but they solve incredibly concrete problems. They power dimensionality reduction in PCA, make regularization work in regression models, and drive similarity searches in recommendation engines and NLP systems.</p> <p>If you’ve followed along this far, you’ve just walked through the foundational math that makes many machine learning techniques possible. And we’re just getting started—these concepts will show up again and again as we explore more advanced topics in the Math for ML series.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> </body> </html>