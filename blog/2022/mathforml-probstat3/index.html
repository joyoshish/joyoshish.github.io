<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Probability &amp; Statistics for Data Science - Bayesian Thinking, MLE, MAP &amp; Inference | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Probability &amp; Statistics 3 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-probstat3/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <a id="search-button" class="nav-link mobile-search-button" href="javascript:void(0)" aria-label="Search"> <i class="ti ti-search"></i> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">GATE CS / JEE Notes</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> <div id="search-modal" class="search-modal"> <div class="search-container"> <div class="search-header"> <div class="search-input-container"> <i class="ti ti-search search-icon"></i> <input type="text" id="search-input" class="search-input" placeholder="Search..." autocomplete="off"> <button id="search-close" class="search-close-button"> <i class="ti ti-x"></i> </button> </div> <div class="search-shortcuts"> <span class="search-shortcut"> <kbd>/</kbd> or <kbd>Ctrl</kbd>+<kbd>K</kbd> to search </span> <span class="search-shortcut"> <kbd>↑</kbd><kbd>↓</kbd> to navigate </span> <span class="search-shortcut"> <kbd>Enter</kbd> to select </span> <span class="search-shortcut"> <kbd>Esc</kbd> to close </span> </div> </div> <div id="search-results" class="search-results"> <div class="search-results-container"></div> </div> </div> </div> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Probability &amp; Statistics for Data Science - Bayesian Thinking, MLE, MAP &amp; Inference</h1> <p class="post-meta"> June 10, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/probability-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> probability-statistics</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#bayes-theorem-and-conditional-probability">Bayes’ Theorem and Conditional Probability</a> <ul> <li class="toc-entry toc-h3"><a href="#example-diagnostic-testing">Example: Diagnostic Testing</a></li> <li class="toc-entry toc-h3"><a href="#python-code-for-the-example">Python Code for the Example</a></li> <li class="toc-entry toc-h3"><a href="#visualization">Visualization</a></li> <li class="toc-entry toc-h3"><a href="#applications-in-data-science">Applications in Data Science</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#prior-likelihood-and-posterior">Prior, Likelihood, and Posterior</a> <ul> <li class="toc-entry toc-h3"> <a href="#prior-distribution">Prior Distribution</a> <ul> <li class="toc-entry toc-h4"><a href="#-beta-prior-for-probabilities-theta-in-0-1">▸ Beta Prior (for probabilities \(\theta \in [0, 1]\)):</a></li> <li class="toc-entry toc-h4"><a href="#-gaussian-prior-for-real-valued-theta">▸ Gaussian Prior (for real-valued \(\theta\)):</a></li> <li class="toc-entry toc-h4"><a href="#-uniform-prior-non-informative">▸ Uniform Prior (non-informative):</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#likelihood-function">Likelihood Function</a> <ul> <li class="toc-entry toc-h4"><a href="#-example-bernoullibinomial-likelihood">▸ Example: Bernoulli/Binomial Likelihood</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#posterior-distribution">Posterior Distribution</a> <ul> <li class="toc-entry toc-h4"><a href="#-beta-prior--binomial-likelihood">▸ Beta Prior + Binomial Likelihood</a></li> <li class="toc-entry toc-h4"> <a href="#-posterior-summary-statistics---beta-distribution">▸ Posterior Summary Statistics - Beta Distribution</a> <ul> <li class="toc-entry toc-h5"><a href="#1-posterior-mean">1. Posterior Mean</a></li> <li class="toc-entry toc-h5"><a href="#2-posterior-variance">2. Posterior Variance</a></li> <li class="toc-entry toc-h5"><a href="#3-map-estimate-posterior-mode">3. MAP Estimate (Posterior Mode)</a></li> </ul> </li> <li class="toc-entry toc-h4"> <a href="#-posterior-derivation-gaussian-likelihood-and-gaussian-prior">▸ Posterior Derivation: Gaussian Likelihood and Gaussian Prior</a> <ul> <li class="toc-entry toc-h5"><a href="#step-1-likelihood-function">Step 1: Likelihood Function</a></li> <li class="toc-entry toc-h5"><a href="#step-2-prior">Step 2: Prior</a></li> <li class="toc-entry toc-h5"> <a href="#step-3-posterior-derivation">Step 3: Posterior Derivation</a> <ul> <li class="toc-entry toc-h6"><a href="#-posterior-mean">▸ Posterior Mean:</a></li> <li class="toc-entry toc-h6"><a href="#-posterior-variance">▸ Posterior Variance:</a></li> </ul> </li> <li class="toc-entry toc-h5"><a href="#interpretation">Interpretation</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h3"><a href="#example-inferring-coin-bias">Example: Inferring Coin Bias</a></li> <li class="toc-entry toc-h3"><a href="#visualization-1">Visualization</a></li> <li class="toc-entry toc-h3"> <a href="#applications-in-data-science-1">Applications in Data Science</a> <ul> <li class="toc-entry toc-h4"><a href="#bayesian-ab-testing">Bayesian A/B Testing</a></li> <li class="toc-entry toc-h4"><a href="#bayesian-regression">Bayesian Regression</a></li> <li class="toc-entry toc-h4"><a href="#fraud-detection">Fraud Detection</a></li> <li class="toc-entry toc-h4"><a href="#recommender-systems">Recommender Systems</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#maximum-likelihood-estimation-mle-vs-maximum-a-posteriori-estimation-map">Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori Estimation (MAP)</a> <ul> <li class="toc-entry toc-h3"> <a href="#mle-derivation-and-explanation">MLE: Derivation and Explanation</a> <ul> <li class="toc-entry toc-h4"><a href="#mle-derivation-bernoulli-case">MLE Derivation: Bernoulli Case</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#map-derivation-and-explanation">MAP: Derivation and Explanation</a> <ul> <li class="toc-entry toc-h4"><a href="#map-derivation-bernoulli-likelihood-and-beta-prior">MAP Derivation: Bernoulli Likelihood and Beta Prior</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#bernoulli-example-with-beta-prior">Bernoulli Example with Beta Prior</a></li> <li class="toc-entry toc-h3"><a href="#visualization-2">Visualization</a></li> <li class="toc-entry toc-h3"><a href="#inferences-we-can-make">Inferences we can make:</a></li> <li class="toc-entry toc-h3"> <a href="#applications-in-data-science-2">Applications in Data Science</a> <ul> <li class="toc-entry toc-h4"><a href="#logistic-regression">Logistic Regression</a></li> <li class="toc-entry toc-h4"><a href="#bayesian-linear-regression">Bayesian Linear Regression</a></li> <li class="toc-entry toc-h4"><a href="#cold-start-and-sparse-data-problems">Cold Start and Sparse Data Problems</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#conjugate-priors">Conjugate Priors</a> <ul> <li class="toc-entry toc-h3"><a href="#formal-definition">Formal Definition</a></li> <li class="toc-entry toc-h3"><a href="#why-it-matters">Why It Matters</a></li> <li class="toc-entry toc-h3"> <a href="#example-beta-prior-for-a-bernoullibinomial-likelihood">Example: Beta Prior for a Bernoulli/Binomial Likelihood</a> <ul> <li class="toc-entry toc-h4"><a href="#likelihood">Likelihood</a></li> <li class="toc-entry toc-h4"><a href="#prior">Prior</a></li> <li class="toc-entry toc-h4"><a href="#posterior">Posterior</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#interpretation-1">Interpretation</a></li> <li class="toc-entry toc-h3"> <a href="#visualization-3">Visualization</a> <ul> <li class="toc-entry toc-h4"><a href="#whats-happening-under-the-hood">What’s Happening Under the Hood:</a></li> <li class="toc-entry toc-h4"><a href="#inferences-to-make">Inferences to make:</a></li> </ul> </li> <li class="toc-entry toc-h3"> <a href="#applications-in-data-science-3">Applications in Data Science</a> <ul> <li class="toc-entry toc-h4"><a href="#bayesian-ab-testing-1">Bayesian A/B Testing</a></li> <li class="toc-entry toc-h4"><a href="#real-time-user-modeling">Real-Time User Modeling</a></li> <li class="toc-entry toc-h4"><a href="#bayesian-filtering-and-probabilistic-robotics">Bayesian Filtering and Probabilistic Robotics</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#gaussian-processes-intro">Gaussian Processes (Intro)</a> <ul> <li class="toc-entry toc-h3"><a href="#what-is-a-gaussian-process">What is a Gaussian Process?</a></li> <li class="toc-entry toc-h3"><a href="#gaussian-process-regression-intuition">Gaussian Process Regression: Intuition</a></li> <li class="toc-entry toc-h3"> <a href="#the-role-of-the-kernel-function">The Role of the Kernel Function</a> <ul> <li class="toc-entry toc-h4"><a href="#1-radial-basis-function-rbf-or-squared-exponential-kernel">1. Radial Basis Function (RBF) or Squared Exponential Kernel</a></li> <li class="toc-entry toc-h4"><a href="#2-matern-kernel">2. Matern Kernel</a></li> <li class="toc-entry toc-h4"><a href="#3-dot-product-linear-kernel">3. Dot Product (Linear) Kernel</a></li> <li class="toc-entry toc-h4"><a href="#4-periodic-kernel">4. Periodic Kernel</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#why-kernels-matter-in-practice">Why Kernels Matter in Practice</a></li> <li class="toc-entry toc-h3"><a href="#visualization-4">Visualization</a></li> <li class="toc-entry toc-h3"> <a href="#applications-in-data-science-4">Applications in Data Science</a> <ul> <li class="toc-entry toc-h4"><a href="#bayesian-optimization">Bayesian Optimization</a></li> <li class="toc-entry toc-h4"><a href="#uncertainty-aware-regression">Uncertainty-Aware Regression</a></li> <li class="toc-entry toc-h4"><a href="#small-data-regimes">Small-Data Regimes</a></li> <li class="toc-entry toc-h4"><a href="#active-learning">Active Learning</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"><a href="#bayesian-foundations-in-modern-data-science">Bayesian Foundations in Modern Data Science</a></li> </ul> </div> <hr> <div id="markdown-content"> <figure style="text-align: center;"> <img src="https://upload.wikimedia.org/wikipedia/commons/b/bf/Winslow_Homer_-_The_Gulf_Stream_-_Metropolitan_Museum_of_Art.jpg" alt="The Gulf Stream (painting)" style="max-width: 100%; height: auto;"> <figcaption><em>The Gulf Stream – Winslow Homer (1899)</em></figcaption> </figure> <p>In data science, where uncertainty is not an exception but the norm, reasoning under uncertainty becomes a core necessity. While traditional frequentist approaches have long provided a framework for estimating parameters and testing hypotheses, the Bayesian paradigm brings an alternative—and in many ways, more intuitive—framework to model beliefs, incorporate prior knowledge, and update our understanding as new data arrives.</p> <p>Bayesian inference treats unknown parameters as random variables and uses probability distributions to express uncertainty. This philosophical shift opens the door to a rich array of techniques and tools that power everything from spam filters to hyperparameter tuning in deep learning.</p> <hr> <h2 id="bayes-theorem-and-conditional-probability">Bayes’ Theorem and Conditional Probability</h2> <p>Bayes’ Theorem is a foundational result in probability theory and statistics that relates conditional probabilities. Given two events \(A\) and \(B\) with \(P(B) &gt; 0\), Bayes’ Theorem states:</p> \[P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}\] <p>In this formulation:</p> <ul> <li>\(P(A)\) is the prior probability of \(A\), reflecting our initial belief before observing \(B\).</li> <li>\(P(B \mid A)\) is the likelihood of observing \(B\) given \(A\).</li> <li>\(P(B)\) is the marginal probability of \(B\), integrated over all possibilities.</li> <li>\(P(A \mid B)\) is the posterior probability: our updated belief about \(A\) after observing \(B\).</li> </ul> <p>This theorem is derived directly from the definition of conditional probability:</p> \[P(A \cap B) = P(A \mid B) P(B) = P(B \mid A) P(A)\] <p>Rearranging gives:</p> \[P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}\] <p>In Bayesian statistics, this result is used to update beliefs about unknown parameters in light of new data. For continuous parameters, the theorem generalizes to:</p> \[P(\theta \mid D) = \frac{P(D \mid \theta) P(\theta)}{P(D)}\] <p>Where:</p> <ul> <li>\(\theta\) is a parameter.</li> <li>\(D\) is observed data.</li> <li>\(P(\theta)\) is the prior distribution.</li> <li>\(P(D \mid \theta)\) is the likelihood.</li> <li>\(P(\theta \mid D)\) is the posterior distribution.</li> <li>\(P(D) = \int P(D \mid \theta) P(\theta) d\theta\) is the evidence or marginal likelihood.</li> </ul> <hr> <h3 id="example-diagnostic-testing">Example: Diagnostic Testing</h3> <p>Suppose a rare disease affects 1% of the population. A diagnostic test has:</p> <ul> <li>Sensitivity (true positive rate): 99%</li> <li>Specificity (true negative rate): 95%</li> </ul> <p>We want to compute the probability that a person has the disease given a positive test result.</p> <p>Let \(D\) denote having the disease, and \(T\) denote a positive test. Then:</p> \[P(D \mid T) = \frac{P(T \mid D) P(D)}{P(T)} = \frac{0.99 \cdot 0.01}{0.99 \cdot 0.01 + 0.05 \cdot 0.99} \approx 0.167\] <p>So despite a highly accurate test, the probability of truly having the disease given a positive test result is only about 16.7%. This demonstrates the importance of the prior (base rate) in interpreting diagnostic results.</p> <hr> <h3 id="python-code-for-the-example">Python Code for the Example</h3> <pre><code class="language-Python"># Bayesian disease diagnosis
P_disease = 0.01
P_pos_given_disease = 0.99
P_pos_given_no_disease = 0.05

P_no_disease = 1 - P_disease
P_pos = P_pos_given_disease * P_disease + P_pos_given_no_disease * P_no_disease
P_disease_given_pos = (P_pos_given_disease * P_disease) / P_pos
print(f"Posterior probability: {P_disease_given_pos:.4f}")
</code></pre> <hr> <h3 id="visualization">Visualization</h3> <p>The following visualization generalizes the example by showing how the posterior probability changes as the prior (disease prevalence) varies.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">prior_probs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">sensitivity</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">specificity</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">false_positive</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">specificity</span>

<span class="n">posterior_probs</span> <span class="o">=</span> <span class="p">(</span><span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior_probs</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
    <span class="n">sensitivity</span> <span class="o">*</span> <span class="n">prior_probs</span> <span class="o">+</span> <span class="n">false_positive</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">prior_probs</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">prior_probs</span><span class="p">,</span> <span class="n">posterior_probs</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">P(Disease | Positive Test)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Prior Probability of Disease</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Bayesian Update: Disease Diagnosis</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center; margin: 2rem 0;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/probstat3_2-480.webp 480w,/assets/img/probstat3_2-800.webp 800w,/assets/img/probstat3_2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/probstat3_2.png" class="img-fluid rounded shadow-sm" width="100%" height="auto" alt="Posterior Probability Visualization" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="font-style: italic; font-size: 0.95rem; color: #666; margin-top: 0.5rem;"> Figure: Posterior Probability vs Prior for Varying Specificity </p> </div> <p>This plot illustrates how the posterior probability of having a disease—after receiving a <strong>positive test result</strong>—varies based on two factors:</p> <ul> <li>The <strong>prior probability</strong> of the disease (x-axis), which corresponds to its prevalence in the population,</li> <li>And the <strong>specificity</strong> of the diagnostic test, or how well the test avoids false positives (multiple curves).</li> </ul> <p>All curves assume a fixed <strong>sensitivity</strong> of 99% (i.e., the test correctly identifies almost all diseased cases).</p> <ul> <li> <p>When the <strong>disease is rare</strong> (e.g., prior probability below 1%), even a <strong>highly accurate test may produce a low posterior</strong>. This is because <strong>false positives dominate</strong> the marginal probability of a positive result at low prevalence.</p> </li> <li> <p>As the <strong>prior probability increases</strong> (e.g., testing a high-risk group), the <strong>posterior probability increases sharply</strong>. This reflects how more confidence in the disease’s presence in the population strengthens the update from a positive result.</p> </li> <li> <p>The <strong>higher the test specificity</strong>, the steeper the curve—and the stronger the belief update. With 99% specificity, a positive test leads to much higher posterior probabilities compared to 90% specificity, especially when the prior is low.</p> </li> </ul> <hr> <h3 id="applications-in-data-science">Applications in Data Science</h3> <p>Bayes’ Theorem is a pillar in many areas of applied machine learning and data science:</p> <ul> <li> <strong>Naive Bayes Classifiers</strong>: Used in email spam detection, text classification, and sentiment analysis.</li> <li> <strong>Medical Diagnostic Systems</strong>: Estimate disease probabilities as symptoms and test results accumulate.</li> <li> <strong>Bayesian A/B Testing</strong>: Provides posterior distributions over conversion rates instead of binary conclusions.</li> <li> <strong>Credit Scoring and Fraud Detection</strong>: Updates risk estimates in real-time based on user behavior.</li> </ul> <p>It provides a mathematically sound, interpretable, and adaptive approach to reasoning and decision-making under uncertainty.</p> <h2 id="prior-likelihood-and-posterior">Prior, Likelihood, and Posterior</h2> <p>In the Bayesian framework, statistical inference is built on the principle of updating <strong>beliefs about parameters</strong> based on observed data. This belief is represented mathematically using probability distributions and updated using <strong>Bayes’ Theorem</strong>. The process relies on three central components:</p> <ul> <li>The <strong>prior</strong>: our belief about the parameter before seeing data,</li> <li>The <strong>likelihood</strong>: the probability of observing the data given a specific value of the parameter,</li> <li>The <strong>posterior</strong>: the revised belief after combining the prior and the likelihood.</li> </ul> <p>Let us denote the unknown parameter by \(\theta\), and the observed data by \(D = \{x_1, x_2, \dots, x_n\}\). Then:</p> \[P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}\] <p>This is Bayes’ Theorem applied to parameter estimation, where:</p> <ul> <li>\(P(\theta)\) is the <strong>prior</strong> distribution,</li> <li>\(P(D \mid \theta)\) is the <strong>likelihood</strong>,</li> <li>\(P(D)\) is the <strong>marginal likelihood</strong> or <strong>evidence</strong>,</li> <li>\(P(\theta \mid D)\) is the <strong>posterior</strong>.</li> </ul> <hr> <h3 id="prior-distribution">Prior Distribution</h3> <p>The <strong>prior distribution</strong> \(P(\theta)\) expresses our belief or uncertainty about the parameter \(\theta\) before observing any data. Mathematically, the prior is a probability density function (pdf) over the domain of \(\theta\):</p> \[\int P(\theta) \, d\theta = 1\] <p>This prior may be:</p> <ul> <li> <strong>Informative</strong>: When strong domain knowledge is available.</li> <li> <strong>Uninformative or weakly informative</strong>: To allow data to dominate inference.</li> <li> <strong>Subjective</strong>: Based on expert intuition or empirical insight.</li> </ul> <p>The specific form of the prior depends on the nature of the parameter:</p> <h4 id="-beta-prior-for-probabilities-theta-in-0-1">▸ Beta Prior (for probabilities \(\theta \in [0, 1]\)):</h4> \[P(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\] <p>Where \(\alpha, \beta &gt; 0\) are shape parameters, and \(B(\alpha, \beta)\) is the <strong>Beta function</strong>:</p> \[B(\alpha, \beta) = \int_0^1 \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} \, d\theta\] <p>This distribution is commonly used as a prior for binary outcomes and proportions. Its mean and variance are:</p> \[\mathbb{E}[\theta] = \frac{\alpha}{\alpha + \beta}, \quad \text{Var}(\theta) = \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}\] <h4 id="-gaussian-prior-for-real-valued-theta">▸ Gaussian Prior (for real-valued \(\theta\)):</h4> \[P(\theta) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left( -\frac{(\theta - \mu)^2}{2\sigma^2} \right)\] <p>Used in models like Bayesian linear regression or Gaussian processes, this prior expresses belief that \(\theta\) is centered around \(\mu\) with spread controlled by variance \(\sigma^2\).</p> <h4 id="-uniform-prior-non-informative">▸ Uniform Prior (non-informative):</h4> <p>If nothing is known about \(\theta\) within an interval \([a, b]\):</p> \[P(\theta) = \frac{1}{b - a}, \quad \text{for } \theta \in [a, b]\] <p>This flat prior assumes all values in \([a, b]\) are equally likely.</p> <hr> <h3 id="likelihood-function">Likelihood Function</h3> <p>The <strong>likelihood function</strong> \(P(D \mid \theta)\) represents how plausible the observed data is for different values of the parameter \(\theta\). While the prior is independent of the data and reflects belief, the likelihood is derived from a <strong>data-generating model</strong>—a statistical assumption about how the data arises conditional on \(\theta\).</p> <p>Formally, if the data consists of independent and identically distributed (i.i.d.) samples:</p> \[D = \{x_1, x_2, \dots, x_n\}, \quad x_i \sim P(x \mid \theta)\] <p>Then the likelihood becomes:</p> \[P(D \mid \theta) = \prod_{i=1}^n P(x_i \mid \theta)\] <p>This is <strong>not</strong> a probability distribution over \(\theta\), but a function of \(\theta\) with data held fixed.</p> <hr> <h4 id="-example-bernoullibinomial-likelihood">▸ Example: Bernoulli/Binomial Likelihood</h4> <p>Suppose each observation is a binary outcome (success/failure), modeled as a Bernoulli trial:</p> \[x_i \sim \text{Bernoulli}(\theta), \quad \text{so } P(x_i \mid \theta) = \theta^{x_i} (1 - \theta)^{1 - x_i}\] <p>If we observe \(k\) successes in \(n\) trials, then:</p> \[P(D \mid \theta) = \prod_{i=1}^n \theta^{x_i} (1 - \theta)^{1 - x_i} = \theta^k (1 - \theta)^{n - k}\] <p>This expression is the <strong>likelihood function</strong>, which evaluates how consistent various \(\theta\) values are with the observed success/failure count.</p> <hr> <h3 id="posterior-distribution">Posterior Distribution</h3> <p>The <strong>posterior</strong> distribution \(P(\theta \mid D)\) combines the <strong>prior</strong> and <strong>likelihood</strong> using Bayes’ Theorem:</p> \[P(\theta \mid D) = \frac{P(D \mid \theta) \cdot P(\theta)}{P(D)}\] <p>Where the denominator is the <strong>marginal likelihood</strong> or <strong>evidence</strong>, ensuring that the posterior integrates to 1:</p> \[P(D) = \int P(D \mid \theta) \cdot P(\theta) \, d\theta\] <hr> <h4 id="-beta-prior--binomial-likelihood">▸ Beta Prior + Binomial Likelihood</h4> <p>Let’s assume:</p> <ul> <li>Prior: \(P(\theta) = \text{Beta}(\alpha, \beta)\)</li> <li>Likelihood: \(P(D \mid \theta) = \theta^k (1 - \theta)^{n - k}\)</li> </ul> <p>Then:</p> <p><strong>Unnormalized posterior</strong>:</p> \[P(\theta \mid D) \propto P(D \mid \theta) \cdot P(\theta) \\ \propto \theta^k (1 - \theta)^{n - k} \cdot \theta^{\alpha - 1}(1 - \theta)^{\beta - 1} \\ = \theta^{k + \alpha - 1}(1 - \theta)^{n - k + \beta - 1}\] <p>This is the kernel of a <strong>Beta distribution</strong>:</p> \[P(\theta \mid D) = \text{Beta}(\alpha + k, \beta + n - k)\] <p>This result highlights the convenience of <strong>conjugate priors</strong>: the prior and posterior belong to the same family, simplifying inference.</p> <hr> <h4 id="-posterior-summary-statistics---beta-distribution">▸ Posterior Summary Statistics - Beta Distribution</h4> <p>Based on the posterior:</p> \[\theta \mid D \sim \text{Beta}(\alpha + k, \beta + n - k)\] <p>where:</p> <ul> <li>\(\alpha\) and \(\beta\) are prior parameters,</li> <li>\(k\) is the number of observed successes,</li> <li>\(n\) is the total number of observations.</li> </ul> <p>We derive:</p> <h5 id="1-posterior-mean">1. <strong>Posterior Mean</strong> </h5> <p>If:</p> \[\theta \sim \text{Beta}(a, b)\] <p>then the mean is given by:</p> \[\mathbb{E}[\theta] = \frac{a}{a + b}\] <p>In our case, the posterior parameters are:</p> <ul> <li> \[a = \alpha + k\] </li> <li> \[b = \beta + n - k\] </li> </ul> <p>So the <strong>posterior mean</strong> is:</p> \[\mathbb{E}[\theta \mid D] = \frac{\alpha + k}{\alpha + \beta + n}\] <p>This is a convex combination of the prior mean and the observed frequency:</p> <ul> <li>Prior mean: \(\frac{\alpha}{\alpha + \beta}\)</li> <li>Observed frequency: \(\frac{k}{n}\)</li> </ul> <p>As \(n\) increases, the posterior mean converges toward the sample mean \(k/n\), and the influence of the prior diminishes.</p> <hr> <h5 id="2-posterior-variance">2. <strong>Posterior Variance</strong> </h5> <p>The variance of a Beta distribution \(\text{Beta}(a, b)\) is:</p> \[\text{Var}[\theta] = \frac{ab}{(a + b)^2 (a + b + 1)}\] <p>Apply this to the posterior:</p> <ul> <li> \[a = \alpha + k\] </li> <li> \[b = \beta + n - k\] </li> </ul> <p>Then:</p> \[\text{Var}[\theta \mid D] = \frac{(\alpha + k)(\beta + n - k)}{(\alpha + \beta + n)^2 (\alpha + \beta + n + 1)}\] <p>This variance shrinks as \(n\) increases — reflecting increased confidence in our estimate of \(\theta\) after observing more data.</p> <hr> <h5 id="3-map-estimate-posterior-mode">3. <strong>MAP Estimate (Posterior Mode)</strong> </h5> <p>The <strong>mode</strong> (maximum a posteriori estimate) of a Beta distribution \(\text{Beta}(a, b)\) is given by:</p> \[\theta_{\text{MAP}} = \frac{a - 1}{a + b - 2}, \quad \text{for } a &gt; 1 \text{ and } b &gt; 1\] <p>Apply to the posterior:</p> <ul> <li> \[a = \alpha + k\] </li> <li> \[b = \beta + n - k\] </li> </ul> <p>So:</p> \[\hat{\theta}_{\text{MAP}} = \frac{\alpha + k - 1}{\alpha + \beta + n - 2}\] <p>This estimate corresponds to the <strong>mode</strong> of the posterior distribution, and will differ from the mean unless the distribution is symmetric.</p> <hr> <p>These quantities are critical in:</p> <ul> <li>Computing expected outcomes and uncertainty,</li> <li>Constructing Bayesian credible intervals,</li> <li>Making point predictions (e.g., MAP for classification),</li> <li>Visualizing posterior summaries.</li> </ul> <hr> <h4 id="-posterior-derivation-gaussian-likelihood-and-gaussian-prior">▸ Posterior Derivation: Gaussian Likelihood and Gaussian Prior</h4> <p>In many data science tasks, we assume that the observed data is generated from a continuous process with <strong>Gaussian noise</strong>, and that our prior belief about the parameter is also normally distributed. This leads to one of the most well-known conjugate pairs: <strong>Gaussian-Gaussian</strong> inference (both the <strong>likelihood and prior are Gaussian</strong> — one of the most important and elegant conjugate models in Bayesian inference).</p> <p>Let us assume:</p> <ul> <li>The parameter of interest is a real-valued scalar \(\theta\).</li> <li> <p>The observed data \(D = \{x_1, x_2, \dots, x_n\}\) are i.i.d. samples from:</p> \[x_i \mid \theta \sim \mathcal{N}(\theta, \sigma^2)\] <p>where \(\sigma^2\) is known (observation noise variance).</p> </li> <li> <p>The prior belief about \(\theta\) is:</p> \[\theta \sim \mathcal{N}(\mu_0, \tau^2)\] <p>where \(\mu_0\) is the prior mean, and \(\tau^2\) is the prior variance.</p> </li> </ul> <hr> <h5 id="step-1-likelihood-function">Step 1: Likelihood Function</h5> <p>Given \(n\) i.i.d. observations \(x_1, ..., x_n\), the likelihood of the data given \(\theta\) is:</p> \[P(D \mid \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( -\frac{(x_i - \theta)^2}{2\sigma^2} \right)\] <p>This is the product of Gaussians with the same mean \(\theta\) and fixed variance \(\sigma^2\).</p> <p>Taking the log-likelihood:</p> \[\log P(D \mid \theta) = -\frac{n}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \theta)^2\] <p>Let \(\bar{x} = \frac{1}{n} \sum x_i\). Then:</p> \[\sum (x_i - \theta)^2 = \sum (x_i - \bar{x} + \bar{x} - \theta)^2 = \sum (x_i - \bar{x})^2 + n(\theta - \bar{x})^2\] <p>So the likelihood becomes:</p> \[P(D \mid \theta) \propto \exp\left( -\frac{n}{2\sigma^2} (\theta - \bar{x})^2 \right)\] <p>Which shows that the likelihood (up to normalization) is itself Gaussian:</p> \[\theta \mid D \propto \mathcal{N}(\bar{x}, \sigma^2 / n)\] <hr> <h5 id="step-2-prior">Step 2: Prior</h5> <p>The prior is:</p> \[P(\theta) = \frac{1}{\sqrt{2\pi\tau^2}} \exp\left( -\frac{(\theta - \mu_0)^2}{2\tau^2} \right)\] <hr> <h5 id="step-3-posterior-derivation">Step 3: Posterior Derivation</h5> <p>Bayes’ Theorem gives:</p> \[P(\theta \mid D) \propto P(D \mid \theta) \cdot P(\theta)\] <p>Since both terms are exponentials of quadratics in \(\theta\), their product is proportional to another Gaussian:</p> <p>Let’s expand both exponentials:</p> \[\log P(\theta \mid D) \propto -\frac{n}{2\sigma^2} (\theta - \bar{x})^2 - \frac{1}{2\tau^2} (\theta - \mu_0)^2\] <p>Combine terms:</p> \[\log P(\theta \mid D) \propto -\frac{1}{2} \left[ \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2} \right) \theta^2 - 2 \left( \frac{n\bar{x}}{\sigma^2} + \frac{\mu_0}{\tau^2} \right) \theta \right]\] <p>This is the kernel of a Gaussian distribution with:</p> <h6 id="-posterior-mean">▸ Posterior Mean:</h6> \[\mu_n = \frac{\frac{n}{\sigma^2} \bar{x} + \frac{1}{\tau^2} \mu_0}{\frac{n}{\sigma^2} + \frac{1}{\tau^2}}\] <h6 id="-posterior-variance">▸ Posterior Variance:</h6> \[\sigma_n^2 = \left( \frac{n}{\sigma^2} + \frac{1}{\tau^2} \right)^{-1}\] <hr> <h5 id="interpretation">Interpretation</h5> <ul> <li>The <strong>posterior mean</strong> \(\mu_n\) is a <strong>weighted average</strong> of the prior mean and sample mean, where the weights are proportional to their respective precisions (inverse variances).</li> <li>The <strong>posterior variance</strong> \(\sigma_n^2\) is always <strong>smaller</strong> than either the prior or the sample variance alone, reflecting increased certainty after combining information.</li> </ul> <hr> <p>This Gaussian-Gaussian model forms the mathematical foundation of many applications:</p> <ul> <li> <strong>Bayesian Linear Regression</strong>: Each weight in a regression model has a Gaussian prior and is updated analytically with Gaussian likelihoods. This allows regularization and closed-form uncertainty quantification.</li> <li> <strong>Bayesian Updating for Streaming Data</strong>: In online learning, new data incrementally shifts the posterior, making it a new prior — enabling scalable, memory-efficient learning.</li> <li> <strong>Sensor Fusion</strong>: In robotics and control systems, Bayesian Gaussian updates allow combining noisy measurements from multiple sensors to produce more confident estimates.</li> <li> <strong>Kalman Filters</strong>: A specialized form of recursive Bayesian estimation based on Gaussian distributions used in tracking and forecasting.</li> </ul> <hr> <p>Stepping back from this specific case, the same pattern—prior, likelihood, and posterior—runs through the entire Bayesian approach.</p> <ul> <li>The <strong>prior</strong> captures what we know (or assume) before seeing any data.</li> <li>The <strong>likelihood</strong> tells us how compatible the data is with different parameter values.</li> <li>The <strong>posterior</strong> combines both to give us a data-informed belief about the parameter.</li> </ul> <p>In data science, this allows:</p> <ul> <li> <strong>Incorporating prior knowledge</strong> (from previous experiments, expert belief, or regulatory constraints),</li> <li> <strong>Updating beliefs incrementally</strong> as more data becomes available,</li> <li> <strong>Quantifying uncertainty</strong> through full distributions instead of point estimates.</li> </ul> <p>This foundation underlies a wide range of Bayesian models — from Naive Bayes classifiers and probabilistic graphical models to Gaussian Processes and Bayesian neural networks.</p> <hr> <hr> <h3 id="example-inferring-coin-bias">Example: Inferring Coin Bias</h3> <p>Let us assume we want to estimate the bias \(\theta\) (probability of heads) of a coin. Suppose we observe 10 tosses and get 6 heads and 4 tails. This is a Binomial process.</p> <ul> <li> <strong>Prior</strong>: Assume \(\theta \sim \text{Beta}(2, 2)\) — a weakly informative prior reflecting fairness.</li> <li> <p><strong>Likelihood</strong>: For 6 heads out of 10 tosses,</p> \[P(D \mid \theta) = \binom{10}{6} \theta^6 (1 - \theta)^4\] </li> <li> <p><strong>Posterior</strong>: Since the Beta prior is conjugate to the Binomial likelihood, the posterior is:</p> \[\theta \mid D \sim \text{Beta}(2 + 6, 2 + 4) = \text{Beta}(8, 6)\] </li> </ul> <p>This posterior reflects our updated belief about the coin’s bias after observing the data.</p> <hr> <h3 id="visualization-1">Visualization</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># Grid of theta values
</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Prior: Beta(2, 2)
</span><span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Likelihood (up to proportionality): theta^6 * (1 - theta)^4
</span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">theta</span><span class="o">**</span><span class="mi">6</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta</span><span class="p">)</span><span class="o">**</span><span class="mi">4</span>
<span class="n">likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">trapz</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>  <span class="c1"># Normalize for plotting
</span>
<span class="c1"># Posterior: Beta(8, 6)
</span><span class="n">posterior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="c1"># Plotting
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Prior: Beta(2, 2)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Likelihood (scaled)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Posterior: Beta(8, 6)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Bayesian Update: Estimating Coin Bias</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">θ (Probability of Heads)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center; margin: 2rem 0;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/probstat3_3-480.webp 480w,/assets/img/probstat3_3-800.webp 800w,/assets/img/probstat3_3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/probstat3_3.png" class="img-fluid rounded shadow-sm" width="100%" height="auto" alt="Bayesian Update: Estimating Coin Bias" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="font-style: italic; font-size: 0.95rem; color: #666; margin-top: 0.5rem;"> Figure: Bayesian Update: Estimating Coin Bias </p> </div> <p>This visualization shows how the prior belief and the observed data interact to form a posterior that reflects both — centered slightly above 0.5 due to the 6 observed heads.</p> <hr> <h3 id="applications-in-data-science-1">Applications in Data Science</h3> <p>This prior-likelihood-posterior triad is a universal framework used across many data science workflows:</p> <h4 id="bayesian-ab-testing"><strong>Bayesian A/B Testing</strong></h4> <ul> <li>Prior: Encodes historical conversion rates for variants.</li> <li>Likelihood: Comes from observed clicks or conversions (e.g., Binomial model).</li> <li>Posterior: Used to make probabilistic comparisons like \(P(\theta_A &gt; \theta_B \mid \text{data})\).</li> </ul> <p>This leads to more robust and interpretable decisions compared to traditional p-values.</p> <h4 id="bayesian-regression"><strong>Bayesian Regression</strong></h4> <ul> <li>Prior: Places distributions (e.g., Normal) over regression coefficients.</li> <li>Likelihood: Based on residuals from training data.</li> <li>Posterior: Yields not just point estimates, but full predictive intervals — crucial for risk-aware applications like pricing, forecasting, or credit scoring.</li> </ul> <h4 id="fraud-detection"><strong>Fraud Detection</strong></h4> <ul> <li>Prior: Reflects expected fraud rate (e.g., from industry benchmarks).</li> <li>Likelihood: Comes from behavioral or transactional data.</li> <li>Posterior: Quantifies the probability of fraud for new transactions in real time.</li> </ul> <h4 id="recommender-systems"><strong>Recommender Systems</strong></h4> <ul> <li>Prior: Reflects assumed user preferences or item popularity.</li> <li>Likelihood: Derived from user-item interaction data (ratings, clicks).</li> <li>Posterior: Enables personalized predictions with uncertainty quantification, improving exploration in recommendation.</li> </ul> <hr> <p>The combination of <strong>prior beliefs</strong> and <strong>observed evidence</strong>, culminating in a <strong>posterior</strong>, provides a powerful and flexible inference engine. This Bayesian updating mechanism equips data scientists to not only make predictions but also understand their <strong>confidence</strong> in those predictions — a critical capability in domains where decisions have consequences.</p> <hr> <h2 id="maximum-likelihood-estimation-mle-vs-maximum-a-posteriori-estimation-map">Maximum Likelihood Estimation (MLE) vs. Maximum A Posteriori Estimation (MAP)</h2> <p>One of the central challenges in statistical inference is the estimation of model parameters from observed data. Two important frameworks for parameter estimation are <strong>Maximum Likelihood Estimation (MLE)</strong> and <strong>Maximum A Posteriori Estimation (MAP)</strong>. While both aim to select parameter values that explain the data well, they differ in how they incorporate prior knowledge.</p> <p>MLE derives purely from the likelihood function, whereas MAP is based on the full Bayesian posterior distribution. Their distinction becomes particularly meaningful in the presence of prior information, limited data, or regularization constraints.</p> <hr> <h3 id="mle-derivation-and-explanation">MLE: Derivation and Explanation</h3> <p>Let \(D = \{x_1, x_2, \dots, x_n\}\) be a set of i.i.d. observations drawn from a distribution parameterized by \(\theta\).</p> <p>The <strong>likelihood function</strong> is:</p> \[L(\theta \mid D) = \prod_{i=1}^{n} P(x_i \mid \theta)\] <p>Taking logs, the <strong>log-likelihood</strong> becomes:</p> \[\ell(\theta) = \log L(\theta \mid D) = \sum_{i=1}^{n} \log P(x_i \mid \theta)\] <p>The <strong>Maximum Likelihood Estimator</strong> is the value of \(\theta\) that maximizes the log-likelihood:</p> \[\hat{\theta}_{\text{MLE}} = \arg\max_\theta \, \ell(\theta)\] <hr> <h4 id="mle-derivation-bernoulli-case">MLE Derivation: Bernoulli Case</h4> <p>Suppose each \(x_i\) is a Bernoulli trial with success probability \(\theta\). Then:</p> \[P(x_i \mid \theta) = \theta^{x_i} (1 - \theta)^{1 - x_i}\] <p>So the log-likelihood becomes:</p> \[\ell(\theta) = \sum_{i=1}^n \left[ x_i \log \theta + (1 - x_i) \log(1 - \theta) \right]\] <p>Let:</p> <ul> <li>\(k = \sum_{i=1}^n x_i\) be the number of successes,</li> <li>\(n - k\) be the number of failures.</li> </ul> <p>Then:</p> \[\ell(\theta) = k \log \theta + (n - k) \log(1 - \theta)\] <p>To maximize, differentiate with respect to \(\theta\) and set the derivative to zero:</p> \[\frac{d\ell}{d\theta} = \frac{k}{\theta} - \frac{n - k}{1 - \theta} = 0\] <p>Solving:</p> \[\frac{k}{\theta} = \frac{n - k}{1 - \theta} \Rightarrow k (1 - \theta) = (n - k) \theta\] <p>Expanding:</p> \[k - k\theta = n\theta - k\theta \Rightarrow k = n\theta \Rightarrow \hat{\theta}_{\text{MLE}} = \frac{k}{n}\] <hr> <h3 id="map-derivation-and-explanation">MAP: Derivation and Explanation</h3> <p>In the Bayesian framework, we update our belief about \(\theta\) after observing \(D\) using Bayes’ Theorem:</p> \[P(\theta \mid D) = \frac{P(D \mid \theta) P(\theta)}{P(D)}\] <p>The <strong>MAP estimator</strong> is:</p> \[\hat{\theta}_{\text{MAP}} = \arg\max_\theta \, P(\theta \mid D) = \arg\max_\theta \, P(D \mid \theta) P(\theta)\] <p>Taking logarithms:</p> \[\hat{\theta}_{\text{MAP}} = \arg\max_\theta \left[ \log P(D \mid \theta) + \log P(\theta) \right]\] <p>This formulation shows that MAP estimation is equivalent to <strong>MLE with a regularization term</strong> derived from the prior.</p> <ul> <li>If \(P(\theta)\) is uniform (uninformative), then MAP reduces to MLE.</li> <li>If \(P(\theta)\) is Gaussian, the log-prior is quadratic and acts like L2 regularization.</li> </ul> <hr> <h4 id="map-derivation-bernoulli-likelihood-and-beta-prior">MAP Derivation: Bernoulli Likelihood and Beta Prior</h4> <p>Let:</p> <ul> <li>Likelihood: \(P(D \mid \theta) \propto \theta^k (1 - \theta)^{n - k}\)</li> <li>Prior: \(P(\theta) = \text{Beta}(\alpha, \beta) \propto \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\)</li> </ul> <p>Then:</p> \[P(\theta \mid D) \propto \theta^{k + \alpha - 1}(1 - \theta)^{n - k + \beta - 1}\] <p>This is a <strong>Beta posterior</strong>: \(\text{Beta}(k + \alpha, n - k + \beta)\).</p> <p>To find the MAP estimate (mode of the Beta distribution), we differentiate the log-posterior:</p> \[\log P(\theta \mid D) = (k + \alpha - 1) \log \theta + (n - k + \beta - 1) \log(1 - \theta)\] <p>Take derivative and set to zero:</p> \[\frac{d}{d\theta} \log P(\theta \mid D) = \frac{k + \alpha - 1}{\theta} - \frac{n - k + \beta - 1}{1 - \theta} = 0\] <p>Solving:</p> \[\frac{k + \alpha - 1}{\theta} = \frac{n - k + \beta - 1}{1 - \theta} \Rightarrow (k + \alpha - 1)(1 - \theta) = (n - k + \beta - 1)\theta\] <p>Expanding:</p> \[k + \alpha - 1 - (k + \alpha - 1)\theta = (n - k + \beta - 1)\theta\] <p>Move terms:</p> \[k + \alpha - 1 = \theta \left[(n - k + \beta - 1) + (k + \alpha - 1)\right] = \theta (n + \alpha + \beta - 2)\] <p>Thus, the <strong>MAP estimate is</strong>:</p> \[\hat{\theta}_{\text{MAP}} = \frac{k + \alpha - 1}{n + \alpha + \beta - 2}\] <p>When \(\alpha = \beta = 1\) (uniform prior), MAP reduces to MLE:</p> \[\hat{\theta}_{\text{MAP}} = \frac{k}{n}\] <hr> <h3 id="bernoulli-example-with-beta-prior">Bernoulli Example with Beta Prior</h3> <p>Suppose we toss a coin \(n = 5\) times and observe \(k = 3\) heads. Let \(\theta\) be the probability of heads.</p> <ul> <li> <p>The <strong>likelihood</strong> is:</p> \[P(k \mid \theta) = \binom{5}{3} \theta^3 (1 - \theta)^2 \propto \theta^3 (1 - \theta)^2\] </li> <li> <p>The <strong>prior</strong> is a Beta distribution: \(P(\theta) \propto \theta^{\alpha - 1} (1 - \theta)^{\beta - 1}\).</p> <p>Let’s use \(\text{Beta}(2, 2)\), so:</p> \[P(\theta) \propto \theta (1 - \theta)\] </li> <li> <p>The <strong>posterior</strong> is then:</p> \[P(\theta \mid D) \propto \theta^3 (1 - \theta)^2 \cdot \theta (1 - \theta) = \theta^4 (1 - \theta)^3\] </li> </ul> <p>This corresponds to a <strong>Beta(5, 4)</strong> posterior.</p> <ul> <li> <p>The <strong>MLE</strong> is:</p> \[\hat{\theta}_{\text{MLE}} = \frac{k}{n} = \frac{3}{5} = 0.6\] </li> <li> <p>The <strong>MAP</strong> estimate (mode of Beta(5,4)) is:</p> \[\hat{\theta}_{\text{MAP}} = \frac{5 - 1}{5 + 4 - 2} = \frac{4}{7} \approx 0.571\] </li> </ul> <hr> <h3 id="visualization-2">Visualization</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">theta_vals</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Likelihood (up to constant)
</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">theta_vals</span><span class="o">**</span><span class="n">k</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">theta_vals</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="n">n</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">/=</span> <span class="n">np</span><span class="p">.</span><span class="nf">trapz</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">theta_vals</span><span class="p">)</span>

<span class="c1"># Prior: Beta(2, 2)
</span><span class="n">prior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Posterior: Beta(5, 4)
</span><span class="n">posterior</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># Estimates
</span><span class="n">theta_mle</span> <span class="o">=</span> <span class="n">k</span> <span class="o">/</span> <span class="n">n</span>
<span class="n">theta_map</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">5</span> <span class="o">+</span> <span class="mi">4</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Likelihood (MLE)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Prior: Beta(2,2)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta_vals</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Posterior: Beta(5,4)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">theta_mle</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">MLE: </span><span class="si">{</span><span class="n">theta_mle</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axvline</span><span class="p">(</span><span class="n">theta_map</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">MAP: </span><span class="si">{</span><span class="n">theta_map</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">MAP vs MLE Estimation for Coin Bias</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">θ (Probability of Heads)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>The plot below demonstrates how <strong>Maximum Likelihood Estimation (MLE)</strong> and <strong>Maximum A Posteriori (MAP)</strong> estimation differ when estimating the bias \(\theta\) of a coin (i.e., the probability of getting heads).</p> <p>The goal is to estimate the most likely value of \(\theta\) based on:</p> <ul> <li>A prior belief about the coin’s fairness (a Beta distribution),</li> <li>A small sample of observed data (3 heads out of 5 tosses).</li> </ul> <div style="text-align: center; margin: 2rem 0;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/probstat3_4-480.webp 480w,/assets/img/probstat3_4-800.webp 800w,/assets/img/probstat3_4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/probstat3_4.png" class="img-fluid rounded shadow-sm" width="100%" height="auto" alt="MAP vs MLE Estimation for Coin Bias" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="font-style: italic; font-size: 0.95rem; color: #666; margin-top: 0.5rem;"> Figure: MAP vs MLE Estimation for Coin Bias </p> </div> <ul> <li> <strong>Prior</strong>: A \(\text{Beta}(2,2)\) distribution, centered at 0.5, representing a <strong>mild belief</strong> that the coin is fair.</li> <li> <strong>Observed Data</strong>: 5 tosses, with <strong>3 heads</strong> and <strong>2 tails</strong>.</li> <li> <p><strong>Likelihood</strong>: Based on the Binomial model:</p> \[P(D \mid \theta) \propto \theta^3 (1 - \theta)^2\] </li> <li> <p><strong>Posterior</strong>: With a conjugate Beta prior and Binomial likelihood, the posterior becomes:</p> \[\text{Beta}(\alpha + k, \beta + n - k) = \text{Beta}(5, 4)\] </li> </ul> <h3 id="inferences-we-can-make">Inferences we can make:</h3> <ol> <li> <p><strong>MLE Ignores Prior Knowledge</strong>:<br> The likelihood peaks at \(\theta = 0.6\)—which is simply the empirical ratio \(k/n = 3/5\). This is the MLE and represents the estimate <strong>purely from data</strong>.</p> </li> <li> <p><strong>MAP Blends Prior and Data</strong>:<br> The posterior peaks at around \(\theta = 0.571\). The MAP estimate is slightly <strong>pulled toward the prior mean (0.5)</strong> compared to MLE. This reflects the influence of the prior, which assumed the coin is more likely to be fair.</p> </li> <li> <p><strong>Priors Act as Regularizers</strong>:<br> The MAP estimator essentially acts like a <strong>regularized version of MLE</strong>—biasing the estimate toward prior beliefs, especially when sample size is small. With more data, the MAP and MLE would converge.</p> </li> <li> <p><strong>Posterior Reflects Uncertainty More Holistically</strong>:<br> Compared to the sharper likelihood, the posterior incorporates both <strong>data and prior uncertainty</strong>, making it slightly wider and smoother—especially relevant in low-data settings.</p> </li> <li> <p><strong>MAP ≠ MLE When Priors Are Informative</strong>:<br> This visualization is a concrete demonstration of how <strong>MAP ≠ MLE</strong> when the prior is not flat. It’s a critical concept when explaining regularization, Bayesian learning, or when modeling with limited data.</p> </li> </ol> <hr> <p>To summarize, this plot provides a visual comparison of two common estimation strategies:</p> <ul> <li> <strong>MLE</strong>: Trusts only the data.</li> <li> <strong>MAP</strong>: Trusts both the data and a prior belief.</li> </ul> <p>When data is scarce (as it often is in real-world applications), the regularization effect of the prior becomes particularly useful. Bayesian methods provide a principled way to implement this regularization through <strong>posterior inference</strong>, and this example makes that visible and intuitive.</p> <hr> <h3 id="applications-in-data-science-2">Applications in Data Science</h3> <h4 id="logistic-regression">Logistic Regression</h4> <p>MLE finds weights that minimize the negative log-likelihood. However, MAP estimation adds a prior over the weights (usually Gaussian), which leads to <strong>regularized logistic regression</strong>:</p> \[\hat{w}_{\text{MAP}} = \arg\min_w \left[ \sum_i \log(1 + e^{-y_i x_i^T w}) + \frac{\lambda}{2} \|w\|^2 \right]\] <p>This helps control overfitting, especially in high-dimensional spaces.</p> <h4 id="bayesian-linear-regression">Bayesian Linear Regression</h4> <p>In linear regression, MAP estimation with a Gaussian prior yields <strong>Ridge regression</strong>:</p> \[\hat{\beta}_{\text{MAP}} = \arg\min_{\beta} \left[ \|y - X\beta\|^2 + \lambda \|\beta\|^2 \right]\] <p>This provides stability when data is sparse or multicollinearity is present.</p> <h4 id="cold-start-and-sparse-data-problems">Cold Start and Sparse Data Problems</h4> <p>MAP estimators are essential when:</p> <ul> <li>Data is limited (e.g., few clicks or ratings per user).</li> <li>You want to encode prior beliefs (e.g., users prefer popular items).</li> <li>You want robust, regularized predictions rather than overfitting.</li> </ul> <p>MAP allows us to “pull” parameter estimates towards prior expectations when data is insufficient, making it highly useful in recommender systems and early-stage modeling.</p> <hr> <h2 id="conjugate-priors">Conjugate Priors</h2> <p>One of the elegant outcomes in Bayesian inference is that certain choices of priors lead to mathematically convenient posteriors. When a prior and its corresponding posterior distribution belong to the same family, the prior is said to be a <strong>conjugate prior</strong> for the likelihood function.</p> <p>This property is not just algebraic elegance — it enables closed-form updates, analytical tractability, and efficient implementation in sequential or real-time inference systems.</p> <hr> <h3 id="formal-definition">Formal Definition</h3> <p>A prior distribution \(P(\theta)\) is said to be <strong>conjugate</strong> to a likelihood function \(P(D \mid \theta)\) if the posterior \(P(\theta \mid D)\) is in the same family of distributions as the prior.</p> <p>That is:</p> \[\text{If } P(\theta) \in \mathcal{F} \text{ and } P(\theta \mid D) \in \mathcal{F} \text{ as well, then } P(\theta) \text{ is conjugate.}\] <p>Some classic conjugate prior–likelihood pairs include:</p> <table> <thead> <tr> <th style="text-align: left">Likelihood</th> <th>Conjugate Prior</th> <th style="text-align: right">Posterior</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Bernoulli/Binomial</td> <td>Beta</td> <td style="text-align: right">Beta</td> </tr> <tr> <td style="text-align: left">Poisson</td> <td>Gamma</td> <td style="text-align: right">Gamma</td> </tr> <tr> <td style="text-align: left">Normal (known variance)</td> <td>Normal</td> <td style="text-align: right">Normal</td> </tr> <tr> <td style="text-align: left">Multinomial</td> <td>Dirichlet</td> <td style="text-align: right">Dirichlet</td> </tr> </tbody> </table> <hr> <h3 id="why-it-matters">Why It Matters</h3> <p>Conjugate priors greatly simplify Bayesian analysis. When using a conjugate prior:</p> <ul> <li>Posterior distributions can be derived analytically.</li> <li>Bayesian updating can be done incrementally and efficiently.</li> <li>The form of the prior helps encode domain knowledge (e.g., belief in fairness, expected rates, etc.)</li> </ul> <p>This is particularly useful in low-latency systems like online learning, A/B testing pipelines, and probabilistic graphical models where recomputation must be fast.</p> <hr> <h3 id="example-beta-prior-for-a-bernoullibinomial-likelihood">Example: Beta Prior for a Bernoulli/Binomial Likelihood</h3> <p>Suppose we are modeling a binary outcome — say, a coin flip. The outcome is modeled as a <strong>Bernoulli process</strong> with unknown probability of success \(\theta\).</p> <h4 id="likelihood">Likelihood</h4> <p>Let \(x_1, \dots, x_n\) be i.i.d. Bernoulli trials with parameter \(\theta\). The likelihood is:</p> \[P(D \mid \theta) = \prod_{i=1}^{n} \theta^{x_i}(1 - \theta)^{1 - x_i}\] <p>Let \(k = \sum x_i\) (number of successes), so:</p> \[P(D \mid \theta) = \theta^k (1 - \theta)^{n - k}\] <h4 id="prior">Prior</h4> <p>We choose a <strong>Beta prior</strong>:</p> \[P(\theta) = \frac{1}{B(\alpha, \beta)} \theta^{\alpha - 1}(1 - \theta)^{\beta - 1}\] <p>Where \(\alpha, \beta &gt; 0\), and \(B(\alpha, \beta)\) is the Beta function:</p> \[B(\alpha, \beta) = \int_0^1 t^{\alpha - 1} (1 - t)^{\beta - 1} dt\] <h4 id="posterior">Posterior</h4> <p>Using Bayes’ theorem:</p> \[P(\theta \mid D) \propto P(D \mid \theta) \cdot P(\theta) = \theta^k (1 - \theta)^{n - k} \cdot \theta^{\alpha - 1} (1 - \theta)^{\beta - 1} = \theta^{\alpha + k - 1} (1 - \theta)^{\beta + n - k - 1}\] <p>Thus, the posterior is:</p> \[P(\theta \mid D) = \text{Beta}(\alpha + k, \beta + n - k)\] <p>This clean and efficient update rule makes the Beta distribution the conjugate prior for the Bernoulli and Binomial likelihood.</p> <hr> <h3 id="interpretation-1">Interpretation</h3> <p>Each time we observe new data in the form of successes and failures (e.g., from a Bernoulli or Binomial process), we can update the parameters of the Beta prior in a simple, additive way. This is one of the most intuitive benefits of conjugate priors.</p> <p>If the prior is \(\text{Beta}(\alpha, \beta)\), and we observe:</p> <ul> <li>\(k\) successes,</li> <li>\(n - k\) failures,</li> </ul> <p>then the posterior becomes \(\text{Beta}(\alpha', \beta')\), where:</p> \[\alpha' = \alpha + k\] \[\beta' = \beta + (n - k)\] <p>This rule makes it easy to perform sequential updates — as new observations come in, we can incrementally revise our posterior without needing to recompute from scratch. It’s especially valuable in streaming, online learning, and real-time probabilistic systems.</p> <hr> <h3 id="visualization-3">Visualization</h3> <p>To better understand how conjugate priors simplify Bayesian updating, let’s visualize how a <strong>Beta prior</strong> gets updated after observing data from a <strong>Binomial process</strong>. In this example, we assume a weakly informative prior belief about a coin’s fairness (Beta(2, 2)) and then observe 10 coin tosses with 7 heads. Because the Beta distribution is conjugate to the Binomial likelihood, we can compute the posterior analytically—resulting in another Beta distribution with updated parameters. This makes it easy to see how the prior and data interact to form the posterior.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="c1"># Observed data: 7 successes in 10 trials
</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">failures</span> <span class="o">=</span> <span class="n">n</span> <span class="o">-</span> <span class="n">k</span>

<span class="c1"># Prior: Beta(2, 2)
</span><span class="n">alpha_prior</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="mi">2</span>

<span class="c1"># Posterior: Beta(2 + 7, 2 + 3) = Beta(9, 5)
</span><span class="n">alpha_post</span> <span class="o">=</span> <span class="n">alpha_prior</span> <span class="o">+</span> <span class="n">k</span>
<span class="n">beta_post</span> <span class="o">=</span> <span class="n">beta_prior</span> <span class="o">+</span> <span class="n">failures</span>

<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">prior_pdf</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha_prior</span><span class="p">,</span> <span class="n">beta_prior</span><span class="p">)</span>
<span class="n">posterior_pdf</span> <span class="o">=</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">alpha_post</span><span class="p">,</span> <span class="n">beta_post</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">prior_pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Prior: Beta(2, 2)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">posterior_pdf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">"</span><span class="s">Posterior: Beta(9, 5)</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Posterior Update with Conjugate Beta Prior</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">θ (Probability of Success)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Density</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <p>This visualization shows how the prior belief (centered at 0.5) gets updated based on data favoring higher success probability.</p> <div style="text-align: center; margin: 2rem 0;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/probstat3_5-480.webp 480w,/assets/img/probstat3_5-800.webp 800w,/assets/img/probstat3_5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/probstat3_5.png" class="img-fluid rounded shadow-sm" width="100%" height="auto" alt="Posterior Update with Conjugate Beta Prior" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="font-style: italic; font-size: 0.95rem; color: #666; margin-top: 0.5rem;"> Figure: Posterior Update with Conjugate Beta Prior </p> </div> <p>This plot compares two distributions:</p> <ul> <li>The <strong>prior</strong>: \(\text{Beta}(2, 2)\) — symmetric around 0.5, representing mild uncertainty about the coin being fair.</li> <li>The <strong>posterior</strong>: \(\text{Beta}(9, 5)\) — updated belief after observing <strong>7 heads out of 10 flips</strong>.</li> </ul> <h4 id="whats-happening-under-the-hood">What’s Happening Under the Hood:</h4> <ul> <li>The <strong>prior parameters</strong> \(\alpha = 2, \beta = 2\) represent 1 prior success and 1 prior failure (Beta counts start from one).</li> <li> <p>After observing the data, we apply the conjugate update:</p> \[\alpha_{\text{posterior}} = \alpha_{\text{prior}} + k = 2 + 7 = 9\] \[\beta_{\text{posterior}} = \beta_{\text{prior}} + (n - k) = 2 + 3 = 5\] </li> <li>The posterior is thus \(\text{Beta}(9, 5)\) — a distribution that favors \(\theta &gt; 0.5\), aligning with the observed data, but still shaped by the prior.</li> </ul> <p>The Plot Shows:</p> <ul> <li>The <strong>prior curve</strong> is flat-ish and centered at 0.5, reflecting openness to a range of \(\theta\) values.</li> <li>The <strong>posterior curve</strong> is steeper and shifted right, peaking around \(\theta \approx 0.64\).</li> <li>The update is <strong>not overconfident</strong> — the posterior still acknowledges uncertainty but now leans toward higher \(\theta\) based on evidence.</li> </ul> <h4 id="inferences-to-make">Inferences to make:</h4> <ol> <li> <p><strong>Bayesian updating is additive and intuitive</strong>:<br> With conjugate priors, updating is just a matter of adjusting counts. This keeps the math clean and interpretability high.</p> </li> <li> <p><strong>The prior influences the posterior more when data is limited</strong>:<br> In this example, with only 10 trials, the prior still has a noticeable effect. Had we used Beta(1, 1), the posterior would shift even further toward the empirical proportion (0.7).</p> </li> <li> <p><strong>Posterior reflects a refined belief</strong>:<br> The posterior balances <strong>prior belief and observed data</strong>, yielding a distribution that’s sharper than the prior but not as sharp as the maximum likelihood would suggest.</p> </li> <li> <p><strong>This approach scales well</strong>:<br> The same logic applies whether you’re flipping a coin, testing an email subject line, or modeling click-through rates. That’s why Beta-Binomial updates are widely used in <strong>A/B testing</strong>, <strong>online learning</strong>, and <strong>Bayesian filtering</strong>.</p> </li> </ol> <hr> <h3 id="applications-in-data-science-3">Applications in Data Science</h3> <h4 id="bayesian-ab-testing-1">Bayesian A/B Testing</h4> <p>In online experimentation:</p> <ul> <li>Each version has a Beta prior over its conversion rate.</li> <li>New data updates the posterior in real-time.</li> <li>This allows comparing \(P(\theta_A &gt; \theta_B \mid \text{data})\) directly, unlike frequentist p-values.</li> </ul> <p>Conjugate priors enable this with fast, closed-form updates and interpretable distributions.</p> <h4 id="real-time-user-modeling">Real-Time User Modeling</h4> <p>Click-through rates, open rates, fraud likelihoods — all modeled as binary outcomes. Beta priors can be updated on-the-fly as new data arrives, powering systems like:</p> <ul> <li>Dynamic personalization</li> <li>Spam filtering</li> <li>Risk scoring in transactions</li> </ul> <h4 id="bayesian-filtering-and-probabilistic-robotics">Bayesian Filtering and Probabilistic Robotics</h4> <p>In robotics and control systems, conjugate priors are used for recursive Bayesian filters (e.g., Kalman filters, where Gaussians are conjugate to themselves) to update beliefs about position, velocity, or sensor noise.</p> <hr> <p>Conjugate priors marry theory and practice in Bayesian modeling. They offer a principled way to integrate domain knowledge, perform fast posterior updates, and maintain mathematical elegance — making them an indispensable tool in the probabilistic data scientist’s toolkit.</p> <hr> <h2 id="gaussian-processes-intro">Gaussian Processes (Intro)</h2> <p>Gaussian Processes (GPs) offer a powerful and flexible framework for <strong>non-parametric Bayesian modeling</strong>. Unlike traditional models that learn a finite set of parameters (like coefficients in linear regression), GPs treat inference as learning a <strong>distribution over functions</strong>.</p> <p>This makes them ideal when you not only want to predict a value but also <strong>quantify uncertainty</strong> about that prediction — a crucial requirement in safety-critical applications such as medical diagnostics, robotics, and autonomous systems.</p> <hr> <h3 id="what-is-a-gaussian-process">What is a Gaussian Process?</h3> <p>A <strong>Gaussian Process</strong> is a collection of random variables, any finite number of which have a joint Gaussian distribution. It is completely specified by:</p> <ul> <li>A <strong>mean function</strong> \(m(x)\), and</li> <li>A <strong>covariance function</strong> or <strong>kernel</strong> \(k(x, x')\).</li> </ul> <p>Formally, we write:</p> \[f(x) \sim \mathcal{GP}(m(x), k(x, x'))\] <p>Where:</p> <ul> <li> \[m(x) = \mathbb{E}[f(x)]\] </li> <li> \[k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\] </li> </ul> <p>This means that for any finite set of inputs \(x_1, \dots, x_n\), the function values \(f(x_1), \dots, f(x_n)\) follow a multivariate normal distribution:</p> \[[f(x_1), \dots, f(x_n)]^\top \sim \mathcal{N}(\mu, K)\] <p>Where:</p> <ul> <li> \[\mu_i = m(x_i)\] </li> <li> \[K_{ij} = k(x_i, x_j)\] </li> </ul> <hr> <h3 id="gaussian-process-regression-intuition">Gaussian Process Regression: Intuition</h3> <p>Let us consider a regression problem where we are given a dataset of \(n\) observed input-output pairs:</p> \[D = \{(x_i, y_i)\}_{i=1}^n\] <p>We assume the outputs are generated from an <strong>unknown latent function</strong> \(f(x)\) with added Gaussian noise:</p> \[y_i = f(x_i) + \epsilon_i, \quad \epsilon_i \sim \mathcal{N}(0, \sigma_n^2)\] <p>Our goal is to learn about \(f(x)\) — not as a fixed parametric model, but as a <strong>distribution over possible functions</strong>. This is where <strong>Gaussian Processes</strong> come into play.</p> <p>A <strong>Gaussian Process (GP)</strong> is a prior over functions such that any finite collection of function values follows a multivariate Gaussian distribution:</p> \[f(x) \sim \mathcal{GP}(m(x), k(x, x'))\] <p>Here:</p> <ul> <li>\(m(x) = \mathbb{E}[f(x)]\) is the <strong>mean function</strong>, often set to zero for simplicity.</li> <li>\(k(x, x') = \mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]\) is the <strong>kernel</strong> or covariance function.</li> </ul> <p>Given training inputs \(X = [x_1, ..., x_n]\) and outputs \(\mathbf{y} = [y_1, ..., y_n]^T\), and test inputs \(X_*\), the GP framework models the joint distribution of training and test outputs as:</p> \[\begin{bmatrix} \mathbf{y} \\ \mathbf{f}_* \end{bmatrix} \sim \mathcal{N} \left( \begin{bmatrix} \mathbf{0} \\ \mathbf{0} \end{bmatrix}, \begin{bmatrix} K(X, X) + \sigma_n^2 I &amp; K(X, X_*) \\ K(X_*, X) &amp; K(X_*, X_*) \end{bmatrix} \right)\] <p>Where:</p> <ul> <li>\(K(X, X)\) is the \(n \times n\) covariance matrix for training inputs.</li> <li>\(K(X, X_*)\) is the \(n \times m\) cross-covariance between training and test inputs.</li> <li>\(K(X_*, X_*)\) is the \(m \times m\) covariance of the test inputs.</li> <li>\(\sigma_n^2 I\) adds noise to the diagonal (due to the assumed observational noise).</li> </ul> <p>The <strong>posterior predictive distribution</strong> for the function values at test points is then given by:</p> \[\mathbf{f}_* \mid X, \mathbf{y}, X_* \sim \mathcal{N}(\mu_*, \Sigma_*)\] <p>Where:</p> <ul> <li> <p><strong>Posterior mean</strong>:</p> \[\mu_* = K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1} \mathbf{y}\] </li> <li> <p><strong>Posterior covariance</strong>:</p> \[\Sigma_* = K(X_*, X_*) - K(X_*, X)[K(X, X) + \sigma_n^2 I]^{-1} K(X, X_*)\] </li> </ul> <p>This formulation gives us both predictions (mean) and uncertainty (variance) at any set of new inputs.</p> <hr> <h3 id="the-role-of-the-kernel-function">The Role of the Kernel Function</h3> <p>The <strong>kernel function</strong> \(k(x, x')\) defines the covariance structure between the function values at different inputs. It encodes prior assumptions about the function’s properties — smoothness, periodicity, linearity, etc. The choice of kernel is crucial as it <strong>determines the shape of the functions the GP considers likely</strong>.</p> <p>Here are some commonly used kernels:</p> <hr> <h4 id="1-radial-basis-function-rbf-or-squared-exponential-kernel">1. <strong>Radial Basis Function (RBF) or Squared Exponential Kernel</strong> </h4> <p>This is the most widely used kernel due to its universal approximation properties and smoothness:</p> \[k(x, x') = \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)\] <ul> <li>\(\ell\) is the <strong>length scale</strong>, controlling how quickly the function varies.</li> <li>Encourages <strong>infinitely differentiable</strong>, smooth functions.</li> <li>Implies that points closer in input space have highly correlated function values.</li> </ul> <hr> <h4 id="2-matern-kernel">2. <strong>Matern Kernel</strong> </h4> <p>A generalization of the RBF kernel that allows for less smoothness:</p> \[k_\nu(r) = \frac{2^{1-\nu}}{\Gamma(\nu)} \left( \frac{\sqrt{2\nu}r}{\ell} \right)^\nu K_\nu \left( \frac{\sqrt{2\nu}r}{\ell} \right)\] <ul> <li> \[r = |x - x'|\] </li> <li>\(\nu\) controls smoothness: e.g., \(\nu = 1/2\) gives exponential kernel, \(\nu \to \infty\) recovers RBF.</li> <li>Suitable for modeling rougher, more realistic functions in real-world applications.</li> </ul> <hr> <h4 id="3-dot-product-linear-kernel">3. <strong>Dot Product (Linear) Kernel</strong> </h4> <p>Used when the function is expected to be linear:</p> \[k(x, x') = x^T x'\] <ul> <li>Equivalent to Bayesian linear regression.</li> <li>Doesn’t model nonlinearity unless combined with other kernels.</li> </ul> <hr> <h4 id="4-periodic-kernel">4. <strong>Periodic Kernel</strong> </h4> <p>Models functions with known repeating structure:</p> \[k(x, x') = \exp\left(-\frac{2 \sin^2(\pi |x - x'| / p)}{\ell^2}\right)\] <ul> <li>\(p\) controls the period, \(\ell\) controls smoothness.</li> <li>Ideal for seasonal data, time series, and cyclic behaviors.</li> </ul> <hr> <h3 id="why-kernels-matter-in-practice">Why Kernels Matter in Practice</h3> <p>The <strong>kernel acts as a prior over function space</strong>, shaping not only the kinds of functions the model will favor but also how information propagates across the input domain. Inference in a GP is guided entirely by the covariance implied by the kernel.</p> <p>Choosing a kernel is a modeling decision — like choosing a neural network architecture or a basis function family — but in GPs, it’s <strong>probabilistically grounded</strong>. And importantly, it’s <strong>differentiable and tunable</strong>: you can learn kernel parameters (like \(\ell\)) by maximizing the marginal likelihood.</p> <hr> <h3 id="visualization-4">Visualization</h3> <p>To illustrate how <strong>Gaussian Processes</strong> model distributions over functions, let’s walk through a simple 1D regression example using scikit-learn. We’ll fit a GP to a <strong>small set of noisy training points</strong>, allowing it to not only predict the mean function but also provide a <strong>confidence interval</strong> that reflects its uncertainty. The GP is equipped with an <strong>RBF (Radial Basis Function)</strong> kernel, which assumes smoothness in the underlying function.</p> <p>This example visually demonstrates one of the GP’s most powerful features: it can interpolate sparse data while expressing its uncertainty about regions it has not seen.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="n">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">RBF</span>

<span class="c1"># Training data (sparse)
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">]])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">X_train</span><span class="p">).</span><span class="nf">ravel</span><span class="p">()</span>

<span class="c1"># Define GP with RBF kernel
</span><span class="n">kernel</span> <span class="o">=</span> <span class="nc">RBF</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">gp</span> <span class="o">=</span> <span class="nc">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
<span class="n">gp</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predictive distribution at test points
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">gp</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="sh">'</span><span class="s">ro</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Training Data</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="sh">'</span><span class="s">b-</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Mean Prediction</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span><span class="n">X_test</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_pred</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">sigma</span><span class="p">,</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">95% Confidence Interval</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Gaussian Process Regression</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">f(x)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="text-align: center; margin: 2rem 0;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/probstat3_6-480.webp 480w,/assets/img/probstat3_6-800.webp 800w,/assets/img/probstat3_6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/probstat3_6.png" class="img-fluid rounded shadow-sm" width="100%" height="auto" alt="Gaussian Process Regression" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="font-style: italic; font-size: 0.95rem; color: #666; margin-top: 0.5rem;"> Figure: Gaussian Process Regression </p> </div> <p>The Plot Shows:</p> <ul> <li> <strong>Red dots</strong> represent the observed training data points.</li> <li>The <strong>blue line</strong> is the mean prediction of the Gaussian Process at every point in the input space.</li> <li> <p>The <strong>shaded region</strong> corresponds to a 95% confidence interval, calculated as:</p> \[\hat{f}(x) \pm 1.96 \cdot \sigma(x)\] <p>where \(\hat{f}(x)\) is the predicted mean and \(\sigma(x)\) is the standard deviation from the posterior.</p> </li> </ul> <hr> <p>Insights we can infer from this are</p> <ol> <li> <p><strong>Probabilistic Predictions, Not Just Point Estimates</strong><br> Unlike traditional regressors (like polynomial or linear regression), the GP predicts a <strong>distribution over functions</strong>, not a single best fit. For each input \(x\), it returns a mean prediction <strong>and</strong> a measure of uncertainty.</p> </li> <li> <p><strong>Uncertainty Reflects Data Coverage</strong><br> The model is most confident (i.e., narrowest uncertainty band) <strong>near the observed data points</strong>, and increasingly uncertain as we move away from them. This is especially visible at the edges (near \(x = 0\) and \(x = 10\)), where the GP hasn’t seen any training data.</p> </li> <li> <p><strong>The Role of the RBF Kernel</strong><br> The <strong>RBF kernel</strong> assumes that points closer in input space produce similar outputs. It’s what gives the GP its smooth, wavy behavior. If a different kernel were used (e.g., linear or periodic), the shape of the mean and uncertainty band would change.</p> </li> <li> <strong>Handling Small Datasets Gracefully</strong><br> Despite having only four training points, the GP constructs a smooth function that captures the structure of the underlying sine wave—without overfitting. This makes it particularly useful in low-data regimes like: <ul> <li>Experimental design,</li> <li>Bayesian optimization,</li> <li>Medical modeling, where samples are expensive.</li> </ul> </li> <li> <strong>Interpretable Uncertainty</strong><br> The shaded band gives us a principled way to <strong>quantify model confidence</strong>. Unlike confidence intervals in frequentist regression, which apply to the parameter estimate, the GP’s uncertainty is <strong>pointwise</strong> and interpretable: “Here’s how unsure the model is at this input.”</li> </ol> <hr> <p>This showcases the core strength of Gaussian Processes: <strong>flexible, non-parametric regression with uncertainty quantification</strong>. By treating the prediction as a distribution over functions, the GP provides not only a best guess but also a principled expression of <strong>how much we trust that guess</strong> at each point. This property is crucial in settings where <strong>uncertainty is as important as accuracy</strong>.</p> <p>This visualization illustrates:</p> <ul> <li>How GPs interpolate observed points with smooth curves,</li> <li>How prediction uncertainty grows away from the training data,</li> <li>And how the kernel controls the shape of the functions we consider probable.</li> </ul> <hr> <h3 id="applications-in-data-science-4">Applications in Data Science</h3> <h4 id="bayesian-optimization"><strong>Bayesian Optimization</strong></h4> <p>GPs are often used as <strong>surrogate models</strong> in Bayesian Optimization, where the goal is to optimize a black-box function that is expensive to evaluate. The GP captures both the function estimate and its uncertainty, enabling <strong>exploration vs. exploitation</strong> strategies.</p> <h4 id="uncertainty-aware-regression"><strong>Uncertainty-Aware Regression</strong></h4> <p>GPs naturally model <strong>predictive uncertainty</strong>. This is vital in:</p> <ul> <li>Medical diagnosis (model confidence matters),</li> <li>Sensor fusion in robotics (merging noisy measurements),</li> <li>Scientific modeling (where understanding the confidence of predictions is key).</li> </ul> <h4 id="small-data-regimes"><strong>Small-Data Regimes</strong></h4> <p>In cases where data is expensive or scarce (e.g., drug discovery, experimental physics), GPs shine because they can learn complex patterns without overfitting and provide principled uncertainty.</p> <h4 id="active-learning"><strong>Active Learning</strong></h4> <p>Because GPs quantify uncertainty, they are ideal for <strong>active learning</strong> — selecting new data points where the model is uncertain to improve learning efficiency.</p> <hr> <p>Gaussian Processes offer an elegant solution to the problem of modeling unknown functions with uncertainty. Their interpretability, flexibility, and strong theoretical foundation make them a powerful tool for data scientists working in probabilistic modeling, especially in safety-critical and small-data domains.</p> <hr> <h2 id="bayesian-foundations-in-modern-data-science">Bayesian Foundations in Modern Data Science</h2> <p>The concepts explored throughout this discussion—Bayes’ theorem, priors and posteriors, likelihoods, conjugate priors, MLE, MAP, and Gaussian Processes—collectively form a foundational perspective on statistical inference. These ideas extend beyond isolated techniques; they represent a systematic approach to learning from data in uncertain settings. Bayesian methods offer a coherent framework for incorporating prior knowledge, updating beliefs based on observed evidence, and reasoning in a probabilistic manner.</p> <p>Among the earliest and most accessible examples is the <strong>Naive Bayes classifier</strong>, which applies Bayes’ theorem to supervised classification tasks. Despite its assumption that features are conditionally independent given the class label, the model often performs well in practice, especially in high-dimensional problems like spam detection or document categorization. It constructs a posterior distribution over class labels:</p> \[P(C \mid x_1, x_2, \dots, x_n) \propto P(C) \prod_{i=1}^n P(x_i \mid C)\] <p>In this formulation, the prior class probabilities \(P(C)\) and the conditional likelihoods \(P(x_i \mid C)\) can be estimated using MLE or MAP techniques. If needed, conjugate priors can be used to incorporate prior knowledge and simplify inference. While simple, the model’s efficiency, transparency, and probabilistic interpretation make it a useful baseline in a variety of real-world systems.</p> <p>More expressive probabilistic modeling is possible when one relaxes the independence assumptions and considers the dependency structure among variables. This leads to the framework of <strong>Bayesian networks</strong>, which use directed acyclic graphs to represent conditional dependencies. The joint distribution is factored as:</p> \[P(X_1, X_2, \dots, X_n) = \prod_{i=1}^n P(X_i \mid \text{Parents}(X_i))\] <p>Each node in the graph corresponds to a random variable, and each edge indicates a dependency. This decomposition generalizes the ideas seen in Naive Bayes by allowing richer structures that capture causality, interaction effects, and shared influences. Parameters in Bayesian networks can be estimated using MLE or MAP, and conjugate priors are often employed to simplify learning, especially in the presence of missing or noisy data. These models are well-suited for complex reasoning tasks, such as medical diagnosis, credit risk modeling, and user behavior analysis.</p> <p>While the models discussed so far rely on explicit parameterizations, <strong>Gaussian Processes</strong> offer a non-parametric alternative by placing a prior over the space of functions themselves. Rather than defining a fixed number of parameters, a Gaussian Process assumes that any finite set of function values follows a multivariate normal distribution. This is particularly valuable in regression tasks where predictive uncertainty is as important as the predicted value.</p> <p>A Gaussian Process model, once conditioned on observed data, yields a posterior distribution over possible functions, complete with mean predictions and confidence intervals. The choice of kernel function encodes assumptions about smoothness, periodicity, or other properties of the underlying function. This makes Gaussian Processes especially useful when working with small datasets, where flexibility and uncertainty quantification are critical.</p> <p>A prominent use case for Gaussian Processes is in <strong>Bayesian optimization</strong>, a strategy for optimizing functions that are expensive or difficult to evaluate. Here, a GP acts as a surrogate model that approximates the true objective, guiding the selection of future evaluations based on both predicted values and uncertainty. This methodology has become a standard tool in hyperparameter tuning, experimental design, and materials discovery.</p> <p>The broader value of Bayesian modeling lies in its capacity to quantify and propagate uncertainty. In domains such as healthcare, autonomous systems, and finance, uncertainty-aware predictions support better decision-making under risk. For instance, posterior class probabilities from a Naive Bayes model can inform risk thresholds in a classifier; MAP estimates provide regularization by incorporating prior constraints; and Gaussian Processes yield confidence bounds that reflect the limits of available information. These capabilities are not peripheral—they are central to deploying models that must make informed decisions under real-world conditions.</p> <hr> <p>As we close our discussion on Bayesian inference, the natural next step in our learning journey is to explore how <strong>statistical inference</strong>—particularly in its frequentist form—shapes decision-making in data science. In the upcoming part of this series, we will dive into the core principles behind <strong>A/B testing</strong>, <strong>sampling techniques</strong>, <strong>confidence intervals</strong>, and <strong>hypothesis testing</strong> frameworks such as the Z-test, T-test, and ANOVA. We’ll also unpack concepts like <strong>p-values</strong>, <strong>statistical power</strong>, and <strong>Type I/II errors</strong> to understand how to validate results under uncertainty. These tools are critical when measuring the effect of product changes, analyzing experiment results, or quantifying the impact of features in machine learning workflows. If Bayesian inference teaches us how to update beliefs, classical inference equips us to rigorously evaluate them—and together, they form a powerful toolkit for modern data science.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> <script src="/assets/js/lunr.min.js"></script> <script src="/assets/js/search.js"></script> </body> </html>