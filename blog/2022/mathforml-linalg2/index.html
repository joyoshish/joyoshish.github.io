<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra Basics for ML - Matrices and Matrix Operations | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Linear Algebra 2 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-linalg2/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">Computer Science and Data Science</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra Basics for ML - Matrices and Matrix Operations</h1> <p class="post-meta"> January 15, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear-algebra</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#matrices-and-matrix-operations">Matrices and Matrix Operations</a></li> <li class="toc-entry toc-h2"><a href="#matrix-addition-and-multiplication">Matrix Addition and Multiplication</a></li> <li class="toc-entry toc-h2"><a href="#transpose-inverse-determinant-trace-and-rank">Transpose, Inverse, Determinant, Trace, and Rank</a></li> <li class="toc-entry toc-h2"><a href="#special-matrices-in-machine-learning">Special Matrices in Machine Learning</a></li> <li class="toc-entry toc-h2"><a href="#block-and-partitioned-matrices">Block and Partitioned Matrices</a></li> <li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>In machine learning, the power of matrices is undeniable. Whether you’re manipulating datasets, performing linear transformations in neural networks, or analyzing graph structures, matrices provide a compact and efficient way to represent and process information. In this post, we’ll explore the world of matrices and matrix operations through a problem-driven approach. Each section grounds the math in real ML use cases, walks through the concepts clearly, and wraps up with code and applications.</p> <hr> <h2 id="matrices-and-matrix-operations">Matrices and Matrix Operations</h2> <p>Think of a matrix as a grid—a two-dimensional array of numbers. On the surface, it may just look like a neat way to organize data, but in machine learning, it does so much more. Matrices are the building blocks for operations like transforming input features, propagating signals through neural networks, and encoding relationships in graph data. Their elegance lies in how they capture complex transformations so succinctly.</p> <hr> <h2 id="matrix-addition-and-multiplication">Matrix Addition and Multiplication</h2> <p>Suppose you have two datasets or feature maps, and you want to combine them before feeding them into a model. Or maybe you need to apply a set of learned weights to an input layer in a neural network. These scenarios boil down to two essential operations: matrix addition and multiplication.</p> <p>Matrix addition is pretty straightforward. If two matrices \(A\) and \(B\) have the same dimensions \(m \times n\), you simply add corresponding elements:</p> \[(A + B)_{ij} = A_{ij} + B_{ij}\] <p>Matrix multiplication is a little more involved—and powerful. Given a matrix \(A\) of size \(m \times n\) and a matrix \(B\) of size \(n \times p\), the resulting matrix \(C = AB\) will be of size \(m \times p\), where each element is calculated as:</p> \[C_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}\] <p>This operation is crucial in machine learning because it represents linear combinations of inputs, weighted by learned parameters.</p> <p>Let’s look at how these operations play out in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>

<span class="c1"># Matrix addition
</span><span class="n">sum_matrix</span> <span class="o">=</span> <span class="n">A</span> <span class="o">+</span> <span class="n">B</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matrix Sum:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">sum_matrix</span><span class="p">)</span>

<span class="c1"># Matrix multiplication
</span><span class="n">product_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Matrix Product:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">product_matrix</span><span class="p">)</span>
</code></pre></div></div> <p>Matrix addition and multiplication are two of the most widely used operations in machine learning. Matrix addition allows us to combine information from multiple sources—whether that’s merging datasets, summing feature maps, or applying a bias term in a model. It’s a simple operation, but it appears throughout every stage of a data pipeline, especially when dealing with mini-batches of data or ensemble-style models.</p> <p>Matrix multiplication, on the other hand, is fundamental to how machine learning models process and learn from data. In neural networks, the input at each layer is multiplied by a weight matrix to transform it into a new representation. This transformation captures patterns and relationships between features, allowing deeper layers to learn more abstract concepts. The same operation is used to combine embeddings, transform spatial information in computer vision, and project data into new feature spaces.</p> <p>Outside of neural networks, matrix multiplication also shows up in dimensionality reduction techniques like PCA, where data is multiplied by a matrix of principal components to produce a compressed version of the original dataset. In graph-based learning, adjacency matrices are multiplied with feature matrices to enable message passing across nodes—allowing information to flow and be aggregated from neighboring nodes in graph neural networks.</p> <hr> <h2 id="transpose-inverse-determinant-trace-and-rank">Transpose, Inverse, Determinant, Trace, and Rank</h2> <p>Sometimes, understanding the structure of your data or solving an equation requires going deeper into what a matrix really <em>does</em>. Is it reversible? How much does it scale the space? How complex is it?</p> <p>The transpose of a matrix \(A\), written as \(A^T\), simply flips its rows and columns:</p> \[(A^T)_{ij} = A_{ji}\] <p>The inverse of a square matrix \(A\), when it exists, satisfies:</p> \[AA^{-1} = A^{-1}A = I\] <p>where \(I\) is the identity matrix.</p> <p>The determinant, \(\det(A)\), gives a scalar that represents how much the transformation defined by \(A\) scales space. If the determinant is zero, the matrix isn’t invertible.</p> <p>The trace of a matrix is the sum of its diagonal entries:</p> \[\text{tr}(A) = \sum_{i=1}^{n} A_{ii}\] <p>And the rank tells you how many dimensions your matrix truly spans—how many linearly independent rows or columns it contains.</p> <p>All of these properties surface in ML. You might invert a matrix to solve a linear system in regression. The determinant and rank tell you if your data is redundant. The trace often appears in loss functions or regularization terms.</p> <p>Here’s how to compute them:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Transpose:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">A</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Determinant:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">det</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Inverse:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">inv</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Trace:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">trace</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Rank:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">matrix_rank</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
</code></pre></div></div> <p>These matrix operations go beyond basic arithmetic and into the territory of <em>understanding the structure of data and transformations</em>. The transpose operation, for example, is used when calculating gradients and aligning matrix dimensions during dot products—especially important in backpropagation in deep learning.</p> <p>The <strong>inverse of a matrix</strong> is essential in solving linear systems analytically. While in practice we often use approximations or decompositions, the concept of matrix inversion is still central to understanding <strong>linear regression</strong>, where the solution for the optimal weights can be written as: \(\mathbf{w} = (X^T X)^{-1} X^T y\) This closed-form solution shows how matrix operations solve real ML problems when the dataset is small and the solution is tractable.</p> <p>The <strong>determinant</strong> tells us whether a transformation preserves volume or collapses space—if it’s zero, the transformation is not invertible. This matters in unsupervised learning, such as <strong>normalizing flows</strong>, where transformations must be invertible and differentiable. Similarly, the <strong>trace</strong> of a matrix shows up in optimization problems as a regularization penalty—for example, in <strong>matrix factorization</strong> or <strong>low-rank approximations</strong>, where trace minimization helps control complexity.</p> <p>Finally, <strong>matrix rank</strong> is crucial for understanding the expressive power of your dataset. If a feature matrix is rank-deficient, it means some features are linear combinations of others—signaling <strong>multicollinearity</strong>, which can break regression models or inflate variance in predictions. Knowing the rank helps us detect redundancy, reduce overfitting, and improve generalization.</p> <hr> <h2 id="special-matrices-in-machine-learning">Special Matrices in Machine Learning</h2> <p>Some matrices have properties that make them especially elegant—and efficient—in machine learning workflows.</p> <p>The <strong>identity matrix</strong> \(I\) acts as a neutral element in multiplication:</p> \[AI = IA = A\] <p><strong>Diagonal matrices</strong> are square matrices with nonzero entries only on the diagonal. They scale vectors component-wise, simplifying many operations.</p> <p><strong>Symmetric matrices</strong> satisfy \(A = A^T\). Covariance matrices, for example, are symmetric and reveal how variables vary together.</p> <p><strong>Orthogonal matrices</strong> satisfy:</p> \[Q^T Q = QQ^T = I\] <p>They preserve angles and lengths, which is why they’re used in rotations, reflections, and orthonormal bases.</p> <p>A <strong>positive definite matrix</strong> is symmetric and satisfies:</p> \[x^T A x &gt; 0\] <p>for any nonzero vector \(x\). These appear in optimization problems, where you want to ensure a unique minimum, and in models like ridge regression.</p> <p>Let’s create and check these properties:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">I</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Identity Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">I</span><span class="p">)</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">diag</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Diagonal Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Symmetric Matrix:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Orthogonal Check (Q^T Q):</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Q</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Q</span><span class="p">))</span>

<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">eigvals</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Eigenvalues (Positive Definite if all &gt; 0):</span><span class="sh">"</span><span class="p">,</span> <span class="n">eigenvalues</span><span class="p">)</span>
</code></pre></div></div> <p>Special matrices such as identity, diagonal, symmetric, orthogonal, and positive definite matrices are not just mathematical curiosities—they are workhorses behind efficient and stable ML algorithms.</p> <p>The <strong>identity matrix</strong> is used to initialize parameters or serve as a “no-op” transformation. It plays a role in <strong>residual networks</strong> (ResNets), where identity shortcuts help preserve gradients in deep architectures. In <strong>regularization techniques</strong>, the identity matrix appears in terms like \(\lambda I\), added to ensure numerical stability when inverting nearly singular matrices.</p> <p><strong>Diagonal matrices</strong> simplify transformations by applying scaling operations—critical in <strong>feature normalization</strong>, where diagonal matrices can represent per-feature standard deviations or inverse variances. They also arise in eigenvalue decompositions, where the diagonal matrix holds the eigenvalues representing the importance of each principal component or latent feature.</p> <p><strong>Symmetric matrices</strong> dominate <strong>statistics and probabilistic ML</strong>. Covariance matrices are symmetric by definition and reflect the relationships among features. In PCA, the symmetric covariance matrix is decomposed to extract the directions of maximum variance. <strong>Orthogonal matrices</strong>, which preserve inner products, form the basis for <strong>QR decomposition</strong> and <strong>SVD</strong>, enabling dimensionality reduction, whitening transformations, and stable numerical methods. And <strong>positive definite matrices</strong> guarantee convexity in optimization, which is why they’re vital in kernel methods (like in SVMs), <strong>Gaussian processes</strong>, and <strong>regularized regression</strong>.</p> <p>Understanding these special types helps you design models that are faster, more stable, and easier to train—and gives you the vocabulary to interpret the results geometrically.</p> <hr> <h2 id="block-and-partitioned-matrices">Block and Partitioned Matrices</h2> <p>When working with massive datasets or large models, it often makes sense to split things up. Block matrices let us partition a large matrix into smaller, more manageable pieces:</p> \[A = \begin{bmatrix} A_{11} &amp; A_{12} \\ A_{21} &amp; A_{22} \end{bmatrix}\] <p>This is especially helpful in distributed computing or batch processing, where each submatrix can be processed independently.</p> <p>You can split a matrix into blocks like this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
              <span class="p">[</span><span class="mi">13</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">16</span><span class="p">]])</span>

<span class="n">A11</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">A12</span> <span class="o">=</span> <span class="n">A</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">:]</span>
<span class="n">A21</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">A22</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="mi">2</span><span class="p">:,</span> <span class="mi">2</span><span class="p">:]</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Block A11:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">A11</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Block A12:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">A12</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Block A21:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">A21</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Block A22:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">A22</span><span class="p">)</span>
</code></pre></div></div> <p>Block matrices offer a powerful abstraction when dealing with <strong>large-scale datasets</strong>, <strong>multi-modal learning</strong>, or <strong>distributed computing</strong>. Rather than operating on an entire matrix at once, we can break it into smaller, logically meaningful parts and process them independently or in parallel.</p> <p>In <strong>mini-batch training</strong>, especially in stochastic gradient descent (SGD), data is already treated as a collection of smaller blocks. Matrix operations are performed on each batch independently, allowing the model to scale to massive datasets without running out of memory.</p> <p>In <strong>multi-view learning</strong> or <strong>multi-task learning</strong>, where different feature sets or targets are grouped by modality or domain, block matrices naturally model these segmented structures. Each block might represent a different view of the same entity—like image features and text descriptions of the same object—and learning proceeds with interactions across blocks.</p> <p>In <strong>Graph ML</strong>, block matrices are used to capture the <strong>community structure</strong> within large graphs. The adjacency matrix of a graph can be partitioned into blocks, each corresponding to a subgraph or cluster, enabling more scalable and interpretable analysis. In <strong>parallel and distributed ML</strong>, partitioning matrices allows data to be distributed across nodes, with operations like block-wise matrix multiplication or block gradient descent running concurrently.</p> <p>Block matrices bring structure and efficiency to matrix computation, helping bridge the gap between mathematical elegance and engineering scalability.</p> <hr> <h2 id="conclusion">Conclusion</h2> <p>From simple addition to sophisticated transformations, matrices give us a powerful framework to represent and manipulate data in machine learning. Their structure captures everything from raw inputs to learned representations and even the relationships between them. Whether you’re solving a system of equations, rotating vectors, or analyzing massive graphs, the right matrix operation unlocks the magic.</p> <p>In this post, we explored core matrix operations with a focus on their relevance in real ML tasks. This foundation will carry you far—as we dive deeper into eigenvectors, decompositions, and optimization in upcoming chapters of the <em>Math for ML</em> series.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> </body> </html>