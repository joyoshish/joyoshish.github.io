<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra Basics for ML - Vector Spaces and Transformations | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Linear Algebra 5 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-linalg5/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">Computer Science and Data Science</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra Basics for ML - Vector Spaces and Transformations</h1> <p class="post-meta"> February 07, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear-algebra</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#the-need-for-vector-spaces-and-transformations">The Need for Vector Spaces and Transformations</a></li> <li class="toc-entry toc-h2"> <a href="#vector-spaces-the-foundation-of-data-representation">Vector Spaces: The Foundation of Data Representation</a> <ul> <li class="toc-entry toc-h3"><a href="#what-is-a-vector-space">What is a Vector Space?</a></li> <li class="toc-entry toc-h3"><a href="#example-vector-space-in-feature-engineering">Example: Vector Space in Feature Engineering</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#motivation-behind-linear-transformations">Motivation Behind Linear Transformations</a></li> <li class="toc-entry toc-h2"> <a href="#linear-transformations-and-change-of-basis">Linear Transformations and Change of Basis</a> <ul> <li class="toc-entry toc-h3"><a href="#what-is-a-change-of-basis">What is a Change of Basis?</a></li> <li class="toc-entry toc-h3"><a href="#practical-example-change-of-basis-in-pca">Practical Example: Change of Basis in PCA</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#affine-transformations-beyond-linear-mappings">Affine Transformations: Beyond Linear Mappings</a> <ul> <li class="toc-entry toc-h3"><a href="#what-is-an-affine-transformation">What is an Affine Transformation?</a></li> <li class="toc-entry toc-h3"><a href="#example-image-manipulation-with-affine-transformations">Example: Image Manipulation with Affine Transformations</a></li> <li class="toc-entry toc-h3"><a href="#affine-transformations-in-feature-engineering">Affine Transformations in Feature Engineering</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#application-data-transformation-in-feature-engineering">Application: Data Transformation in Feature Engineering</a> <ul> <li class="toc-entry toc-h3"><a href="#why-transform-data">Why Transform Data?</a></li> <li class="toc-entry toc-h3"><a href="#example-data-scaling-with-an-affine-transformation">Example: Data Scaling with an Affine Transformation</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h2 id="the-need-for-vector-spaces-and-transformations">The Need for Vector Spaces and Transformations</h2> <p>When we work with machine learning and data science, we often deal with data in various forms, such as images, text, or numerical tables. The real challenge is to represent and manipulate this data efficiently. This is where the concept of vector spaces and transformations comes into play. In simple terms, vector spaces allow us to represent data, and transformations help us modify or map this data to different representations.</p> <p>In machine learning, data is typically represented as vectors in a vector space, and various algorithms manipulate these vectors to extract features, perform operations, or make predictions. Linear transformations help us understand how the data changes when we move it between different coordinate systems. Affine transformations go a step further by adding translations, which are particularly useful in fields like computer vision, where we often deal with translations and rotations of images.</p> <p>In this blog post, we will explore vector spaces and transformations in detail, linking them to feature engineering and model performance improvements. Let’s start by understanding vector spaces and linear transformations.</p> <h2 id="vector-spaces-the-foundation-of-data-representation">Vector Spaces: The Foundation of Data Representation</h2> <h3 id="what-is-a-vector-space">What is a Vector Space?</h3> <p>A vector space (or linear space) is a collection of vectors that can be added together and multiplied by scalars, subject to certain rules. Mathematically, a vector space \(V\) over a field \(F\) is a set of objects (called vectors), along with two operations: vector addition and scalar multiplication. These operations must satisfy the following axioms:</p> <ul> <li> <p><strong>Commutativity of addition</strong>:<br> \(\mathbf{v} + \mathbf{w} = \mathbf{w} + \mathbf{v}\)</p> </li> <li> <p><strong>Associativity of addition</strong>:<br> \((\mathbf{v} + \mathbf{w}) + \mathbf{u} = \mathbf{v} + (\mathbf{w} + \mathbf{u})\)</p> </li> <li> <p><strong>Additive identity</strong>: There exists a zero vector \(\mathbf{0}\) such that<br> \(\mathbf{v} + \mathbf{0} = \mathbf{v}\)</p> </li> <li> <p><strong>Additive inverse</strong>: For every vector \(\mathbf{v}\), there exists \(-\mathbf{v}\) such that<br> \(\mathbf{v} + (-\mathbf{v}) = \mathbf{0}\)</p> </li> <li> <p><strong>Distributive properties of scalar multiplication</strong>:<br> \(a(\mathbf{v} + \mathbf{w}) = a\mathbf{v} + a\mathbf{w}\)<br> \((a + b)\mathbf{v} = a\mathbf{v} + b\mathbf{v}\)</p> </li> <li> <p><strong>Multiplicative identity of scalars</strong>:<br> \(1 \cdot \mathbf{v} = \mathbf{v}\)</p> </li> <li> <p><strong>Compatibility of scalar multiplication</strong>:<br> \(a(b\mathbf{v}) = (ab)\mathbf{v}\)</p> </li> </ul> <p>In machine learning, we often think of a vector space as the space in which the data exists. For example, if you have a dataset with \(n\) features, each data point is a vector in \(n\)-dimensional space. The entire dataset is a collection of vectors that form a subspace of that \(n\)-dimensional space.</p> <h3 id="example-vector-space-in-feature-engineering">Example: Vector Space in Feature Engineering</h3> <p>Let’s say we are working with a dataset containing two features, such as height and weight of individuals. These features can be represented as vectors in a 2-dimensional vector space, where each point corresponds to a person’s height and weight. The feature vectors in this space can be manipulated, transformed, and analyzed for machine learning tasks like clustering, regression, and classification.</p> <h2 id="motivation-behind-linear-transformations">Motivation Behind Linear Transformations</h2> <p>A linear transformation is a function that maps a vector in one vector space to a vector in another vector space, preserving the operations of vector addition and scalar multiplication. This is extremely important in machine learning, as many algorithms involve transforming data into new spaces to extract more useful features or to make it easier for the model to learn.</p> <p>For example, in Principal Component Analysis (PCA), we perform a linear transformation that projects high-dimensional data onto a lower-dimensional subspace, allowing us to visualize or analyze the data more effectively. The key point is that linear transformations preserve the structure of the data in a way that simplifies operations like regression or classification.</p> <h2 id="linear-transformations-and-change-of-basis">Linear Transformations and Change of Basis</h2> <h3 id="what-is-a-change-of-basis">What is a Change of Basis?</h3> <p>In machine learning, we may need to switch between different coordinate systems, depending on how we want to represent our data. This is where the concept of a change of basis comes in. When we apply a linear transformation to a vector, we can represent it in a new basis (a new set of basis vectors) that might be more convenient for our analysis.</p> <p>For example, imagine you have a 2D dataset with features height and weight. These features are represented in the standard basis of the 2D plane, i.e., the x-axis and y-axis. However, you may want to rotate the data such that the axes align with the directions of maximum variance. This is a change of basis, where we apply a linear transformation to rotate the data.</p> <p>Mathematically, if \(A\) is the transformation matrix and \(\mathbf{x}\) is a vector in the original basis, then the transformed vector \(\mathbf{x'}\) in the new basis is given by:<br> \(\mathbf{x'} = A \mathbf{x}\)</p> <h3 id="practical-example-change-of-basis-in-pca">Practical Example: Change of Basis in PCA</h3> <p>Consider PCA, where we perform an eigen-decomposition of the covariance matrix to find the principal components. These principal components form a new basis, and we can transform the data into this new basis by applying the linear transformation defined by the eigenvectors of the covariance matrix. The new dataset will have the same data points but represented in terms of the directions of maximum variance (the principal components).</p> <p>This transformation often helps in reducing the dimensionality of the data while retaining the most important features. In essence, PCA is a linear transformation that changes the basis from the original feature space to the space of principal components.</p> <h2 id="affine-transformations-beyond-linear-mappings">Affine Transformations: Beyond Linear Mappings</h2> <h3 id="what-is-an-affine-transformation">What is an Affine Transformation?</h3> <p>An affine transformation is a linear transformation followed by a translation. This means that affine transformations include not just rotations, scaling, and shearing, but also shifts in space. Mathematically, an affine transformation can be represented as:<br> \(\mathbf{x'} = A \mathbf{x} + \mathbf{b}\) where \(A\) is the linear transformation matrix, \(\mathbf{x}\) is the original vector, \(\mathbf{b}\) is the translation vector, and \(\mathbf{x'}\) is the transformed vector.</p> <p>Affine transformations are extremely useful in image processing and computer vision, where we often need to rotate, scale, or translate images.</p> <h3 id="example-image-manipulation-with-affine-transformations">Example: Image Manipulation with Affine Transformations</h3> <p>In computer vision, affine transformations are commonly used for tasks such as rotating or resizing images. Consider a scenario where we want to rotate an image by a certain angle or scale it up or down. These operations can be represented as affine transformations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">cv2</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Load image
</span><span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="sh">'</span><span class="s">image.jpg</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Define affine transformation matrix (rotation)
</span><span class="n">angle</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">center</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">rotation_matrix</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">getRotationMatrix2D</span><span class="p">(</span><span class="n">center</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Apply affine transformation (rotation)
</span><span class="n">rotated_image</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="nf">warpAffine</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">rotation_matrix</span><span class="p">,</span> <span class="p">(</span><span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">image</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Show the result
</span><span class="n">cv2</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="sh">'</span><span class="s">Rotated Image</span><span class="sh">'</span><span class="p">,</span> <span class="n">rotated_image</span><span class="p">)</span>
<span class="n">cv2</span><span class="p">.</span><span class="nf">waitKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cv2</span><span class="p">.</span><span class="nf">destroyAllWindows</span><span class="p">()</span>
</code></pre></div></div> <p>In this example, the image is rotated by 45 degrees around its center, and the transformation is applied using an affine matrix.</p> <h3 id="affine-transformations-in-feature-engineering">Affine Transformations in Feature Engineering</h3> <p>In feature engineering, affine transformations can be used to modify data features, for example, by scaling or translating them. This can help improve the performance of machine learning models, especially when the features vary widely in magnitude or range.</p> <p>For instance, we might apply an affine transformation to normalize or standardize data by subtracting the mean (translation) and scaling the data by its standard deviation (scaling). This is a common preprocessing step to improve the performance of algorithms like linear regression or neural networks.</p> <h2 id="application-data-transformation-in-feature-engineering">Application: Data Transformation in Feature Engineering</h2> <h3 id="why-transform-data">Why Transform Data?</h3> <p>In machine learning, feature engineering refers to the process of applying mathematical functions to features to improve model performance. Data transformations, such as scaling, normalizing, and encoding, are common operations used in feature engineering.</p> <ul> <li> <p><strong>Scaling</strong>: Many machine learning algorithms, such as k-nearest neighbors or gradient descent, perform better when features are on a similar scale. Affine transformations such as min-max scaling or standardization (subtracting the mean and dividing by the standard deviation) are commonly used to scale features.</p> </li> <li> <p><strong>Dimensionality Reduction</strong>: Techniques like PCA, as we mentioned earlier, involve linear transformations to reduce the dimensionality of data. By transforming the data into a lower-dimensional space, we retain the most important features while discarding noise or less useful features.</p> </li> <li> <p><strong>Non-linear Transformations</strong>: Sometimes, applying non-linear transformations to data, such as log transformations or polynomial features, can help make the data more suitable for machine learning models.</p> </li> </ul> <h3 id="example-data-scaling-with-an-affine-transformation">Example: Data Scaling with an Affine Transformation</h3> <p>Consider a dataset with two features, age and income, that have vastly different scales. Income may range from thousands to millions, while age ranges from 0 to 100. Applying an affine transformation like standardization can help bring both features onto a similar scale:</p> <ul> <li>Translate each feature by subtracting the mean.</li> <li>Scale each feature by dividing by its standard deviation.</li> </ul> <p>Mathematically, we can apply this affine transformation to each feature:<br> \(\mathbf{x'} = \frac{\mathbf{x} - \mu}{\sigma}\) where \(\mu\) is the mean and \(\sigma\) is the standard deviation of the feature.</p> <p>In Python, we can perform this transformation using <code class="language-plaintext highlighter-rouge">sklearn</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Example dataset
</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">Age</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">23</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">29</span><span class="p">],</span> <span class="sh">'</span><span class="s">Income</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">120000</span><span class="p">,</span> <span class="mi">85000</span><span class="p">,</span> <span class="mi">150000</span><span class="p">,</span> <span class="mi">65000</span><span class="p">]}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Apply standardization (affine transformation)
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">df_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">df_scaled</span><span class="p">)</span>
</code></pre></div></div> <p>This will standardize both features, bringing them to the same scale, which can help improve the performance of models like linear regression or k-means clustering.</p> <hr> <p>In this post, we explored <strong>vector spaces</strong>, <strong>linear transformations</strong>, and <strong>affine transformations</strong>, all of which are foundational concepts in linear algebra that play a critical role in machine learning and data science. From <strong>representing data in vector spaces</strong> to <strong>transforming that data</strong> in meaningful ways, these concepts help us better understand and manipulate data, improving our ability to build and refine machine learning models.</p> <p>Whether you’re working with <strong>feature engineering</strong>, <strong>dimensionality reduction</strong>, or <strong>image transformations</strong>, understanding vector spaces and transformations allows you to tackle complex problems and improve your models’ performance.</p> <p>Remember, the next time you’re faced with a complex dataset or need to manipulate features, think about how <strong>vector spaces</strong> and <strong>transformations</strong> can help simplify the problem and enhance your machine learning pipeline.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> </body> </html>