<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Linear Algebra Basics for ML - Advanced Topics | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Linear Algebra 6 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-linalg6/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">Computer Science and Data Science</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="nav-item"> <a id="search-button" class="nav-link" href="javascript:void(0)"> <i class="ti ti-search"></i> <span class="sr-only">Search</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> <div id="search-modal" class="search-modal"> <div class="search-container"> <div class="search-header"> <div class="search-input-container"> <i class="ti ti-search search-icon"></i> <input type="text" id="search-input" class="search-input" placeholder="Search..." autocomplete="off"> <button id="search-close" class="search-close-button"> <i class="ti ti-x"></i> </button> </div> <div class="search-shortcuts"> <span class="search-shortcut"> <kbd>/</kbd> or <kbd>Ctrl</kbd>+<kbd>K</kbd> to search </span> <span class="search-shortcut"> <kbd>↑</kbd><kbd>↓</kbd> to navigate </span> <span class="search-shortcut"> <kbd>Enter</kbd> to select </span> <span class="search-shortcut"> <kbd>Esc</kbd> to close </span> </div> </div> <div id="search-results" class="search-results"> <div class="search-results-container"></div> </div> </div> </div> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Linear Algebra Basics for ML - Advanced Topics</h1> <p class="post-meta"> February 07, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/linear-algebra"> <i class="fa-solid fa-hashtag fa-sm"></i> linear-algebra</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h1"> <a href="#advanced-matrix-factorizations-in-machine-learning">Advanced Matrix Factorizations in Machine Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#1-lu-decomposition">1. LU Decomposition</a> <ul> <li class="toc-entry toc-h3"><a href="#definition">Definition</a></li> <li class="toc-entry toc-h3"><a href="#algorithm">Algorithm</a></li> <li class="toc-entry toc-h3"><a href="#applications-in-machine-learning">Applications in Machine Learning</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-cholesky-decomposition">2. Cholesky Decomposition</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-1">Definition</a></li> <li class="toc-entry toc-h3"><a href="#construction">Construction</a></li> <li class="toc-entry toc-h3"><a href="#applications-in-machine-learning-1">Applications in Machine Learning</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-qr-decomposition">3. QR Decomposition</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-2">Definition</a></li> <li class="toc-entry toc-h3"><a href="#gram-schmidt-orthogonalization">Gram-Schmidt Orthogonalization</a></li> <li class="toc-entry toc-h3"><a href="#least-squares-via-qr">Least Squares via QR</a></li> <li class="toc-entry toc-h3"><a href="#applications-in-machine-learning-2">Applications in Machine Learning</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-non-negative-matrix-factorization-nmf">4. Non-negative Matrix Factorization (NMF)</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-3">Definition</a></li> <li class="toc-entry toc-h3"><a href="#optimization-problem">Optimization Problem</a></li> <li class="toc-entry toc-h3"><a href="#multiplicative-update-rules">Multiplicative Update Rules</a></li> <li class="toc-entry toc-h3"><a href="#applications-in-machine-learning-3">Applications in Machine Learning</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#orthogonalization-techniques-in-machine-learning-and-deep-learning">Orthogonalization Techniques in Machine Learning and Deep Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#1-orthogonalization-and-orthonormal-bases">1. Orthogonalization and Orthonormal Bases</a> <ul> <li class="toc-entry toc-h3"><a href="#motivation">Motivation</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-the-gram-schmidt-process">2. The Gram-Schmidt Process</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-4">Definition</a></li> <li class="toc-entry toc-h3"><a href="#step-by-step-algorithm">Step-by-step Algorithm</a></li> <li class="toc-entry toc-h3"><a href="#example">Example</a></li> <li class="toc-entry toc-h3"><a href="#applications">Applications</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-orthogonal-initialization-in-neural-networks">3. Orthogonal Initialization in Neural Networks</a> <ul> <li class="toc-entry toc-h3"><a href="#motivation-1">Motivation</a></li> <li class="toc-entry toc-h3"><a href="#mathematical-principle">Mathematical Principle</a></li> <li class="toc-entry toc-h3"><a href="#how-to-initialize-orthogonal-matrices">How to Initialize Orthogonal Matrices</a></li> <li class="toc-entry toc-h3"><a href="#implementing-in-deep-learning-libraries">Implementing in Deep Learning Libraries</a></li> <li class="toc-entry toc-h3"><a href="#applications-1">Applications</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#kernel-methods-and-hilbert-spaces-in-machine-learning">Kernel Methods and Hilbert Spaces in Machine Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#1-inner-products-in-high-dimensional-spaces">1. Inner Products in High-Dimensional Spaces</a> <ul> <li class="toc-entry toc-h3"><a href="#motivation-2">Motivation</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-kernel-functions-and-the-kernel-trick">2. Kernel Functions and the Kernel Trick</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-5">Definition</a></li> <li class="toc-entry toc-h3"><a href="#examples-of-kernel-functions">Examples of Kernel Functions</a></li> <li class="toc-entry toc-h3"><a href="#why-it-works">Why It Works</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-reproducing-kernel-hilbert-space-rkhs">3. Reproducing Kernel Hilbert Space (RKHS)</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-6">Definition</a></li> <li class="toc-entry toc-h3"><a href="#intuition">Intuition</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-mercers-theorem">4. Mercer’s Theorem</a> <ul> <li class="toc-entry toc-h3"><a href="#statement">Statement</a></li> <li class="toc-entry toc-h3"><a href="#practical-use">Practical Use</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#5-applications-in-machine-learning">5. Applications in Machine Learning</a> <ul> <li class="toc-entry toc-h3"><a href="#51-support-vector-machines-svms">5.1 Support Vector Machines (SVMs)</a></li> <li class="toc-entry toc-h3"><a href="#52-gaussian-processes-gps">5.2 Gaussian Processes (GPs)</a></li> <li class="toc-entry toc-h3"><a href="#53-kernel-pca">5.3 Kernel PCA</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#summary">Summary</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#sparse-matrices-and-sparsity-in-machine-learning">Sparse Matrices and Sparsity in Machine Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#1-sparse-representations">1. Sparse Representations</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-7">Definition</a></li> <li class="toc-entry toc-h3"><a href="#storage-and-efficiency">Storage and Efficiency</a></li> <li class="toc-entry toc-h3"><a href="#use-cases">Use Cases</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-compressed-sensing">2. Compressed Sensing</a> <ul> <li class="toc-entry toc-h3"><a href="#motivation-3">Motivation</a></li> <li class="toc-entry toc-h3"><a href="#problem-setup">Problem Setup</a></li> <li class="toc-entry toc-h3"><a href="#recovery-via-optimization">Recovery via Optimization</a></li> <li class="toc-entry toc-h3"><a href="#lasso-relaxed-version">Lasso (Relaxed version)</a></li> <li class="toc-entry toc-h3"><a href="#applications-2">Applications</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-sparse-coding">3. Sparse Coding</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-8">Definition</a></li> <li class="toc-entry toc-h3"><a href="#optimization-objective">Optimization Objective</a></li> <li class="toc-entry toc-h3"><a href="#interpretation">Interpretation</a></li> <li class="toc-entry toc-h3"><a href="#applications-3">Applications</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#summary-1">Summary</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#numerical-stability-and-conditioning-in-machine-learning">Numerical Stability and Conditioning in Machine Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#1-matrix-conditioning-and-condition-numbers">1. Matrix Conditioning and Condition Numbers</a> <ul> <li class="toc-entry toc-h3"><a href="#definition-9">Definition</a></li> <li class="toc-entry toc-h3"><a href="#application-linear-systems-and-inversion-stability">Application: Linear Systems and Inversion Stability</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#2-instability-in-optimization">2. Instability in Optimization</a> <ul> <li class="toc-entry toc-h3"><a href="#gradient-descent-sensitivity">Gradient Descent Sensitivity</a></li> <li class="toc-entry toc-h3"><a href="#application-neural-network-training">Application: Neural Network Training</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#3-tikhonov-regularization">3. Tikhonov Regularization</a> <ul> <li class="toc-entry toc-h3"><a href="#theory">Theory</a></li> <li class="toc-entry toc-h3"><a href="#application-ill-posed-inverse-problems">Application: Ill-posed Inverse Problems</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-ridge-regression">4. Ridge Regression</a> <ul> <li class="toc-entry toc-h3"><a href="#theory-1">Theory</a></li> <li class="toc-entry toc-h3"><a href="#application-high-dimensional-linear-models">Application: High-Dimensional Linear Models</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#5-conditioning-in-deep-learning">5. Conditioning in Deep Learning</a> <ul> <li class="toc-entry toc-h3"><a href="#common-problems">Common Problems</a></li> <li class="toc-entry toc-h3"><a href="#solutions-and-their-mathematical-roles">Solutions and Their Mathematical Roles</a></li> <li class="toc-entry toc-h3"><a href="#application-training-stability-in-deep-networks">Application: Training Stability in Deep Networks</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#summary-2">Summary</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#rotation-reflection-and-markov-matrices-in-machine-learning">Rotation, Reflection, and Markov Matrices in Machine Learning</a> <ul> <li class="toc-entry toc-h2"> <a href="#rotation-and-reflection-matrices">Rotation and Reflection Matrices</a> <ul> <li class="toc-entry toc-h3"> <a href="#rotation-matrices">Rotation Matrices</a> <ul> <li class="toc-entry toc-h4"><a href="#2d-rotation">2D Rotation</a></li> <li class="toc-entry toc-h4"><a href="#3d-rotation-and-euler-angles">3D Rotation and Euler Angles</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#application-computer-vision-and-robotics">Application: Computer Vision and Robotics</a></li> <li class="toc-entry toc-h3"> <a href="#reflection-matrices">Reflection Matrices</a> <ul> <li class="toc-entry toc-h4"><a href="#reflection-in-2d">Reflection in 2D</a></li> </ul> </li> <li class="toc-entry toc-h3"><a href="#application-data-augmentation-in-cv">Application: Data Augmentation in CV</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#markov-matrices-and-stochastic-processes">Markov Matrices and Stochastic Processes</a> <ul> <li class="toc-entry toc-h3"><a href="#stochastic-matrices">Stochastic Matrices</a></li> <li class="toc-entry toc-h3"><a href="#stationary-distribution">Stationary Distribution</a></li> <li class="toc-entry toc-h3"><a href="#application-pagerank-and-random-walks">Application: PageRank and Random Walks</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#summary-3">Summary</a></li> </ul> </li> <li class="toc-entry toc-h1"> <a href="#advanced-projections-random-projections-and-the-johnsonlindenstrauss-lemma">Advanced Projections: Random Projections and the Johnson–Lindenstrauss Lemma</a> <ul> <li class="toc-entry toc-h2"><a href="#1-the-need-for-dimensionality-reduction">1. The Need for Dimensionality Reduction</a></li> <li class="toc-entry toc-h2"><a href="#2-random-projections">2. Random Projections</a></li> <li class="toc-entry toc-h2"> <a href="#3-johnsonlindenstrauss-lemma">3. Johnson–Lindenstrauss Lemma</a> <ul> <li class="toc-entry toc-h3"><a href="#theorem-jl-lemma">Theorem (JL Lemma)</a></li> <li class="toc-entry toc-h3"><a href="#intuition-1">Intuition</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#4-construction-of-projection-matrix">4. Construction of Projection Matrix</a> <ul> <li class="toc-entry toc-h3"><a href="#41-gaussian-random-projection">4.1 Gaussian Random Projection</a></li> <li class="toc-entry toc-h3"><a href="#42-sparse-random-projection">4.2 Sparse Random Projection</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#5-applications">5. Applications</a> <ul> <li class="toc-entry toc-h3"><a href="#51-natural-language-processing-nlp">5.1 Natural Language Processing (NLP)</a></li> <li class="toc-entry toc-h3"><a href="#52-information-retrieval-and-ann-search">5.2 Information Retrieval and ANN Search</a></li> <li class="toc-entry toc-h3"><a href="#53-differential-privacy-and-data-privacy">5.3 Differential Privacy and Data Privacy</a></li> <li class="toc-entry toc-h3"><a href="#54-kernel-approximation">5.4 Kernel Approximation</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#6-comparison-with-pca">6. Comparison with PCA</a></li> <li class="toc-entry toc-h2"><a href="#summary-4">Summary</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <h1 id="advanced-matrix-factorizations-in-machine-learning">Advanced Matrix Factorizations in Machine Learning</h1> <p>Matrix factorizations are a foundational tool in linear algebra and play a critical role in modern machine learning. They simplify matrix computations, enable numerical stability, and reveal latent structures in data. This post explores several advanced matrix factorizations used in machine learning and deep learning, with complete mathematical derivations and practical insights.</p> <p>This section covers:</p> <ul> <li>LU Decomposition</li> <li>Cholesky Decomposition</li> <li>QR Decomposition</li> <li>Non-negative Matrix Factorization (NMF)</li> </ul> <hr> <h2 id="1-lu-decomposition">1. LU Decomposition</h2> <h3 id="definition">Definition</h3> <p>LU decomposition factors a square matrix \(A \in \mathbb{R}^{n \times n}\) into the product of two matrices:</p> \[A = LU\] <p>Where:</p> <ul> <li>\(L\) is a lower triangular matrix with ones on its diagonal (\(L_{ii} = 1\)),</li> <li>\(U\) is an upper triangular matrix.</li> </ul> <p>If row pivoting is required, the decomposition is written as:</p> \[PA = LU\] <p>Where \(P\) is a permutation matrix.</p> <h3 id="algorithm">Algorithm</h3> <p>Given:</p> \[A = \begin{bmatrix} a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\ a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ a_{n1} &amp; a_{n2} &amp; \cdots &amp; a_{nn} \end{bmatrix},\] <p>we use Gaussian elimination to eliminate entries below the diagonal. The multipliers used are stored in \(L\), and the remaining upper triangular form becomes \(U\).</p> <p>For each row \(j &gt; i\), compute the multiplier:</p> \[l_{ji} = \frac{a_{ji}}{a_{ii}}\] <p>Then update row \(j\):</p> \[a_{j\cdot} = a_{j\cdot} - l_{ji} \cdot a_{i\cdot}\] <p>After completing all iterations, the matrix \(A\) is factorized into \(L\) and \(U\).</p> <h3 id="applications-in-machine-learning">Applications in Machine Learning</h3> <ul> <li>Efficiently solving systems of equations \(Ax = b\) by solving \(Ly = b\) then \(Ux = y\),</li> <li>Matrix inversion,</li> <li>Numerical optimization routines in regression and convex problems.</li> </ul> <hr> <h2 id="2-cholesky-decomposition">2. Cholesky Decomposition</h2> <h3 id="definition-1">Definition</h3> <p>Cholesky decomposition applies to <strong>symmetric, positive-definite</strong> matrices. For a matrix \(A \in \mathbb{R}^{n \times n}\) satisfying:</p> <ul> <li>\(A = A^T\) (symmetry),</li> <li>\(x^T A x &gt; 0\) for all non-zero \(x \in \mathbb{R}^n\) (positive-definiteness),</li> </ul> <p>there exists a unique lower triangular matrix \(L\) such that:</p> \[A = LL^T\] <h3 id="construction">Construction</h3> <p>We compute each entry of \(L\) as follows:</p> <p>For diagonal entries:</p> \[L_{ii} = \sqrt{A_{ii} - \sum_{k=1}^{i-1} L_{ik}^2}\] <p>For off-diagonal entries where \(i &gt; j\):</p> \[L_{ij} = \frac{1}{L_{jj}} \left( A_{ij} - \sum_{k=1}^{j-1} L_{ik} L_{jk} \right)\] <p>All entries above the diagonal are zero.</p> <h3 id="applications-in-machine-learning-1">Applications in Machine Learning</h3> <ul> <li>Efficient sampling from multivariate Gaussian distributions,</li> <li>Gaussian Processes (for inverting the kernel matrix),</li> <li>Kalman filters and Bayesian updates.</li> </ul> <hr> <h2 id="3-qr-decomposition">3. QR Decomposition</h2> <h3 id="definition-2">Definition</h3> <p>For any matrix \(A \in \mathbb{R}^{m \times n}\) where \(m \geq n\), the QR decomposition expresses \(A\) as:</p> \[A = QR\] <p>Where:</p> <ul> <li>\(Q \in \mathbb{R}^{m \times m}\) is orthogonal (\(Q^T Q = I\)),</li> <li>\(R \in \mathbb{R}^{m \times n}\) is upper triangular.</li> </ul> <p>For economy-size decomposition (when only \(n\) orthogonal vectors are needed), we use:</p> \[A = Q_{\text{red}} R, \quad Q_{\text{red}} \in \mathbb{R}^{m \times n}\] <h3 id="gram-schmidt-orthogonalization">Gram-Schmidt Orthogonalization</h3> <p>Let \(a_1, a_2, \ldots, a_n\) be the columns of \(A\). We generate orthonormal vectors \(q_1, q_2, \ldots, q_n\) using:</p> <ol> <li>Initialization:</li> </ol> \[u_1 = a_1, \quad q_1 = \frac{u_1}{\|u_1\|}\] <ol> <li>For \(k = 2, \ldots, n\):</li> </ol> \[u_k = a_k - \sum_{j=1}^{k-1} \langle a_k, q_j \rangle q_j\] \[q_k = \frac{u_k}{\|u_k\|}\] <p>Then define:</p> \[Q = [q_1, q_2, \ldots, q_n], \quad R = Q^T A\] <h3 id="least-squares-via-qr">Least Squares via QR</h3> <p>Given an overdetermined system \(Ax = b\), we solve:</p> \[A = QR \Rightarrow QRx = b\] <p>Then:</p> \[Rx = Q^T b\] <p>Since \(R\) is upper triangular, this is solved efficiently via back substitution.</p> <h3 id="applications-in-machine-learning-2">Applications in Machine Learning</h3> <ul> <li>Numerically stable solution for linear regression,</li> <li>Eigenvalue computations using the QR algorithm,</li> <li>Orthonormal basis construction.</li> </ul> <hr> <h2 id="4-non-negative-matrix-factorization-nmf">4. Non-negative Matrix Factorization (NMF)</h2> <h3 id="definition-3">Definition</h3> <p>Given a non-negative matrix \(A \in \mathbb{R}^{m \times n}\) (i.e., \(A_{ij} \geq 0\)), NMF seeks matrices \(W \in \mathbb{R}^{m \times k}\) and \(H \in \mathbb{R}^{k \times n}\) such that:</p> \[A \approx WH\] <p>subject to:</p> \[W \geq 0, \quad H \geq 0\] <h3 id="optimization-problem">Optimization Problem</h3> <p>The factorization is found by solving:</p> \[\min_{W, H \geq 0} \|A - WH\|_F^2\] <p>where \(\|\cdot\|_F\) denotes the Frobenius norm.</p> <h3 id="multiplicative-update-rules">Multiplicative Update Rules</h3> <p>A common approach (Lee &amp; Seung, 2001) involves the following update rules:</p> <ol> <li>Update \(H\):</li> </ol> \[H_{ij} \leftarrow H_{ij} \cdot \frac{(W^T A)_{ij}}{(W^T W H)_{ij}}\] <ol> <li>Update \(W\):</li> </ol> \[W_{ij} \leftarrow W_{ij} \cdot \frac{(A H^T)_{ij}}{(W H H^T)_{ij}}\] <p>These updates are applied iteratively until convergence.</p> <h3 id="applications-in-machine-learning-3">Applications in Machine Learning</h3> <ul> <li> <strong>Topic modeling</strong> from document-term matrices,</li> <li> <strong>Collaborative filtering</strong> in recommendation systems,</li> <li> <strong>Image compression and decomposition</strong>,</li> <li> <strong>Clustering</strong> with parts-based representation.</li> </ul> <hr> <h1 id="orthogonalization-techniques-in-machine-learning-and-deep-learning">Orthogonalization Techniques in Machine Learning and Deep Learning</h1> <p>Orthogonalization plays a central role in linear algebra and is extensively used in various machine learning and deep learning tasks. Whether it’s <strong>constructing orthonormal bases</strong>, <strong>decorrelating features</strong>, or <strong>stabilizing neural network training</strong>, orthogonal structures are powerful due to their numerical and geometric properties.</p> <p>Now we shall cover:</p> <ul> <li>The <strong>Gram-Schmidt orthogonalization process</strong> for constructing orthonormal bases,</li> <li> <strong>Orthogonal initialization</strong> in neural networks for improved stability and convergence.</li> </ul> <hr> <h2 id="1-orthogonalization-and-orthonormal-bases">1. Orthogonalization and Orthonormal Bases</h2> <h3 id="motivation">Motivation</h3> <p>Given a set of linearly independent vectors \(\{v_1, v_2, \ldots, v_n\}\) in \(\mathbb{R}^n\), it is often desirable to construct an orthonormal basis \(\{q_1, q_2, \ldots, q_n\}\) that spans the same subspace, where:</p> <ul> <li>Vectors are <strong>orthogonal</strong>: \(\langle q_i, q_j \rangle = 0\) for \(i \ne j\),</li> <li>Vectors are <strong>normalized</strong>: \(\|q_i\| = 1\).</li> </ul> <p>Orthonormal bases are easier to work with:</p> <ul> <li>Projections are straightforward,</li> <li>Matrix representations (e.g., \(Q^T Q = I\)) are numerically stable,</li> <li>Useful for dimensionality reduction and decorrelation (e.g., PCA).</li> </ul> <hr> <h2 id="2-the-gram-schmidt-process">2. The Gram-Schmidt Process</h2> <h3 id="definition-4">Definition</h3> <p>The <strong>Gram-Schmidt process</strong> transforms a set of linearly independent vectors \(\{v_1, v_2, \ldots, v_n\}\) into an orthonormal set \(\{q_1, q_2, \ldots, q_n\}\) spanning the same subspace.</p> <p>This is achieved by iteratively subtracting projections onto previously computed vectors and normalizing.</p> <hr> <h3 id="step-by-step-algorithm">Step-by-step Algorithm</h3> <p>Let the input be vectors \(v_1, v_2, \ldots, v_n \in \mathbb{R}^d\).</p> <ol> <li> <strong>Initialize</strong> the first orthonormal vector:</li> </ol> \[q_1 = \frac{v_1}{\|v_1\|}\] <ol> <li> <p><strong>Iterate</strong> for \(k = 2, \ldots, n\):</p> <ul> <li>Project \(v_k\) onto each previous \(q_j\):</li> </ul> \[\text{proj}_{q_j}(v_k) = \langle v_k, q_j \rangle q_j\] <ul> <li>Subtract the projections:</li> </ul> \[u_k = v_k - \sum_{j=1}^{k-1} \langle v_k, q_j \rangle q_j\] <ul> <li>Normalize to get the next orthonormal vector:</li> </ul> \[q_k = \frac{u_k}{\|u_k\|}\] </li> </ol> <p>After the process, the vectors \(\{q_1, \ldots, q_n\}\) form an <strong>orthonormal basis</strong> of the span of \(\{v_1, \ldots, v_n\}\).</p> <hr> <h3 id="example">Example</h3> <p>Let:</p> \[v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad v_2 = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\] <p>Compute:</p> \[q_1 = \frac{v_1}{\|v_1\|} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}\] <p>Project \(v_2\) onto \(q_1\):</p> \[\langle v_2, q_1 \rangle = \frac{1}{\sqrt{2}}(1 \cdot 1 + 0 \cdot 1) = \frac{1}{\sqrt{2}}\] <p>Then:</p> \[u_2 = v_2 - \langle v_2, q_1 \rangle q_1 = \begin{bmatrix} 1 \\ 0 \end{bmatrix} - \frac{1}{\sqrt{2}} \cdot \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} - \frac{1}{2} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} \frac{1}{2} \\ -\frac{1}{2} \end{bmatrix}\] <p>Normalize:</p> \[q_2 = \frac{u_2}{\|u_2\|} = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}\] <p>So the orthonormal basis is:</p> \[q_1 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ 1 \end{bmatrix}, \quad q_2 = \frac{1}{\sqrt{2}} \begin{bmatrix} 1 \\ -1 \end{bmatrix}\] <hr> <h3 id="applications">Applications</h3> <ul> <li> <strong>QR Decomposition</strong>: Gram-Schmidt is used to compute orthogonal matrix \(Q\) in QR.</li> <li> <strong>PCA and SVD</strong>: Constructs orthonormal eigenvectors or singular vectors.</li> <li> <strong>Feature Embeddings</strong>: Ensures orthogonality between embedding dimensions for decorrelation.</li> </ul> <hr> <h2 id="3-orthogonal-initialization-in-neural-networks">3. Orthogonal Initialization in Neural Networks</h2> <h3 id="motivation-1">Motivation</h3> <p>Deep networks are sensitive to weight initialization. Poorly chosen initializations can lead to:</p> <ul> <li>Vanishing or exploding gradients,</li> <li>Poor convergence,</li> <li>Suboptimal generalization.</li> </ul> <p>Orthogonal initialization addresses these issues by ensuring that:</p> <ul> <li>The weight matrix \(W\) preserves the norm of the signal during forward and backward passes,</li> <li>Gradients are not distorted across layers.</li> </ul> <hr> <h3 id="mathematical-principle">Mathematical Principle</h3> <p>Let \(W \in \mathbb{R}^{n \times n}\) be an orthogonal matrix, so that:</p> \[W^T W = WW^T = I\] <p>If input \(x \in \mathbb{R}^n\) is passed through such a weight matrix:</p> \[y = Wx \Rightarrow \|y\|_2 = \|x\|_2\] <p>Hence, the norm is preserved, avoiding scaling issues layer by layer.</p> <p>In backpropagation, the gradient flow also remains stable:</p> \[\frac{\partial L}{\partial x} = W^T \frac{\partial L}{\partial y}\] <p>If \(W\) is orthogonal, the gradient is rotated but not scaled, avoiding vanishing/exploding gradients.</p> <hr> <h3 id="how-to-initialize-orthogonal-matrices">How to Initialize Orthogonal Matrices</h3> <ul> <li>Generate a random matrix \(A\),</li> <li>Perform QR decomposition: \(A = QR\),</li> <li>Use \(Q\) as the initialization matrix (optionally scale by a factor \(\sigma\)):</li> </ul> \[W = \sigma Q\] <h3 id="implementing-in-deep-learning-libraries">Implementing in Deep Learning Libraries</h3> <p>In PyTorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">orthogonal_</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div> <p>In TensorFlow:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nc">Orthogonal</span><span class="p">(</span><span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div> <h3 id="applications-1">Applications</h3> <ul> <li> <strong>Recurrent Neural Networks (RNNs)</strong>: Orthogonal or unitary weight matrices are crucial for long-term memory retention.</li> <li> <strong>Deep Fully Connected Networks</strong>: Improves training dynamics for very deep MLPs.</li> <li> <strong>Transformer Layers</strong>: Can help in initializing dense layers to preserve signal variance.</li> </ul> <hr> <p>Orthogonalization is not only a theoretical concept but also a practical tool that:</p> <ul> <li>Enhances numerical stability,</li> <li>Helps decorrelate features,</li> <li>Enables better gradient flow in deep neural networks.</li> </ul> <p>Whether through <strong>Gram-Schmidt orthogonalization</strong> for structured bases or <strong>orthogonal initialization</strong> for training deep networks, mastering these tools improves both understanding and implementation of modern ML models.</p> <hr> <h1 id="kernel-methods-and-hilbert-spaces-in-machine-learning">Kernel Methods and Hilbert Spaces in Machine Learning</h1> <p>Kernel methods are powerful tools that enable learning algorithms to operate in <strong>high-dimensional feature spaces</strong> without explicitly computing those spaces. They are foundational to algorithms like <strong>Support Vector Machines (SVMs)</strong>, <strong>Gaussian Processes (GPs)</strong>, and <strong>Kernel PCA</strong>. This is achieved through the <strong>kernel trick</strong> and formalized via the theory of <strong>Hilbert spaces</strong> and <strong>Mercer’s theorem</strong>.</p> <p>In this section, we will cover:</p> <ul> <li>Inner products in high-dimensional (possibly infinite-dimensional) spaces,</li> <li>The concept of Reproducing Kernel Hilbert Spaces (RKHS),</li> <li>The kernel trick and its use in ML algorithms,</li> <li>Mercer’s theorem and the mathematical foundation of kernels.</li> </ul> <hr> <h2 id="1-inner-products-in-high-dimensional-spaces">1. Inner Products in High-Dimensional Spaces</h2> <h3 id="motivation-2">Motivation</h3> <p>In many ML algorithms, we compute <strong>inner products</strong> between feature vectors:</p> \[\langle x, x' \rangle = x^\top x'\] <p>However, linear models using this inner product are limited to <strong>linear decision boundaries</strong>.</p> <p>To handle <strong>non-linear patterns</strong>, we can <strong>map</strong> the inputs into a higher-dimensional space:</p> \[\phi: \mathcal{X} \rightarrow \mathcal{H}, \quad x \mapsto \phi(x)\] <p>where \(\mathcal{H}\) is a <strong>Hilbert space</strong> (a complete inner product space). Then, inner products become:</p> \[\langle \phi(x), \phi(x') \rangle\] <p>If \(\phi\) maps into a <strong>very high-dimensional</strong> or <strong>infinite-dimensional</strong> space, computation becomes infeasible. However, we can avoid this cost using <strong>kernels</strong>.</p> <hr> <h2 id="2-kernel-functions-and-the-kernel-trick">2. Kernel Functions and the Kernel Trick</h2> <h3 id="definition-5">Definition</h3> <p>A <strong>kernel function</strong> \(k: \mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}\) is defined as:</p> \[k(x, x') = \langle \phi(x), \phi(x') \rangle_{\mathcal{H}}\] <p>This allows computing the inner product <strong>without explicitly computing</strong> \(\phi(x)\) or \(\phi(x')\).</p> <p>This is known as the <strong>kernel trick</strong>.</p> <h3 id="examples-of-kernel-functions">Examples of Kernel Functions</h3> <ul> <li> <strong>Linear kernel</strong>:</li> </ul> \[k(x, x') = x^\top x'\] <ul> <li> <strong>Polynomial kernel</strong> (degree \(d\)):</li> </ul> \[k(x, x') = (x^\top x' + c)^d\] <ul> <li> <strong>Radial Basis Function (RBF) / Gaussian kernel</strong>:</li> </ul> \[k(x, x') = \exp\left(-\frac{\|x - x'\|^2}{2\sigma^2}\right)\] <ul> <li> <strong>Sigmoid kernel</strong>:</li> </ul> \[k(x, x') = \tanh(\alpha x^\top x' + c)\] <hr> <h3 id="why-it-works">Why It Works</h3> <p>Many algorithms (like SVMs) rely on computing dot products between data points:</p> <ul> <li>Training: \(x_i^\top x_j\)</li> <li>Prediction: \(x^\top x_i\)</li> </ul> <p>By replacing these with \(k(x_i, x_j)\) and \(k(x, x_i)\), we effectively operate in the high-dimensional space <strong>without ever computing it</strong> explicitly.</p> <p>This is computationally and memory-efficient and allows fitting complex, non-linear decision boundaries.</p> <hr> <h2 id="3-reproducing-kernel-hilbert-space-rkhs">3. Reproducing Kernel Hilbert Space (RKHS)</h2> <h3 id="definition-6">Definition</h3> <p>A <strong>Reproducing Kernel Hilbert Space</strong> is a Hilbert space \(\mathcal{H}\) of functions from \(\mathcal{X} \rightarrow \mathbb{R}\) such that:</p> <ol> <li>For all \(x \in \mathcal{X}\), the evaluation functional \(f \mapsto f(x)\) is continuous.</li> <li>There exists a <strong>kernel function</strong> \(k(x, \cdot) \in \mathcal{H}\) such that for all \(f \in \mathcal{H}\):</li> </ol> \[f(x) = \langle f, k(x, \cdot) \rangle_{\mathcal{H}}\] <p>This is called the <strong>reproducing property</strong>, and it guarantees that the kernel uniquely defines the Hilbert space.</p> <h3 id="intuition">Intuition</h3> <p>RKHS is the space induced by the kernel. Every function in the RKHS can be written in terms of kernels evaluated at training points. This leads to <strong>kernel representer theorems</strong> and simplifies optimization.</p> <hr> <h2 id="4-mercers-theorem">4. Mercer’s Theorem</h2> <h3 id="statement">Statement</h3> <p>Mercer’s Theorem provides a condition under which a function is a <strong>valid kernel</strong> (i.e., corresponds to an inner product in some Hilbert space).</p> <p>Let \(k(x, x')\) be a continuous, symmetric, and positive semi-definite kernel on a compact domain \(\mathcal{X} \subset \mathbb{R}^n\). Then:</p> <ul> <li>There exists a sequence of <strong>orthonormal eigenfunctions</strong> \(\{\phi_i\}\) and <strong>non-negative eigenvalues</strong> \(\{\lambda_i\}\) such that:</li> </ul> \[k(x, x') = \sum_{i=1}^{\infty} \lambda_i \phi_i(x) \phi_i(x')\] <p>This shows that a kernel corresponds to an inner product in a (possibly infinite-dimensional) feature space.</p> <h3 id="practical-use">Practical Use</h3> <p>Mercer’s Theorem justifies that functions like the RBF or polynomial kernel are <strong>valid kernels</strong> and thus induce a real Hilbert space with meaningful inner products.</p> <hr> <h2 id="5-applications-in-machine-learning">5. Applications in Machine Learning</h2> <h3 id="51-support-vector-machines-svms">5.1 Support Vector Machines (SVMs)</h3> <p>The dual form of the SVM optimization problem involves only dot products. With a kernel function:</p> \[\text{maximize}_\alpha \sum_i \alpha_i - \frac{1}{2} \sum_{i,j} \alpha_i \alpha_j y_i y_j k(x_i, x_j)\] <p>Predictions are made using:</p> \[f(x) = \sum_i \alpha_i y_i k(x, x_i) + b\] <p>This enables <strong>non-linear classification</strong> by implicitly mapping into a high-dimensional space.</p> <hr> <h3 id="52-gaussian-processes-gps">5.2 Gaussian Processes (GPs)</h3> <p>A Gaussian Process is a distribution over functions:</p> \[f(x) \sim \mathcal{GP}(0, k(x, x'))\] <p>The kernel function \(k\) defines the <strong>covariance</strong> between function values. This allows modeling smoothness, periodicity, and other function properties.</p> <p>Prediction involves computing:</p> \[\mathbb{E}[f(x_*)] = k(x_*, X) [K(X, X) + \sigma^2 I]^{-1} y\] <p>Where:</p> <ul> <li>\(K(X, X)\) is the Gram matrix over the training data using kernel \(k\),</li> <li>\(x_*\) is the test input.</li> </ul> <hr> <h3 id="53-kernel-pca">5.3 Kernel PCA</h3> <p>Standard PCA uses the covariance matrix:</p> \[C = \frac{1}{n} \sum_{i=1}^n x_i x_i^T\] <p>Kernel PCA generalizes this using the kernel trick by computing the <strong>kernel Gram matrix</strong>:</p> \[K_{ij} = k(x_i, x_j)\] <p>Then perform eigen-decomposition on the <strong>centered</strong> kernel matrix:</p> \[K_c = K - \mathbf{1}K - K\mathbf{1} + \mathbf{1}K\mathbf{1}\] <p>Where \(\mathbf{1}\) is the centering matrix.</p> <p>The result is non-linear dimensionality reduction using kernel-defined similarities.</p> <hr> <h2 id="summary">Summary</h2> <table> <thead> <tr> <th style="text-align: left">Concept</th> <th style="text-align: center">Mathematical Foundation</th> <th style="text-align: right">Role in Machine Learning</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Feature Mapping</td> <td style="text-align: center">\(\phi: x \mapsto \mathcal{H}\)</td> <td style="text-align: right">Transforms data into high-dimensional space</td> </tr> <tr> <td style="text-align: left">Kernel Function</td> <td style="text-align: center">\(k(x, x') = \langle \phi(x), \phi(x') \rangle\)</td> <td style="text-align: right">Computes inner products without explicit mapping</td> </tr> <tr> <td style="text-align: left">RKHS</td> <td style="text-align: center">Function space induced by a valid kernel</td> <td style="text-align: right">Guarantees expressiveness and optimization theory</td> </tr> <tr> <td style="text-align: left">Mercer’s Theorem</td> <td style="text-align: center">\(k(x, x') = \sum \lambda_i \phi_i(x) \phi_i(x')\)</td> <td style="text-align: right">Validates kernel as inner product in Hilbert space</td> </tr> <tr> <td style="text-align: left">Kernel Trick</td> <td style="text-align: center">Replace \(\langle x, x' \rangle\) with \(k(x, x')\)</td> <td style="text-align: right">Enables non-linear learning with linear algorithms</td> </tr> </tbody> </table> <hr> <p>Kernel methods let us apply powerful linear algorithms in high-dimensional non-linear spaces—<strong>without computing those spaces directly</strong>. The kernel trick, rooted in Hilbert space theory and Mercer’s theorem, forms the backbone of some of the most elegant and effective machine learning algorithms.</p> <hr> <h1 id="sparse-matrices-and-sparsity-in-machine-learning">Sparse Matrices and Sparsity in Machine Learning</h1> <p>Modern machine learning applications often involve <strong>high-dimensional data</strong>, where most entries are <strong>zero</strong>. In such settings, <strong>sparsity</strong> becomes a powerful property to exploit—both for <strong>computational efficiency</strong> and for <strong>improving generalization</strong>.</p> <p>This section explores:</p> <ul> <li>Sparse matrix representation and storage,</li> <li>The role of sparsity in large-scale ML problems,</li> <li>Compressed sensing and sparse coding,</li> <li>Applications in recommender systems, NLP, computer vision, and feature selection.</li> </ul> <hr> <h2 id="1-sparse-representations">1. Sparse Representations</h2> <h3 id="definition-7">Definition</h3> <p>A matrix \(A \in \mathbb{R}^{m \times n}\) is called <strong>sparse</strong> if most of its entries are zero:</p> \[\text{Sparsity}(A) = \frac{\text{Number of zero entries}}{mn} \gg 0\] <p>Equivalently, it has <strong>only a few non-zero entries</strong> compared to its total size.</p> <hr> <h3 id="storage-and-efficiency">Storage and Efficiency</h3> <p>Instead of storing all \(mn\) entries, we store only the non-zero elements and their indices. Common sparse matrix formats include:</p> <ul> <li> <p><strong>Compressed Sparse Row (CSR)</strong>: Stores non-zero values, column indices, and row pointer.</p> </li> <li> <p><strong>Compressed Sparse Column (CSC)</strong>: Similar to CSR, but column-wise.</p> </li> <li> <p><strong>Coordinate (COO)</strong>: Stores each non-zero entry as a tuple \((i, j, A_{ij})\).</p> </li> </ul> <p>These formats enable:</p> <ul> <li> <strong>O(1)</strong> access to non-zero elements,</li> <li>Efficient matrix-vector multiplication in \(O(\text{nnz})\) time,</li> <li>Reduced memory usage.</li> </ul> <hr> <h3 id="use-cases">Use Cases</h3> <ul> <li> <p><strong>Recommender Systems</strong>: User-item interaction matrices are sparse; most users rate very few items.</p> </li> <li> <p><strong>NLP</strong>: Bag-of-Words, TF-IDF, one-hot encoding—all result in sparse representations.</p> </li> <li> <p><strong>Graph ML</strong>: Adjacency matrices of large graphs (social networks, web graphs) are sparse.</p> </li> </ul> <hr> <h2 id="2-compressed-sensing">2. Compressed Sensing</h2> <h3 id="motivation-3">Motivation</h3> <p>Compressed sensing addresses the question:</p> <p><strong>Can we recover high-dimensional signals from few linear measurements if the signal is sparse?</strong></p> <p>The answer is yes—under specific conditions.</p> <hr> <h3 id="problem-setup">Problem Setup</h3> <p>Let:</p> <ul> <li>\(x \in \mathbb{R}^n\) be the <strong>original signal</strong>, sparse in some basis.</li> <li>\(y \in \mathbb{R}^m\) be the <strong>observed measurements</strong>, where \(m \ll n\).</li> <li>\(A \in \mathbb{R}^{m \times n}\) be a measurement matrix.</li> </ul> <p>We want to solve:</p> \[y = A x \quad \text{with} \quad x \text{ sparse}\] <p>Since \(m &lt; n\), this is underdetermined. But if \(x\) is sparse (i.e., \(\|x\|_0 \ll n\)), recovery is possible.</p> <hr> <h3 id="recovery-via-optimization">Recovery via Optimization</h3> <p>The sparse recovery problem is posed as:</p> \[\min_x \|x\|_0 \quad \text{subject to} \quad y = Ax\] <p>But this is NP-hard. Instead, we solve:</p> \[\min_x \|x\|_1 \quad \text{subject to} \quad y = Ax\] <p>This is known as <strong>Basis Pursuit</strong>, and under the <strong>Restricted Isometry Property (RIP)</strong>, it provably recovers the sparse signal.</p> <h3 id="lasso-relaxed-version">Lasso (Relaxed version)</h3> <p>If measurements are noisy:</p> \[y = Ax + \varepsilon\] <p>Then we solve:</p> \[\min_x \|y - Ax\|_2^2 + \lambda \|x\|_1\] <p>This is the <strong>Lasso</strong> (Least Absolute Shrinkage and Selection Operator) formulation.</p> <hr> <h3 id="applications-2">Applications</h3> <ul> <li> <strong>Medical imaging</strong>: MRI and CT scan reconstruction from fewer samples.</li> <li> <strong>Signal processing</strong>: Denoising, compression.</li> <li> <strong>NLP and CV</strong>: Learning compact word/image representations.</li> </ul> <hr> <h2 id="3-sparse-coding">3. Sparse Coding</h2> <h3 id="definition-8">Definition</h3> <p>Given an input signal \(x \in \mathbb{R}^d\), sparse coding assumes:</p> \[x \approx D h\] <p>Where:</p> <ul> <li>\(D \in \mathbb{R}^{d \times k}\) is a <strong>dictionary</strong> of basis vectors (atoms),</li> <li>\(h \in \mathbb{R}^k\) is a <strong>sparse code</strong> (i.e., few non-zero entries).</li> </ul> <p>The goal is to <strong>learn \(D\) and \(h\)</strong> such that \(x\) is well-represented with a <strong>sparse \(h\)</strong>.</p> <hr> <h3 id="optimization-objective">Optimization Objective</h3> <p>Given training data \(X = [x^{(1)}, \ldots, x^{(n)}]\), we solve:</p> \[\min_{D, H} \sum_{i=1}^n \left( \|x^{(i)} - D h^{(i)}\|_2^2 + \lambda \|h^{(i)}\|_1 \right)\] <p>Subject to normalization constraints on \(D\) (e.g., \(\|d_j\|_2 \leq 1\)).</p> <p>This problem is <strong>bi-convex</strong> and typically solved via alternating minimization:</p> <ol> <li>Fix \(D\), update sparse codes \(H\).</li> <li>Fix \(H\), update dictionary \(D\).</li> </ol> <hr> <h3 id="interpretation">Interpretation</h3> <ul> <li>Sparse coding learns <strong>overcomplete dictionaries</strong> that can represent inputs efficiently.</li> <li>The sparse codes \(h\) capture <strong>salient features</strong> with fewer active components.</li> </ul> <hr> <h3 id="applications-3">Applications</h3> <ul> <li> <strong>Image processing</strong>: <ul> <li>Denoising, super-resolution, texture synthesis.</li> </ul> </li> <li> <strong>NLP</strong>: <ul> <li>Sparse embedding representations.</li> </ul> </li> <li> <strong>Feature learning</strong>: <ul> <li>Unsupervised pretraining for deep networks.</li> </ul> </li> <li> <strong>Compression</strong>: <ul> <li>Reduces storage and computation for large models.</li> </ul> </li> </ul> <hr> <h2 id="summary-1">Summary</h2> <table> <thead> <tr> <th style="text-align: left">Concept</th> <th style="text-align: center">Mathematical Description</th> <th style="text-align: right">Applications</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Sparse Matrix</td> <td style="text-align: center">Mostly zero entries, stored in CSR/COO format</td> <td style="text-align: right">Recommender systems, NLP, graph ML</td> </tr> <tr> <td style="text-align: left">Compressed Sensing</td> <td style="text-align: center">\(y = Ax, |x|_0 \ll n\), recover \(x\) from \(y\)</td> <td style="text-align: right">Imaging, signal processing, low-data regimes</td> </tr> <tr> <td style="text-align: left">Lasso</td> <td style="text-align: center">\(\min_x |y - Ax|_2^2 + \lambda |x|_1\)</td> <td style="text-align: right">Feature selection, regularization</td> </tr> <tr> <td style="text-align: left">Sparse Coding</td> <td style="text-align: center">\(x \approx D h\) with sparse \(h\)</td> <td style="text-align: right">Feature learning, representation compression</td> </tr> </tbody> </table> <hr> <p>Sparsity is a crucial structural assumption in many ML settings. Whether it’s handling massive sparse data structures, recovering signals from limited observations, or learning compressed representations, <strong>sparsity enables scalable and interpretable learning</strong>.</p> <p>The underlying mathematical principles—\(\ell_0\) and \(\ell_1\) norms, underdetermined systems, and structured optimization—form the basis of compressed sensing and sparse coding.</p> <hr> <h1 id="numerical-stability-and-conditioning-in-machine-learning">Numerical Stability and Conditioning in Machine Learning</h1> <p>In large-scale machine learning models and numerical computations, <strong>numerical stability</strong> plays a vital role in ensuring accuracy, convergence, and generalization. When data is high-dimensional, features are correlated, or optimization problems are ill-posed, small numerical errors can lead to large deviations in results.</p> <p>This section explores:</p> <ul> <li>Matrix conditioning and condition numbers,</li> <li>Their effect on optimization and linear models,</li> <li>Regularization strategies like <strong>Tikhonov regularization</strong> and <strong>Ridge regression</strong>,</li> <li>Real-world applications after each theoretical concept.</li> </ul> <hr> <h2 id="1-matrix-conditioning-and-condition-numbers">1. Matrix Conditioning and Condition Numbers</h2> <h3 id="definition-9">Definition</h3> <p>Given a matrix \(A \in \mathbb{R}^{n \times n}\), its <strong>condition number</strong> in the 2-norm is:</p> \[\kappa(A) = \|A\|_2 \cdot \|A^{-1}\|_2\] <p>If \(A\) is symmetric positive definite:</p> \[\kappa(A) = \frac{\lambda_{\max}}{\lambda_{\min}}\] <p>Where \(\lambda_{\max}\) and \(\lambda_{\min}\) are the largest and smallest eigenvalues of \(A\).</p> <p>This value measures the sensitivity of the solution \(x\) to small perturbations in the system \(Ax = b\).</p> <hr> <h3 id="application-linear-systems-and-inversion-stability">Application: Linear Systems and Inversion Stability</h3> <p>In <strong>linear regression</strong> or <strong>least squares</strong>, solving:</p> \[\hat{x} = (X^T X)^{-1} X^T y\] <p>can be unstable if \(X^T X\) is ill-conditioned (e.g., due to multicollinearity).</p> <p><strong>Real-World Scenarios</strong>:</p> <ul> <li>High-dimensional regression models,</li> <li>Polynomial regression (where powers of features become highly correlated),</li> <li>PCA: covariance matrix conditioning affects eigenvalue computation.</li> </ul> <hr> <h2 id="2-instability-in-optimization">2. Instability in Optimization</h2> <h3 id="gradient-descent-sensitivity">Gradient Descent Sensitivity</h3> <p>Consider optimizing a quadratic loss:</p> \[f(x) = \frac{1}{2} x^T A x\] <p>Gradient descent update:</p> \[x_{k+1} = x_k - \eta A x_k\] <p>If \(A\) is ill-conditioned (i.e., eigenvalues vary widely), the optimization will:</p> <ul> <li>Converge slowly,</li> <li>Zigzag across the cost surface,</li> <li>Require small learning rates to remain stable.</li> </ul> <hr> <h3 id="application-neural-network-training">Application: Neural Network Training</h3> <p>In <strong>deep networks</strong>, layers may learn at drastically different rates due to ill-conditioning. Common symptoms include:</p> <ul> <li>Exploding or vanishing gradients,</li> <li>Slow convergence even on simple tasks,</li> <li>Difficulty tuning learning rates.</li> </ul> <p><strong>Example</strong>: In early training of deep MLPs or RNNs, poor weight scaling leads to ill-conditioned Jacobians and Hessians.</p> <hr> <h2 id="3-tikhonov-regularization">3. Tikhonov Regularization</h2> <h3 id="theory">Theory</h3> <p>To solve an ill-posed least squares problem:</p> \[\min_x \|Ax - b\|_2^2\] <p>we add a regularization term:</p> \[\min_x \|Ax - b\|_2^2 + \lambda \|x\|_2^2\] <p>This is <strong>Tikhonov regularization</strong>. The solution becomes:</p> \[\hat{x}_\lambda = (A^T A + \lambda I)^{-1} A^T b\] <p>This improves conditioning by ensuring the matrix being inverted is better-behaved.</p> <hr> <h3 id="application-ill-posed-inverse-problems">Application: Ill-posed Inverse Problems</h3> <p><strong>Tikhonov regularization</strong> is used in:</p> <ul> <li> <strong>Image deblurring and denoising</strong>,</li> <li> <strong>Medical imaging (MRI, CT)</strong>,</li> <li> <strong>Physics-based simulations</strong> with uncertain measurements.</li> </ul> <p>In ML, it improves:</p> <ul> <li> <strong>Matrix inversion stability</strong> in linear models,</li> <li> <strong>Numerical robustness</strong> in batch/mini-batch computations.</li> </ul> <hr> <h2 id="4-ridge-regression">4. Ridge Regression</h2> <h3 id="theory-1">Theory</h3> <p>Ridge regression is a specific case of Tikhonov regularization applied to linear regression. Given \(X \in \mathbb{R}^{n \times d}\) and \(y \in \mathbb{R}^n\):</p> \[\min_w \|Xw - y\|_2^2 + \lambda \|w\|_2^2\] <p>Solution:</p> \[w = (X^T X + \lambda I)^{-1} X^T y\] <p>Benefits:</p> <ul> <li>Stabilizes matrix inversion,</li> <li>Reduces overfitting in high-dimensional settings.</li> </ul> <hr> <h3 id="application-high-dimensional-linear-models">Application: High-Dimensional Linear Models</h3> <p>Ridge regression is essential when:</p> <ul> <li>The number of features exceeds the number of samples,</li> <li>Features are highly correlated (multicollinearity),</li> <li>Predictors are noisy or redundant.</li> </ul> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="n">model</span> <span class="o">=</span> <span class="nc">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>Common in:</p> <ul> <li> <strong>Genomics</strong> (p » n),</li> <li> <strong>Text regression models</strong> with bag-of-words features,</li> <li> <strong>Financial models</strong> with redundant indicators.</li> </ul> <hr> <h2 id="5-conditioning-in-deep-learning">5. Conditioning in Deep Learning</h2> <h3 id="common-problems">Common Problems</h3> <p>In deep neural networks:</p> <ul> <li>Weight matrices may become poorly conditioned,</li> <li>Gradients may vanish or explode during backpropagation,</li> <li>Activations may saturate, leading to optimization stalls.</li> </ul> <hr> <h3 id="solutions-and-their-mathematical-roles">Solutions and Their Mathematical Roles</h3> <ol> <li> <p><strong>Orthogonal Initialization</strong>: Weight matrices initialized to be orthogonal preserve input norm and maintain conditioning.</p> \[W^T W = I \Rightarrow \|Wx\|_2 = \|x\|_2\] <p><strong>Code</strong>:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="nf">orthogonal_</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>Weight Decay</strong> (L2 regularization): Equivalent to Ridge on weights. Controls weight growth, stabilizes learning.</p> \[\min_w \mathcal{L}(w) + \lambda \|w\|_2^2\] </li> <li> <p><strong>Gradient Clipping</strong>: Prevents gradient explosion by clipping:</p> \[\nabla \mathcal{L} \leftarrow \frac{\nabla \mathcal{L}}{\max(1, \|\nabla \mathcal{L}\| / \tau)}\] <p><strong>Code</strong>:</p> <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
</code></pre></div> </div> </li> </ol> <hr> <h3 id="application-training-stability-in-deep-networks">Application: Training Stability in Deep Networks</h3> <p>These techniques are used in:</p> <ul> <li> <strong>RNNs</strong>: To avoid exploding gradients through time.</li> <li> <strong>Transformers</strong>: LayerNorm and initialization stabilize long-depth training.</li> <li> <strong>CNNs</strong>: Weight decay improves generalization and convergence.</li> </ul> <hr> <h2 id="summary-2">Summary</h2> <table> <thead> <tr> <th style="text-align: left">Concept</th> <th style="text-align: center">Mathematical Description</th> <th style="text-align: right">Role in ML / DL</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Condition Number</td> <td style="text-align: center">\(\kappa(A) = |A| \cdot |A^{-1}|\)</td> <td style="text-align: right">Measures sensitivity to noise</td> </tr> <tr> <td style="text-align: left">Ill-conditioning</td> <td style="text-align: center">\(\kappa(A) \gg 1\)</td> <td style="text-align: right">Leads to instability in training and optimization</td> </tr> <tr> <td style="text-align: left">Tikhonov Regularization</td> <td style="text-align: center">\(\min_x |Ax - b|^2 + \lambda |x|^2\)</td> <td style="text-align: right">Improves matrix invertibility</td> </tr> <tr> <td style="text-align: left">Ridge Regression</td> <td style="text-align: center">\(w = (X^T X + \lambda I)^{-1} X^T y\)</td> <td style="text-align: right">Stabilizes regression with correlated features</td> </tr> <tr> <td style="text-align: left">Orthogonal Init.</td> <td style="text-align: center">\(W^T W = I\)</td> <td style="text-align: right">Preserves norm in forward/backward pass</td> </tr> <tr> <td style="text-align: left">Weight Decay</td> <td style="text-align: center">\(\min_w \mathcal{L} + \lambda |w|^2\)</td> <td style="text-align: right">Regularizes weights and enhances generalization</td> </tr> <tr> <td style="text-align: left">Gradient Clipping</td> <td style="text-align: center">\(\nabla \mathcal{L} \rightarrow \frac{\nabla \mathcal{L}}{\max(\cdot)}\)</td> <td style="text-align: right">Prevents exploding gradients during training</td> </tr> </tbody> </table> <hr> <p>Understanding numerical stability and matrix conditioning helps you:</p> <ul> <li>Design models that train efficiently,</li> <li>Use optimization methods that converge reliably,</li> <li>Avoid silent failures due to ill-conditioning.</li> </ul> <p>By incorporating <strong>regularization</strong>, <strong>better initialization</strong>, and <strong>gradient control</strong>, you can ensure your machine learning models are not only performant—but also numerically robust.</p> <hr> <h1 id="rotation-reflection-and-markov-matrices-in-machine-learning">Rotation, Reflection, and Markov Matrices in Machine Learning</h1> <p>Geometric and probabilistic transformations form the backbone of many machine learning systems—especially in computer vision, robotics, and probabilistic reasoning. This section explores two foundational categories of matrices:</p> <ol> <li> <strong>Rotation and Reflection matrices</strong> (used in spatial transformations),</li> <li> <strong>Markov (Stochastic) matrices</strong> (used in probabilistic models and temporal systems).</li> </ol> <hr> <h2 id="rotation-and-reflection-matrices">Rotation and Reflection Matrices</h2> <p>Geometric transformations such as rotation and reflection are represented by orthogonal matrices in linear algebra. These are crucial for tasks in <strong>computer vision</strong>, <strong>robotics</strong>, <strong>3D graphics</strong>, and <strong>data augmentation</strong>.</p> <hr> <h3 id="rotation-matrices">Rotation Matrices</h3> <p>A <strong>rotation matrix</strong> in \(\mathbb{R}^n\) rotates a vector about the origin while preserving its norm. A matrix \(R\) is a rotation matrix if:</p> \[R^T R = RR^T = I \quad \text{and} \quad \det(R) = 1\] <h4 id="2d-rotation">2D Rotation</h4> <p>In two dimensions, rotation by an angle \(\theta\) counterclockwise is given by:</p> \[R(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\] <p>For any vector \(x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\), the rotated vector is:</p> \[x' = R(\theta) x\] <h4 id="3d-rotation-and-euler-angles">3D Rotation and Euler Angles</h4> <p>In 3D, rotations can be performed around each axis. The elementary rotations are:</p> <ul> <li> <p>Around \(x\)-axis: \(R_x(\theta) = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; \cos\theta &amp; -\sin\theta \\ 0 &amp; \sin\theta &amp; \cos\theta \end{bmatrix}\)</p> </li> <li> <p>Around \(y\)-axis: \(R_y(\theta) = \begin{bmatrix} \cos\theta &amp; 0 &amp; \sin\theta \\ 0 &amp; 1 &amp; 0 \\ -\sin\theta &amp; 0 &amp; \cos\theta \end{bmatrix}\)</p> </li> <li> <p>Around \(z\)-axis: \(R_z(\theta) = \begin{bmatrix} \cos\theta &amp; -\sin\theta &amp; 0 \\ \sin\theta &amp; \cos\theta &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</p> </li> </ul> <p>Any 3D rotation can be represented by a combination of these, often using <strong>Euler angles</strong>.</p> <hr> <h3 id="application-computer-vision-and-robotics">Application: Computer Vision and Robotics</h3> <ul> <li> <strong>Image Augmentation</strong>: Rotating images during training increases robustness to orientation.</li> <li> <strong>Pose Estimation</strong>: Estimating camera or robot orientation using rotation matrices.</li> <li> <strong>3D Reconstruction</strong>: Applying transformations to point clouds and mesh data.</li> <li> <strong>Robotics Control</strong>: Planning and executing movement using rotation matrices in kinematics.</li> </ul> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torchvision.transforms</span> <span class="k">as</span> <span class="n">T</span>
<span class="n">T</span><span class="p">.</span><span class="nc">RandomRotation</span><span class="p">(</span><span class="n">degrees</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</code></pre></div></div> <hr> <h3 id="reflection-matrices">Reflection Matrices</h3> <p>A <strong>reflection matrix</strong> flips a vector across a subspace (hyperplane). It is also orthogonal, but unlike a rotation matrix:</p> \[\det(R) = -1\] <h4 id="reflection-in-2d">Reflection in 2D</h4> <p>Reflection across the \(x\)-axis:</p> \[R = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}\] <p>Reflection across an arbitrary line through the origin with unit normal vector \(n\):</p> \[R = I - 2nn^T\] <p>Where \(n \in \mathbb{R}^d\) and \(\|n\| = 1\).</p> <hr> <h3 id="application-data-augmentation-in-cv">Application: Data Augmentation in CV</h3> <p>Reflections are used to simulate different perspectives of the same object.</p> <ul> <li> <strong>Horizontal Flip</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">T</span><span class="p">.</span><span class="nc">RandomHorizontalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Vertical Flip</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">T</span><span class="p">.</span><span class="nc">RandomVerticalFlip</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <strong>Symmetry-based Learning</strong>: Useful in object detection and scene understanding.</li> </ul> <hr> <h2 id="markov-matrices-and-stochastic-processes">Markov Matrices and Stochastic Processes</h2> <p>Markov models describe systems that evolve probabilistically over time, where the future depends only on the current state. Their underlying structure is captured by <strong>stochastic (Markov) matrices</strong>.</p> <hr> <h3 id="stochastic-matrices">Stochastic Matrices</h3> <p>A <strong>Markov matrix</strong> (or <strong>row-stochastic matrix</strong>) is a square matrix \(P \in \mathbb{R}^{n \times n}\) where:</p> <ol> <li>\(P_{ij} \geq 0\) for all \(i, j\),</li> <li>\(\sum_{j=1}^n P_{ij} = 1\) for all \(i\) (each row sums to 1).</li> </ol> <p>This means \(P_{ij}\) is the probability of transitioning from state \(i\) to state \(j\).</p> <p>Let \(x_t\) be a <strong>probability distribution vector</strong> at time \(t\). Then:</p> \[x_{t+1} = x_t P\] <p>This recurrence describes the <strong>evolution of a Markov chain</strong> over time.</p> <hr> <h3 id="stationary-distribution">Stationary Distribution</h3> <p>A distribution \(\pi\) is <strong>stationary</strong> if:</p> \[\pi = \pi P\] <p>This represents the long-term distribution of states. Under mild conditions (irreducibility, aperiodicity), every Markov chain has a unique stationary distribution.</p> <hr> <h3 id="application-pagerank-and-random-walks">Application: PageRank and Random Walks</h3> <p><strong>PageRank Algorithm</strong>:</p> <ul> <li>Models the web as a Markov chain.</li> <li>Pages are states, links are transitions.</li> <li>Uses a stochastic matrix with damping:</li> </ul> \[P' = \alpha P + (1 - \alpha) \frac{1}{n} \mathbf{1}\mathbf{1}^T\] <p>Stationary distribution \(\pi\) is computed such that:</p> \[\pi = \pi P'\] <p>Other Applications:</p> <ul> <li> <strong>Language Modeling</strong>: Character or word-level Markov chains.</li> <li> <strong>Hidden Markov Models (HMMs)</strong>: NLP, speech, time-series.</li> <li> <strong>Graph algorithms</strong>: Random walk-based node ranking and clustering.</li> </ul> <hr> <h2 id="summary-3">Summary</h2> <table> <thead> <tr> <th style="text-align: left">Concept</th> <th style="text-align: center">Mathematical Description</th> <th style="text-align: right">Applications</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Rotation Matrix</td> <td style="text-align: center">\(R^T R = I, \ \det(R) = 1\)</td> <td style="text-align: right">CV, robotics, 3D vision, pose estimation</td> </tr> <tr> <td style="text-align: left">Reflection Matrix</td> <td style="text-align: center">\(R = I - 2nn^T, \ \det(R) = -1\)</td> <td style="text-align: right">Data augmentation, symmetry modeling</td> </tr> <tr> <td style="text-align: left">Euler Angles</td> <td style="text-align: center">Composition of axis-wise rotations</td> <td style="text-align: right">Robotics, camera modeling</td> </tr> <tr> <td style="text-align: left">Stochastic Matrix</td> <td style="text-align: center">\(P_{ij} \geq 0, \ \sum_j P_{ij} = 1\)</td> <td style="text-align: right">Markov chains, PageRank, probabilistic models</td> </tr> <tr> <td style="text-align: left">Stationary Distribution</td> <td style="text-align: center">\(\pi = \pi P\)</td> <td style="text-align: right">Long-term behavior modeling</td> </tr> </tbody> </table> <hr> <p>Rotation and reflection matrices form the mathematical backbone of <strong>spatial transformations</strong> in ML applications like vision and robotics. Meanwhile, Markov matrices provide a <strong>probabilistic framework</strong> to model temporal evolution in tasks such as <strong>language modeling</strong>, <strong>search ranking</strong>, and <strong>sequence prediction</strong>.</p> <hr> <h1 id="advanced-projections-random-projections-and-the-johnsonlindenstrauss-lemma">Advanced Projections: Random Projections and the Johnson–Lindenstrauss Lemma</h1> <p>In modern machine learning and data science, high-dimensional datasets are common—particularly in fields like <strong>natural language processing</strong>, <strong>image processing</strong>, and <strong>information retrieval</strong>. However, high dimensionality brings computational and storage challenges, as well as the <strong>curse of dimensionality</strong>.</p> <p>One elegant solution to this is <strong>random projection</strong>, a method for dimensionality reduction that is fast, scalable, and surprisingly effective. Its theoretical foundation is the <strong>Johnson–Lindenstrauss Lemma</strong>, which guarantees that random projections approximately preserve distances between points.</p> <p>This section explains:</p> <ul> <li>The theory and math behind random projections,</li> <li>The Johnson–Lindenstrauss Lemma and its guarantees,</li> <li>Applications in NLP, IR, privacy, and beyond.</li> </ul> <hr> <h2 id="1-the-need-for-dimensionality-reduction">1. The Need for Dimensionality Reduction</h2> <p>Let \(X \in \mathbb{R}^{n \times d}\) be a dataset with \(n\) samples in a high-dimensional space \(\mathbb{R}^d\).</p> <p>Problems with large \(d\):</p> <ul> <li> <strong>Computational inefficiency</strong>: Matrix operations are expensive.</li> <li> <strong>Memory consumption</strong>: Storing all features is costly.</li> <li> <strong>Overfitting</strong>: Too many features relative to data points.</li> <li> <strong>Distance concentration</strong>: In high dimensions, pairwise distances become less informative.</li> </ul> <p>Goal: Reduce the dimensionality from \(d\) to \(k \ll d\) such that <strong>geometric structure (e.g., pairwise distances)</strong> is preserved.</p> <hr> <h2 id="2-random-projections">2. Random Projections</h2> <p>Instead of learning an optimal projection (like PCA), <strong>random projections</strong> use a <strong>random linear map</strong>:</p> <p>Let \(R \in \mathbb{R}^{k \times d}\) be a random matrix. Then:</p> \[z_i = \frac{1}{\sqrt{k}} R x_i \in \mathbb{R}^k\] <p>for each data point \(x_i \in \mathbb{R}^d\).</p> <p>The random matrix \(R\) typically has entries sampled from:</p> <ul> <li>Standard Gaussian: \(R_{ij} \sim \mathcal{N}(0, 1)\),</li> <li>Sparse sign matrices: \(R_{ij} \in \{-1, 0, +1\}\) with controlled sparsity.</li> </ul> <hr> <h2 id="3-johnsonlindenstrauss-lemma">3. Johnson–Lindenstrauss Lemma</h2> <p>The <strong>Johnson–Lindenstrauss Lemma</strong> states that a small set of points in high-dimensional space can be mapped into a lower-dimensional space such that pairwise distances are approximately preserved.</p> <h3 id="theorem-jl-lemma">Theorem (JL Lemma)</h3> <p>For any \(0 &lt; \epsilon &lt; 1\) and integer \(n\), let \(X = \{x_1, \ldots, x_n\} \subset \mathbb{R}^d\) be a set of \(n\) points. Then for:</p> \[k = O\left(\frac{\log n}{\epsilon^2}\right),\] <p>there exists a linear map \(f: \mathbb{R}^d \rightarrow \mathbb{R}^k\) such that for all \(i, j\):</p> \[(1 - \epsilon)\|x_i - x_j\|_2^2 \leq \|f(x_i) - f(x_j)\|_2^2 \leq (1 + \epsilon)\|x_i - x_j\|_2^2\] <p>This means that <strong>random projections preserve pairwise distances up to small distortion</strong> with high probability.</p> <hr> <h3 id="intuition-1">Intuition</h3> <ul> <li>The JL Lemma shows that <strong>no information-theoretic bottleneck</strong> exists when compressing data from \(\mathbb{R}^d\) to \(\mathbb{R}^k\), as long as \(k = O(\log n)\).</li> <li>This is <strong>data-independent</strong>: No need to look at the data when designing the projection.</li> </ul> <hr> <h2 id="4-construction-of-projection-matrix">4. Construction of Projection Matrix</h2> <p>Let \(R \in \mathbb{R}^{k \times d}\) be the random matrix. Some common constructions:</p> <h3 id="41-gaussian-random-projection">4.1 Gaussian Random Projection</h3> <p>Each entry is drawn i.i.d. from:</p> \[R_{ij} \sim \mathcal{N}(0, 1)\] <p>Then the projection is:</p> \[f(x) = \frac{1}{\sqrt{k}} R x\] <p>This satisfies the JL lemma with high probability.</p> <hr> <h3 id="42-sparse-random-projection">4.2 Sparse Random Projection</h3> <p>To improve speed and memory:</p> \[R_{ij} = \sqrt{s} \cdot \begin{cases} +1 &amp; \text{with probability } \frac{1}{2s}, \\ 0 &amp; \text{with probability } 1 - \frac{1}{s}, \\ -1 &amp; \text{with probability } \frac{1}{2s} \end{cases}\] <p>For example, \(s = 3\) gives ~67% sparsity.</p> <hr> <h2 id="5-applications">5. Applications</h2> <h3 id="51-natural-language-processing-nlp">5.1 Natural Language Processing (NLP)</h3> <ul> <li> <strong>TF-IDF vectors</strong> for documents can have tens of thousands of dimensions.</li> <li>Random projections reduce dimensionality for: <ul> <li> <strong>Document classification</strong>,</li> <li> <strong>Similarity search</strong>,</li> <li> <strong>Topic modeling</strong> pre-processing.</li> </ul> </li> </ul> <p><strong>Example</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.random_projection</span> <span class="kn">import</span> <span class="n">GaussianRandomProjection</span>
<span class="n">transformer</span> <span class="o">=</span> <span class="nc">GaussianRandomProjection</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">X_new</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_tfidf</span><span class="p">)</span>
</code></pre></div></div> <hr> <h3 id="52-information-retrieval-and-ann-search">5.2 Information Retrieval and ANN Search</h3> <ul> <li>Used to <strong>index high-dimensional vectors</strong> for approximate nearest neighbors (ANN).</li> <li>Efficiently reduce the dimension of image embeddings, word embeddings, etc.</li> <li>Compatible with LSH (Locality-Sensitive Hashing).</li> </ul> <hr> <h3 id="53-differential-privacy-and-data-privacy">5.3 Differential Privacy and Data Privacy</h3> <ul> <li>Random projections are used to <strong>obscure sensitive dimensions</strong> while preserving utility.</li> <li>Also appear in <strong>private matrix factorization</strong> and federated learning pipelines.</li> </ul> <hr> <h3 id="54-kernel-approximation">5.4 Kernel Approximation</h3> <ul> <li> <strong>Random Fourier Features</strong> approximate Gaussian/RBF kernels by projecting into a low-dimensional space.</li> <li>Scales kernel methods to large datasets: \(k(x, y) \approx \phi(x)^T \phi(y)\)</li> </ul> <p>Where \(\phi(x)\) is obtained via random projections.</p> <hr> <h2 id="6-comparison-with-pca">6. Comparison with PCA</h2> <table> <thead> <tr> <th style="text-align: left">Aspect</th> <th style="text-align: center">PCA</th> <th style="text-align: right">Random Projection</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Data-dependent</td> <td style="text-align: center">Yes</td> <td style="text-align: right">No</td> </tr> <tr> <td style="text-align: left">Computational cost</td> <td style="text-align: center">High (SVD-based)</td> <td style="text-align: right">Low (matrix multiplication)</td> </tr> <tr> <td style="text-align: left">Distance preservation</td> <td style="text-align: center">Optimal for top-variance directions</td> <td style="text-align: right">Approximate, but probabilistically guaranteed</td> </tr> <tr> <td style="text-align: left">Scalability</td> <td style="text-align: center">Less scalable for large \(d\)</td> <td style="text-align: right">Highly scalable</td> </tr> <tr> <td style="text-align: left">Interpretability</td> <td style="text-align: center">High (axes = principal components)</td> <td style="text-align: right">Low</td> </tr> </tbody> </table> <hr> <h2 id="summary-4">Summary</h2> <table> <thead> <tr> <th style="text-align: left">Concept</th> <th style="text-align: center">Mathematical Idea</th> <th style="text-align: right">Application Domains</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Johnson–Lindenstrauss Lemma</td> <td style="text-align: center">\(k = O\left(\frac{\log n}{\epsilon^2}\right)\)</td> <td style="text-align: right">Distance-preserving low-dim embedding</td> </tr> <tr> <td style="text-align: left">Gaussian Projection</td> <td style="text-align: center">\(R_{ij} \sim \mathcal{N}(0, 1)\)</td> <td style="text-align: right">NLP, embeddings, privacy</td> </tr> <tr> <td style="text-align: left">Sparse Projection</td> <td style="text-align: center">\(R_{ij} \in \{-1, 0, +1\}\) with sparsity</td> <td style="text-align: right">Faster computation</td> </tr> <tr> <td style="text-align: left">Random Fourier Features</td> <td style="text-align: center">Approx. kernel projection via random bases</td> <td style="text-align: right">Kernel methods on large datasets</td> </tr> <tr> <td style="text-align: left">Distance Preservation</td> <td style="text-align: center">\(|x_i - x_j| \approx |f(x_i) - f(x_j)|\)</td> <td style="text-align: right">ANN, clustering, manifold learning</td> </tr> </tbody> </table> <hr> <p><strong>Random projections</strong> offer a principled, efficient, and theoretically sound method to <strong>compress high-dimensional data</strong> while preserving its <strong>geometric structure</strong>. Thanks to the <strong>Johnson–Lindenstrauss Lemma</strong>, we can apply these projections without worrying about distortion—making them perfect for large-scale ML systems.</p> <p>Whether you’re dealing with:</p> <ul> <li>Large vocabulary document matrices in NLP,</li> <li>Embedding vectors in image retrieval,</li> <li>Kernel methods in high dimensions,</li> </ul> <p>random projections are a tool you should definitely have in your toolbox.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> <script src="/assets/js/lunr.min.js"></script> <script src="/assets/js/search.js"></script> </body> </html>