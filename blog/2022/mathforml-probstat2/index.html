<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Probability &amp; Statistics for Data Science - Probability Distributions | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Probability &amp; Statistics 2 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-probstat2/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">GATE CS / JEE Notes</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="nav-item"> <a id="search-button" class="nav-link" href="javascript:void(0)"> <i class="ti ti-search"></i> <span class="sr-only">Search</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> <div id="search-modal" class="search-modal"> <div class="search-container"> <div class="search-header"> <div class="search-input-container"> <i class="ti ti-search search-icon"></i> <input type="text" id="search-input" class="search-input" placeholder="Search..." autocomplete="off"> <button id="search-close" class="search-close-button"> <i class="ti ti-x"></i> </button> </div> <div class="search-shortcuts"> <span class="search-shortcut"> <kbd>/</kbd> or <kbd>Ctrl</kbd>+<kbd>K</kbd> to search </span> <span class="search-shortcut"> <kbd>↑</kbd><kbd>↓</kbd> to navigate </span> <span class="search-shortcut"> <kbd>Enter</kbd> to select </span> <span class="search-shortcut"> <kbd>Esc</kbd> to close </span> </div> </div> <div id="search-results" class="search-results"> <div class="search-results-container"></div> </div> </div> </div> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Probability &amp; Statistics for Data Science - Probability Distributions</h1> <p class="post-meta"> June 05, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/probability-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> probability-statistics</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#discrete-distributions">Discrete Distributions</a> <ul> <li class="toc-entry toc-h3"><a href="#1-bernoulli-distribution">1. Bernoulli Distribution</a></li> <li class="toc-entry toc-h3"><a href="#2-binomial-distribution">2. Binomial Distribution</a></li> <li class="toc-entry toc-h3"><a href="#3-poisson-distribution">3. Poisson Distribution</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#continuous-distributions">Continuous Distributions</a> <ul> <li class="toc-entry toc-h3"><a href="#1-uniform-distribution">1. Uniform Distribution</a></li> <li class="toc-entry toc-h3"><a href="#2-normal-gaussian-distribution">2. Normal (Gaussian) Distribution</a></li> <li class="toc-entry toc-h3"><a href="#3-exponential-distribution">3. Exponential Distribution</a></li> <li class="toc-entry toc-h3"><a href="#4-beta-distribution">4. Beta Distribution</a></li> <li class="toc-entry toc-h3"><a href="#5-gamma-distribution">5. Gamma Distribution</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#multivariate-distributions">Multivariate Distributions</a> <ul> <li class="toc-entry toc-h3"><a href="#joint-marginal-and-conditional-distributions">Joint, Marginal, and Conditional Distributions</a></li> <li class="toc-entry toc-h3"><a href="#multivariate-normal-distribution">Multivariate Normal Distribution</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#simulating-datasets-in-practice">Simulating Datasets in Practice</a> <ul> <li class="toc-entry toc-h3"><a href="#example-binary-classification-dataset">Example: Binary Classification Dataset</a></li> <li class="toc-entry toc-h3"><a href="#example-two-class-gaussian-clusters">Example: Two-Class Gaussian Clusters</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#summary-table-choosing-the-right-distribution">Summary Table: Choosing the Right Distribution</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>Probability distributions are the foundation of machine learning. They shape how we simulate data, quantify uncertainty, and reason about model behavior. Whether you are generating synthetic samples, fitting probabilistic models, or understanding errors, distributions provide the structure for everything that follows.</p> <p>In this article, we will explore:</p> <ul> <li>Important <strong>discrete and continuous distributions</strong> </li> <li>Concepts around <strong>multivariate distributions</strong> </li> <li>Key <strong>applications in machine learning and data science</strong> </li> <li>Demonstrations using <strong>Python and Plotly visualizations</strong> </li> </ul> <hr> <h2 id="discrete-distributions">Discrete Distributions</h2> <p>Discrete probability distributions represent outcomes that are countable — such as binary labels, event counts, or integer results of repeated trials. These are particularly relevant in classification tasks, anomaly detection, and simulations of real-world processes.</p> <hr> <h3 id="1-bernoulli-distribution">1. Bernoulli Distribution</h3> <p>The Bernoulli distribution is used to model a single trial with two possible outcomes: success (1) or failure (0). It is the building block of binary classification problems.</p> <p><strong>Probability Mass Function (PMF)</strong>: \(P(X = x) = p^x (1 - p)^{1 - x}, \quad x \in \{0,1\}\)</p> <ul> <li> <strong>Mean</strong>: \(\mathbb{E}[X] = p\)</li> <li> <strong>Variance</strong>: \(\text{Var}(X) = p(1 - p)\)</li> </ul> <p><strong>Applications</strong>:</p> <ul> <li>Binary classification (e.g., logistic regression target)</li> <li>Simulating binary labels for synthetic datasets</li> </ul> <blockquote> <p>The plot below illustrates a Bernoulli distribution where the probability of success (1) is 0.7 and failure (0) is 0.3. This type of distribution is ideal for binary classification tasks where outcomes are either “yes” or “no”, such as predicting if a customer will churn. The interactive bar chart helps visualize how the probability is distributed between the two outcomes.</p> </blockquote> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/probstat2_1_bernoulli.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <hr> <h3 id="2-binomial-distribution">2. Binomial Distribution</h3> <p>The Binomial distribution generalizes the Bernoulli trial to \(n\) repeated trials. It models the number of successes in a fixed number of independent experiments.</p> <p><strong>PMF</strong>: \(P(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}\)</p> <ul> <li> <strong>Mean</strong>: \(\mathbb{E}[X] = np\)</li> <li> <strong>Variance</strong>: \(\text{Var}(X) = np(1 - p)\)</li> </ul> <p><strong>Applications</strong>:</p> <ul> <li>Ensemble model success probability</li> <li>Simulating repeated trials</li> </ul> <blockquote> <p>This interactive chart represents the probability of getting a certain number of successes in 10 independent trials, each with a 50% success rate. The binomial distribution models many real-world situations in machine learning, such as predicting how many models in an ensemble will make correct predictions. Notice how the distribution is symmetric when \(p = 0.5\) and peaks around \(n \cdot p = 5\).</p> </blockquote> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/probstat2_2_binomial.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <hr> <h3 id="3-poisson-distribution">3. Poisson Distribution</h3> <p>The Poisson distribution models the number of events that occur in a fixed interval of time or space, assuming the events occur independently and at a constant rate \(\lambda\).</p> <p><strong>PMF</strong>: \(P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\)</p> <ul> <li> <strong>Mean</strong>: \(\mathbb{E}[X] = \lambda\)</li> <li> <strong>Variance</strong>: \(\text{Var}(X) = \lambda\)</li> </ul> <p><strong>Applications</strong>:</p> <ul> <li>Anomaly detection (e.g., fraud or system failures)</li> <li>Modeling event frequency (web traffic, queueing)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">poisson</span>

<span class="n">lmbda</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">pmf</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">pmf</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Poisson Distribution (λ=</span><span class="si">{</span><span class="n">lmbda</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">k (events)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Probability</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>The Poisson distribution is commonly used for modeling the number of times an event occurs in a fixed interval. In the plot below, we use \(\lambda = 4\) to show the probability distribution of event counts. This distribution is especially useful in anomaly detection — for example, identifying if the number of failed login attempts is abnormally high.</p> </blockquote> <div id="poissonPlot" style="width:100%;max-width:700px; margin: 2rem auto;"></div> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <script>document.addEventListener("DOMContentLoaded",function(){const t=Array.from({length:15},(t,o)=>o),o=4,e=t=>t?t*e(t-1):1,i=t.map(t=>Math.pow(o,t)*Math.exp(-o)/e(t));Plotly.newPlot("poissonPlot",[{x:t,y:i,type:"bar",marker:{color:"slateblue"}}],{title:"Poisson Distribution (\u03bb = 4)",xaxis:{title:"k (Events)"},yaxis:{title:"Probability"}})});</script> <hr> <h2 id="continuous-distributions">Continuous Distributions</h2> <p>Continuous distributions represent variables that can take on any value within a range. They are essential in feature modeling, regression analysis, generative modeling, and more.</p> <hr> <h3 id="1-uniform-distribution">1. Uniform Distribution</h3> <p>This distribution assigns equal probability to all values in an interval $[a, b]$.</p> <p><strong>PDF</strong>: \(f(x) = \frac{1}{b - a}, \quad a \leq x \leq b\)</p> <p><strong>Applications</strong>:</p> <ul> <li>Random initialization (e.g., neural network weights)</li> <li>Data simulation and control baselines</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Uniform Distribution [0,1]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr> <h3 id="2-normal-gaussian-distribution">2. Normal (Gaussian) Distribution</h3> <p>The Normal distribution is the most widely used distribution in statistics and machine learning, due to the Central Limit Theorem.</p> <p><strong>PDF</strong>: \(f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{- \frac{(x - \mu)^2}{2\sigma^2}}\)</p> <ul> <li> <strong>Mean</strong>: \(\mu\)</li> <li> <strong>Variance</strong>: \(\sigma^2\)</li> </ul> <p><strong>Applications</strong>:</p> <ul> <li>Linear regression assumptions</li> <li>PCA and feature decorrelation</li> <li>Modeling errors and noise</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Normal Distribution (μ=0, σ=1)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>The bell-shaped curve shown below represents the standard normal distribution. This is foundational to many algorithms in statistics and machine learning. It models noise, errors, and is used in methods like Principal Component Analysis (PCA). Thanks to the Central Limit Theorem, many sample-based statistics tend to follow this distribution even when the original data is not normal.</p> </blockquote> <div style="display: flex; justify-content: center;"> <div id="normalPlot" style="width:100%;max-width:700px;"></div> </div> <script>const normalX=Array.from({length:100},(t,a)=>(a-50)/10),mu=0,sigma=1,normalY=normalX.map(t=>1/(Math.sqrt(2*Math.PI)*sigma)*Math.exp(-.5*Math.pow((t-mu)/sigma,2)));Plotly.newPlot("normalPlot",[{x:normalX,y:normalY,type:"scatter",mode:"lines",line:{color:"seagreen"}}],{title:"Normal Distribution (\u03bc = 0, \u03c3 = 1)",xaxis:{title:"x"},yaxis:{title:"Density"}});</script> <hr> <h3 id="3-exponential-distribution">3. Exponential Distribution</h3> <p>The exponential distribution describes the time between events in a Poisson process.</p> <p><strong>PDF</strong>: \(f(x) = \lambda e^{-\lambda x}, \quad x \geq 0\)</p> <p><strong>Applications</strong>:</p> <ul> <li>Survival and reliability analysis</li> <li>Modeling waiting times</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lmbda</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">lmbda</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Exponential Distribution</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>This visualization shows the exponential distribution, which is often used to model the time between events in a Poisson process. It’s especially useful in survival analysis and reliability engineering. The curve declines rapidly, showing that events are most likely to happen shortly after the last one, and become less likely over time.</p> </blockquote> <div style="display: flex; justify-content: center;"> <div id="exponentialPlot" style="width:100%;max-width:700px;"></div> </div> <script>const expX=Array.from({length:100},(t,e)=>.1*e),lambdaExp=1,expY=expX.map(t=>lambdaExp*Math.exp(-lambdaExp*t));Plotly.newPlot("exponentialPlot",[{x:expX,y:expY,type:"scatter",mode:"lines",line:{color:"tomato"}}],{title:"Exponential Distribution (\u03bb = 1)",xaxis:{title:"x"},yaxis:{title:"Density"}});</script> <hr> <h3 id="4-beta-distribution">4. Beta Distribution</h3> <p>A flexible distribution on the interval [0,1], defined by two shape parameters \(\alpha\) and \(\beta\).</p> <p><strong>PDF</strong>: \(f(x; \alpha, \beta) = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha, \beta)}\)</p> <p><strong>Applications</strong>:</p> <ul> <li>Bayesian modeling of probabilities</li> <li>Thompson sampling in reinforcement learning</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">α=</span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s">, β=</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Beta Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <blockquote> <p>The Beta distribution is defined over the interval [0,1] and is commonly used to model probabilities themselves. In the visualization below, we use shape parameters \(\alpha = 2\) and \(\beta = 5\). This creates a distribution skewed toward 0, reflecting a belief that lower probability values are more likely. Beta distributions are essential in Bayesian statistics and exploration-exploitation algorithms like Thompson Sampling.</p> </blockquote> <div style="display: flex; justify-content: center;"> <div id="betaPlot" style="width:100%;max-width:700px;"></div> </div> <script>function gamma(t){return 1===t?1:(t-1)*gamma(t-1)}function betaPDF(t,a,e){const o=(t,a)=>gamma(t)*gamma(a)/gamma(t+a);return Math.pow(t,a-1)*Math.pow(1-t,e-1)/o(a,e)}const betaX=Array.from({length:100},(t,a)=>a/100),betaY=betaX.map(t=>betaPDF(t,2,5));Plotly.newPlot("betaPlot",[{x:betaX,y:betaY,type:"scatter",mode:"lines",line:{color:"steelblue"}}],{title:"Beta Distribution (\u03b1 = 2, \u03b2 = 5)",xaxis:{title:"x (0 to 1)"},yaxis:{title:"Density"}});</script> <hr> <h3 id="5-gamma-distribution">5. Gamma Distribution</h3> <p>A two-parameter generalization of the exponential distribution.</p> <p><strong>PDF</strong>: \(f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}\)</p> <p><strong>Applications</strong>:</p> <ul> <li>Waiting time models</li> <li>Priors in Bayesian inference</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">gamma</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">shape</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shape=</span><span class="si">{</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Gamma Distributions</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr> <h2 id="multivariate-distributions">Multivariate Distributions</h2> <p>Multivariate distributions model <strong>joint behavior of multiple variables</strong>, especially when they are correlated.</p> <hr> <h3 id="joint-marginal-and-conditional-distributions">Joint, Marginal, and Conditional Distributions</h3> <ul> <li> <strong>Joint distribution</strong>: \(P(X, Y)\) gives the probability of two variables occurring together.</li> <li> <strong>Marginal distribution</strong>: \(P(X) = \sum_Y P(X, Y)\), projecting one variable.</li> <li> <strong>Conditional distribution</strong>: \(P(Y \mid X) = \frac{P(X, Y)}{P(X)}\)</li> </ul> <p>These are critical for reasoning about dependencies, causality, and generative models.</p> <hr> <h3 id="multivariate-normal-distribution">Multivariate Normal Distribution</h3> <p>Defined by a mean vector \(\mu\) and a covariance matrix \(\Sigma\).</p> \[X \sim \mathcal{N}(\mu, \Sigma)\] <p><strong>Applications</strong>:</p> <ul> <li>PCA (eigen decomposition of \(\Sigma\))</li> <li>Gaussian Mixture Models (GMMs)</li> <li>Modeling correlated features</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">mean</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">cov</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Samples from Bivariate Normal Distribution</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X1</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">X2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <hr> <h2 id="simulating-datasets-in-practice">Simulating Datasets in Practice</h2> <p>Simulating data allows us to prototype models, test ideas, and understand algorithms.</p> <h3 id="example-binary-classification-dataset">Example: Binary Classification Dataset</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="sh">"</span><span class="s">Feature_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
<span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">Target</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span>
</code></pre></div></div> <h3 id="example-two-class-gaussian-clusters">Example: Two-Class Gaussian Clusters</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">class1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">500</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">500</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">class0</span><span class="p">,</span> <span class="n">class1</span><span class="p">])</span>
</code></pre></div></div> <hr> <h2 id="summary-table-choosing-the-right-distribution">Summary Table: Choosing the Right Distribution</h2> <table> <thead> <tr> <th><strong>Distribution</strong></th> <th><strong>Common Application</strong></th> </tr> </thead> <tbody> <tr> <td>Bernoulli</td> <td>Binary classification</td> </tr> <tr> <td>Binomial</td> <td>Ensemble voting, success trials</td> </tr> <tr> <td>Poisson</td> <td>Anomaly detection, event counts</td> </tr> <tr> <td>Normal</td> <td>Error modeling, PCA, regression</td> </tr> <tr> <td>Exponential</td> <td>Time-to-event, survival modeling</td> </tr> <tr> <td>Beta</td> <td>Probability modeling, Bayesian inference</td> </tr> <tr> <td>Gamma</td> <td>Duration modeling, Bayesian priors</td> </tr> <tr> <td>Multivariate Normal</td> <td>Feature correlation, PCA, GMM</td> </tr> </tbody> </table> <hr> <p>Probability distributions are at the core of every model you build in machine learning. They guide how you generate, structure, and analyze data. Whether you are simulating features, modeling uncertainty, or decomposing variance in PCA, the right distribution makes all the difference.</p> <p>In the next post, we’ll move into the world of <strong>Bayesian inference</strong> and learn how concepts like <strong>MLE, MAP</strong>, and <strong>priors</strong> shape our understanding of parameter estimation.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> <script src="/assets/js/lunr.min.js"></script> <script src="/assets/js/search.js"></script> </body> </html>