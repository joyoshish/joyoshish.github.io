<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Probability &amp; Statistics for Data Science - Foundations of Probability | Joyoshish Saha </title> <meta name="author" content="Joyoshish Saha"> <meta name="description" content="Probability &amp; Statistics 1 - Mathematics for Machine Learning"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E0%A6%9C&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://joyoshish.github.io/blog/2022/mathforml-probstat1/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%6A%6F%79%6F%73%68%69%73%68@%70%72%6F%74%6F%6E%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://www.linkedin.com/in/joyoshishsaha" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://facebook.com/joyoshish" title="Facebook" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-facebook"></i></a> </div> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Joyoshish</span> Saha </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">Résumé </a> </li> <li class="nav-item "> <a class="nav-link" href="/blogging/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/reserach/">Research </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/acadresrc/">Computer Science and Data Science</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/rndmresrc/">Random</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">Contact </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Probability &amp; Statistics for Data Science - Foundations of Probability</h1> <p class="post-meta"> June 01, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/probability-statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> probability-statistics</a>   <a href="/blog/tag/math"> <i class="fa-solid fa-hashtag fa-sm"></i> math</a>     ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/math-for-ml"> <i class="fa-solid fa-tag fa-sm"></i> math-for-ml</a>   </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"> <a href="#sample-space-events-and-conditional-probability">Sample Space, Events, and Conditional Probability</a> <ul> <li class="toc-entry toc-h3"><a href="#independence">Independence</a></li> <li class="toc-entry toc-h3"><a href="#conditional-probability">Conditional Probability</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#random-variables-discrete-and-continuous">Random Variables (Discrete and Continuous)</a> <ul> <li class="toc-entry toc-h3"><a href="#discrete-random-variables">Discrete Random Variables</a></li> <li class="toc-entry toc-h3"><a href="#continuous-random-variables">Continuous Random Variables</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#expectation-variance-and-standard-deviation">Expectation, Variance, and Standard Deviation</a> <ul> <li class="toc-entry toc-h3"><a href="#expectation">Expectation</a></li> <li class="toc-entry toc-h3"><a href="#variance">Variance</a></li> <li class="toc-entry toc-h3"><a href="#standard-deviation">Standard Deviation</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#law-of-large-numbers-lln">Law of Large Numbers (LLN)</a> <ul> <li class="toc-entry toc-h3"><a href="#ml-insight">ML Insight:</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#central-limit-theorem-clt">Central Limit Theorem (CLT)</a></li> <li class="toc-entry toc-h2"> <a href="#applications-in-machine-learning">Applications in Machine Learning</a> <ul> <li class="toc-entry toc-h3"><a href="#probabilistic-modeling">Probabilistic Modeling</a></li> <li class="toc-entry toc-h3"><a href="#uncertainty-quantification">Uncertainty Quantification</a></li> <li class="toc-entry toc-h3"><a href="#why-empirical-means-work-ensemble-models">Why Empirical Means Work (Ensemble Models)</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#wrapping-up">Wrapping Up</a></li> </ul> </div> <hr> <div id="markdown-content"> <p>When we build machine learning models, we’re not just dealing with data — we’re dealing with <strong>uncertainty</strong>. Whether we’re classifying emails, predicting stock prices, or detecting fraud, our models are built on probabilistic foundations that allow them to reason under uncertainty.</p> <p>This post covers the <strong>core probability theory concepts</strong> that underpin machine learning, deep learning, and AI. We’ll explore how concepts like <strong>sample space, events, random variables, expectation, variance, the Law of Large Numbers</strong>, and the <strong>Central Limit Theorem</strong> connect directly to real-world ML applications.</p> <hr> <h2 id="sample-space-events-and-conditional-probability">Sample Space, Events, and Conditional Probability</h2> <p>To understand how probability plays a role in ML, we must first start with the basics.</p> <ul> <li> <strong>Sample Space ( \(\Omega\) )</strong>: The set of all possible outcomes of an experiment.</li> <li> <strong>Event</strong>: A subset of the sample space. It’s a collection of outcomes we’re interested in.</li> <li> <strong>Probability Function</strong>: Assigns a number between 0 and 1 to each event, satisfying the axioms of probability.</li> </ul> <p><strong>Example</strong>: Suppose we build a binary classifier to detect spam.</p> <ul> <li> \[\Omega = \{\text{spam}, \text{not spam}\}\] </li> <li>\(P(\text{spam}) = 0.4\), \(P(\text{not spam}) = 0.6\)</li> </ul> <h3 id="independence">Independence</h3> <p>Two events \(A\) and \(B\) are <strong>independent</strong> if the occurrence of one does not affect the probability of the other:</p> \[P(A \cap B) = P(A) \cdot P(B)\] <p>In ML, this concept appears in <strong>Naive Bayes classifiers</strong>, where we assume that features are conditionally independent given the class label — a simplification that works surprisingly well in practice.</p> <h3 id="conditional-probability">Conditional Probability</h3> <p>Conditional probability tells us the probability of an event \(A\) given that event \(B\) has occurred:</p> \[P(A \mid B) = \frac{P(A \cap B)}{P(B)}\] <p>This is the foundation of <strong>Bayes’ Theorem</strong>, which is used to update predictions as new information becomes available — exactly what happens when your email spam filter learns from new messages.</p> <hr> <h2 id="random-variables-discrete-and-continuous">Random Variables (Discrete and Continuous)</h2> <p>A <strong>random variable</strong> maps outcomes from the sample space to numerical values.</p> <h3 id="discrete-random-variables">Discrete Random Variables</h3> <p>These take <strong>countable</strong> values (like integers). Examples include:</p> <ul> <li>Number of clicks on an ad</li> <li>Whether a transaction is fraudulent (0 or 1)</li> </ul> <p>A discrete random variable \(X\) has a <strong>probability mass function (PMF)</strong>:</p> \[P(X = x) = p(x)\] <h3 id="continuous-random-variables">Continuous Random Variables</h3> <p>These take <strong>uncountably infinite</strong> values (e.g., any real number). Examples:</p> <ul> <li>The exact temperature in a room</li> <li>Probability of customer spending</li> </ul> <p>A continuous random variable has a <strong>probability density function (PDF)</strong> such that:</p> \[P(a \leq X \leq b) = \int_a^b f(x) \, dx\] <p>In ML, we use random variables to model data distributions. For example, we assume weights in Bayesian models come from a Gaussian prior — a continuous random variable.</p> <hr> <h2 id="expectation-variance-and-standard-deviation">Expectation, Variance, and Standard Deviation</h2> <p>These are the building blocks of <strong>descriptive statistics</strong> and are critical to understanding model behavior.</p> <h3 id="expectation">Expectation</h3> <p>Also called the <strong>mean</strong> or expected value:</p> <ul> <li> <p>For discrete \(X\): \(\mathbb{E}[X] = \sum_x x \cdot P(X = x)\)</p> </li> <li> <p>For continuous \(X\): \(\mathbb{E}[X] = \int_{-\infty}^{\infty} x \cdot f(x) \, dx\)</p> </li> </ul> <p>Think of it as the <strong>center of mass</strong> of the distribution.</p> <h3 id="variance">Variance</h3> <p>Measures the <strong>spread</strong> of the random variable around the mean:</p> \[\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]\] <p>This gives us insight into how uncertain a prediction might be. A model with low variance makes <strong>consistent predictions</strong>.</p> <h3 id="standard-deviation">Standard Deviation</h3> <p>The square root of variance:</p> \[\sigma = \sqrt{\text{Var}(X)}\] <p>Used commonly in ML to <strong>normalize features</strong> and analyze error distributions.</p> <hr> <h2 id="law-of-large-numbers-lln">Law of Large Numbers (LLN)</h2> <p>The Law of Large Numbers states that as the sample size \(n\) increases, the <strong>sample mean</strong> of random variables converges to the <strong>true mean</strong>:</p> \[\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n X_i = \mathbb{E}[X]\] <p>This justifies why <strong>averaging predictions across many models (ensembles)</strong> improves performance — individual errors average out.</p> <p>Let’s simulate this idea by drawing samples from a uniform distribution \(\mathcal{U}(0, 1)\) and watching how the sample mean stabilizes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Reproducibility
</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Generate 10,000 samples from a Uniform[0,1] distribution
</span><span class="n">samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">)</span>

<span class="c1"># Compute sample means as we increase sample size
</span><span class="n">sample_means</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">[:</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>

<span class="c1"># Plotting
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">sample_means</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">Sample Mean</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">'</span><span class="s">red</span><span class="sh">'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="sh">'</span><span class="s">--</span><span class="sh">'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sh">'</span><span class="s">True Mean = 0.5</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Samples</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample Mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Law of Large Numbers (LLN)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="display: flex; justify-content: center;"> <div id="llnPlot" style="width:100%;max-width:700px;"></div> </div> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <script>const xVals=Array.from({length:1e3},(e,a)=>a+1),yVals=xVals.map(e=>{let a=0;for(let l=0;l<e;l++)a+=Math.random();return a/e}),trace1={x:xVals,y:yVals,mode:"lines",name:"Sample Mean"},trace2={x:xVals,y:Array(xVals.length).fill(.5),mode:"lines",name:"True Mean = 0.5",line:{dash:"dot",color:"red"}};Plotly.newPlot("llnPlot",[trace1,trace2],{title:"Law of Large Numbers",xaxis:{title:"Number of Samples"},yaxis:{title:"Sample Mean"}});</script> <p>As shown in the plot, the sample mean starts off noisy but <strong>converges toward 0.5</strong>, which is the true expected value of a uniform distribution over [0, 1].</p> <h3 id="ml-insight">ML Insight:</h3> <p>In <strong>bagging methods</strong> (e.g., Random Forests), we train many models on bootstrapped samples and average their predictions. The LLN guarantees that as the number of trees increases, the aggregate prediction becomes more stable and closer to the true signal.</p> <hr> <h2 id="central-limit-theorem-clt">Central Limit Theorem (CLT)</h2> <p>The CLT is one of the most powerful ideas in all of statistics.</p> <blockquote> <p><strong>Theorem</strong>: The sum (or average) of a large number of <strong>independent, identically distributed (i.i.d.)</strong> random variables approaches a <strong>Normal distribution</strong>, regardless of the original distribution.</p> </blockquote> <p>Formally, for i.i.d. variables \(X_1, X_2, \dots, X_n\):</p> \[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \quad \text{and} \quad \frac{\bar{X}_n - \mu}{\sigma / \sqrt{n}} \xrightarrow{d} \mathcal{N}(0,1)\] <p>This explains <strong>why the normal distribution is so prevalent</strong> in ML — it naturally arises when averaging data, sampling errors, or even when computing model parameter estimates.</p> <hr> <p>Let’s visualize this with a non-Gaussian distribution — the <strong>exponential distribution</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># Draw means from 1000 samples, each of size 50, from an Exponential distribution
</span><span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">exponential</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">means</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span>

<span class="c1"># Plotting
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">means</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Central Limit Theorem (CLT): Means from Exponential Distribution</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample Mean</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div> <div style="display: flex; justify-content: center;"> <div id="cltPlot" style="width:100%;max-width:700px;"></div> </div> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <script>const sampleMeans=[];for(let e=0;e<1e3;e++){let e=0;for(let t=0;t<50;t++){const t=Math.random();e+=-Math.log(1-t)}sampleMeans.push(e/50)}const trace={x:sampleMeans,type:"histogram",marker:{color:"skyblue"}};Plotly.newPlot("cltPlot",[trace],{title:"Central Limit Theorem: Sampling Means from Exponential Distribution",xaxis:{title:"Sample Mean"},yaxis:{title:"Frequency"}});</script> <p>Despite the exponential distribution being <strong>highly skewed</strong>, the <strong>histogram of sample means is bell-shaped</strong>, showing the CLT in action.</p> <hr> <h2 id="applications-in-machine-learning">Applications in Machine Learning</h2> <p>Now that we’ve built up the mathematical foundations, let’s see how they <strong>directly impact practical machine learning workflows</strong>.</p> <hr> <h3 id="probabilistic-modeling">Probabilistic Modeling</h3> <p>Probabilistic models like:</p> <ul> <li><strong>Naive Bayes</strong></li> <li><strong>Gaussian Mixture Models</strong></li> <li><strong>Bayesian Linear Regression</strong></li> </ul> <p>…are all rooted in these probability concepts. They define <strong>likelihoods</strong>, <strong>priors</strong>, and <strong>posteriors</strong>, which use <strong>conditional probability</strong>, <strong>distributions</strong>, and <strong>expectation</strong>.</p> <p>Even advanced models like <strong>Variational Autoencoders (VAEs)</strong> rely on these basics — using expectations, KL divergence, and Normal distributions.</p> <hr> <h3 id="uncertainty-quantification">Uncertainty Quantification</h3> <p>Models that output probabilities (like softmax classifiers or probabilistic regressors) provide a <strong>distribution over outputs</strong>, allowing you to quantify uncertainty.</p> <ul> <li>Knowing the <strong>variance</strong> of a model’s prediction helps determine confidence.</li> <li>Using the <strong>CLT</strong>, you can estimate confidence intervals around predictions.</li> </ul> <p>This is critical in <strong>high-stakes applications</strong> like medicine, finance, and autonomous driving.</p> <hr> <h3 id="why-empirical-means-work-ensemble-models">Why Empirical Means Work (Ensemble Models)</h3> <p>Ensemble techniques like:</p> <ul> <li><strong>Bagging (Random Forests)</strong></li> <li><strong>Boosting (XGBoost, LightGBM)</strong></li> <li><strong>Model Averaging (e.g., Stacking)</strong></li> </ul> <p>…all rely on the <strong>Law of Large Numbers</strong> and <strong>Central Limit Theorem</strong>.</p> <p>By aggregating multiple noisy models, the final prediction has:</p> <ul> <li>Lower variance</li> <li>Greater stability</li> <li>Better generalization</li> </ul> <p>The magic lies in averaging — and the math tells us why that works.</p> <hr> <h2 id="wrapping-up">Wrapping Up</h2> <p>Probability theory is more than a theoretical curiosity — it’s the <strong>engine behind machine learning</strong>. From modeling uncertainties, understanding feature distributions, building generative models, and even constructing deep learning layers — these foundational ideas are everywhere.</p> <hr> <p><strong>Next Up</strong>: In the next post, we’ll dive into <strong>Probability Distributions</strong> — understanding Bernoulli, Binomial, Gaussian, and how they shape the models we build.</p> </div> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"joyoshish/joyoshish.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © <a href="https://joyoshish.github.io/">Joyoshish Saha</a> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <script src="https://cdn.plot.ly/plotly-latest.min.js"></script> <link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css"> <script defer src="https://pyscript.net/latest/pyscript.js"></script> </body> </html>