<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://joyoshish.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://joyoshish.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-01T09:32:37+00:00</updated><id>https://joyoshish.github.io/feed.xml</id><title type="html">blank</title><subtitle>Joyoshish&apos;s Digital Space </subtitle><entry><title type="html">Linear Algebra Basics for ML - Vector Operations, Norms, and Projections</title><link href="https://joyoshish.github.io/blog/2024/mathforml-linalg1/" rel="alternate" type="text/html" title="Linear Algebra Basics for ML - Vector Operations, Norms, and Projections"/><published>2024-05-13T11:20:00+00:00</published><updated>2024-05-13T11:20:00+00:00</updated><id>https://joyoshish.github.io/blog/2024/mathforml-linalg1</id><content type="html" xml:base="https://joyoshish.github.io/blog/2024/mathforml-linalg1/"><![CDATA[<p>In machine learning, every problem—whether it’s image recognition, natural language processing, or anomaly detection—begins with how we represent data. In this post, we’ll take a deep dive into vectors and vector spaces, exploring the underlying mathematics that enables ML algorithms to learn from data. We’ll follow a problem-driven approach: each section starts with a real-world ML challenge, introduces the mathematical tool needed, explains the theory from the ground up, and concludes with a detailed solution along with Python coding examples and real-world applications in NLP and Computer Vision.</p> <hr/> <h2 id="vector-addition-and-scalar-multiplication">Vector Addition and Scalar Multiplication</h2> <p>Imagine you’re training a neural network. At each step, your model needs to update its parameters—those weights that define how the network behaves. But how exactly are these updates performed? Behind the scenes, you’re combining current weights with gradient information and scaling them based on how much you want to change. This simple operation, which powers the heart of deep learning, relies entirely on vector addition and scalar multiplication.</p> <p>Let’s break it down.</p> <p>A vector \(\mathbf{v}\) in \(\mathbb{R}^n\) is an ordered list of \(n\) real numbers:</p> \[\mathbf{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}\] <p>Vectors represent points, directions, or quantities in space—and in machine learning, they can represent feature values, model parameters, or gradients.</p> <p>Adding two vectors \(\mathbf{u}\) and \(\mathbf{v}\) looks like this:</p> \[\mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}\] <p>This operation is performed element-wise. You can think of it as combining two data points or updating a parameter by adding the change suggested by a gradient.</p> <p>Now, multiplying a vector by a scalar \(\alpha\) stretches or shrinks it:</p> \[\alpha \mathbf{u} = \begin{bmatrix} \alpha u_1 \\ \alpha u_2 \\ \vdots \\ \alpha u_n \end{bmatrix}\] <p>This changes the length of the vector (its magnitude), but not its direction—unless \(\alpha\) is negative, in which case the vector flips.</p> <p>This brings us to the classic weight update rule in gradient descent:</p> \[\mathbf{w}_{\text{new}} = \mathbf{w}_{\text{old}} - \alpha \nabla \mathbf{w}\] <p>Here, \(\nabla \mathbf{w}\) is the gradient vector, and \(\alpha\) is the learning rate. You’re subtracting a scaled version of the gradient from the current weights—a simple but powerful operation that helps your model learn.</p> <p>Let’s see this in action:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define weight vector and gradient vector
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">])</span>

<span class="c1"># Update rule using gradient descent (learning rate = 0.1)
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">new_weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">gradients</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Updated Weights:</span><span class="sh">"</span><span class="p">,</span> <span class="n">new_weights</span><span class="p">)</span>
</code></pre></div></div> <p>Running this snippet simulates one step of gradient descent. Each component of the weight vector is nudged slightly in the direction opposite to the gradient, scaled by how aggressively you want to learn (i.e., the learning rate).</p> <p>These same operations show up everywhere. In NLP, we update word embeddings using gradients—every word vector in a model like Word2Vec gets refined through such updates. In Computer Vision, convolutional filters (which are essentially matrices or higher-dimensional tensors made up of vector-like slices) are tuned via backpropagation, relying on vector addition and scaling.</p> <p>Whether you’re fine-tuning the weights of a deep network or learning dense word representations, these basic operations—vector addition and scalar multiplication—are always in play. They may seem simple, but they’re fundamental to everything that follows in the world of machine learning.</p> <hr/> <h2 id="linear-combinations-span-basis-and-dimensionality">Linear Combinations, Span, Basis, and Dimensionality</h2> <p>In many machine learning applications, especially in domains like text and image processing, the data we work with lives in very high-dimensional spaces. Word embeddings might have 300 dimensions, images can have thousands of pixel values—and all of this contributes to increased computation, memory usage, and sometimes even noise. But do we really need all those dimensions?</p> <p>Often, we don’t. The trick lies in representing high-dimensional data more compactly—without losing the essence of what makes that data useful. To do that, we need to understand the concepts of linear combinations, span, basis, and dimensionality.</p> <p>Let’s start with the basics.</p> <p>A <strong>linear combination</strong> allows us to build a new vector using a set of existing vectors. Suppose we have vectors \(\mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_k\) in \(\mathbb{R}^n\), then a linear combination is:</p> \[\mathbf{v} = \alpha_1 \mathbf{v}_1 + \alpha_2 \mathbf{v}_2 + \dots + \alpha_k \mathbf{v}_k\] <p>In other words, we scale each vector by a coefficient and add them together.</p> <p>The <strong>span</strong> of a set of vectors is the collection of all possible vectors you can form using linear combinations of those vectors. If your set spans \(\mathbb{R}^n\), then it’s powerful enough to represent any point in that space.</p> <p>Now, when you want the most compact and efficient representation, you need a <strong>basis</strong>: a set of linearly independent vectors that spans the entire space. With a basis, every vector in the space can be uniquely expressed as a linear combination of these basis vectors.</p> <p>The number of vectors in the basis gives us the <strong>dimension</strong> of the space. And in machine learning, reducing this dimensionality—while preserving the most important structure in the data—is exactly what techniques like PCA (Principal Component Analysis) aim to do.</p> <p>PCA finds a new basis where each new vector (called a principal component) captures as much variance in the data as possible. These new basis vectors are orthogonal, and often, you only need the first few to explain most of your data’s structure. This simplifies your dataset, making models faster and potentially more robust.</p> <p>Here’s a quick example of how we can represent a vector using a standard basis:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Representing a point in 2D space using the standard basis
</span><span class="n">basis_vectors</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">coefficients</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">new_vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">coefficients</span><span class="p">,</span> <span class="n">basis_vectors</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">New Vector from Linear Combination:</span><span class="sh">"</span><span class="p">,</span> <span class="n">new_vector</span><span class="p">)</span>
</code></pre></div></div> <p>This gives us the vector \([3, 4]\) as a combination of the basis vectors \([1, 0]\) and \([0, 1]\).</p> <p>In practice, especially in NLP, we apply dimensionality reduction techniques like PCA to compress high-dimensional word vectors into more manageable sizes. This helps in both visualization and reducing complexity for downstream models.</p> <p>In computer vision, PCA is used for image compression—transforming large image data into a smaller set of values that still retain the key visual features. This can drastically reduce storage and computation without sacrificing much accuracy in tasks like recognition or classification.</p> <p>So whether you’re dealing with text or pixels, these foundational ideas—linear combinations, span, basis, and dimension—are what let us tame the curse of dimensionality and make sense of our data in smarter ways.</p> <hr/> <h2 id="orthogonality-and-projections">Orthogonality and Projections</h2> <p>Imagine you’re working on a computer vision task and want to compress image data without losing important information. You could reduce the number of features, but how do you ensure you’re keeping the parts that matter?</p> <p>The answer lies in projections. More specifically, <em>orthogonal projections</em> onto lower-dimensional subspaces help us reduce dimensionality while preserving the most important aspects of the data.</p> <p>In math terms, two vectors \(\mathbf{u}\) and \(\mathbf{v}\) are said to be orthogonal if their dot product is zero:</p> \[\mathbf{u} \cdot \mathbf{v} = 0\] <p>This means the vectors point in completely independent directions—think of axes in 3D space.</p> <p>When you project one vector onto another, you’re essentially extracting its component in that direction. The projection of \(\mathbf{u}\) onto \(\mathbf{v}\) is calculated as:</p> \[\text{proj}_{\mathbf{v}}(\mathbf{u}) = \left( \frac{\mathbf{u} \cdot \mathbf{v}}{\|\mathbf{v}\|^2} \right) \mathbf{v}\] <p>This formula plays a major role in Principal Component Analysis (PCA), where data is projected onto orthogonal axes—called principal components—that maximize variance. In simpler terms, PCA finds the directions in which your data varies the most and projects it there, compressing the information without much loss.</p> <p>Here’s a simple example to visualize a projection:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Define a data point and a principal component direction
</span><span class="n">data_point</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">principal_component</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>

<span class="c1"># Project the data point onto the principal component
</span><span class="n">proj_scalar</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">data_point</span><span class="p">,</span> <span class="n">principal_component</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">principal_component</span><span class="p">,</span> <span class="n">principal_component</span><span class="p">)</span>
<span class="n">projection</span> <span class="o">=</span> <span class="n">proj_scalar</span> <span class="o">*</span> <span class="n">principal_component</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Projection of Data Point onto Principal Component:</span><span class="sh">"</span><span class="p">,</span> <span class="n">projection</span><span class="p">)</span>
</code></pre></div></div> <p>In real-world NLP tasks, we use projections to reduce the dimensionality of word vectors, helping to clean up noisy embeddings and improve interpretability. In computer vision, orthogonal projections help extract relevant image features, enabling better classification, detection, or compression.</p> <hr/> <h2 id="vector-norms-and-model-complexity">Vector Norms and Model Complexity</h2> <p>Now suppose you’re training a regression model, and it starts overfitting—performing great on training data but terribly on unseen examples. One common trick to fix this is regularization, which involves penalizing large weights. But that raises a question: how do you actually measure the “size” of a vector?</p> <p>That’s where vector norms come in.</p> <p>The <strong>L1 norm</strong> (also known as the Manhattan norm) sums up the absolute values of all the vector components:</p> \[\|\mathbf{v}\|_1 = \sum_{i=1}^{n} |v_i|\] <p>The <strong>L2 norm</strong> (Euclidean norm) is the straight-line distance from the origin:</p> \[\|\mathbf{v}\|_2 = \sqrt{\sum_{i=1}^{n} v_i^2}\] <p>And the <strong>L∞ norm</strong> captures the largest absolute value in the vector:</p> \[\|\mathbf{v}\|_\infty = \max_i |v_i|\] <p>Each of these norms gives a different perspective on vector size, and each is used in different types of regularization. Lasso regression uses the L1 norm to encourage sparsity (pushing some weights to zero), while Ridge regression uses the L2 norm to shrink all weights evenly. L∞ isn’t as common, but it can be useful when you care about controlling the biggest contributor.</p> <p>Here’s a quick example showing how to calculate all three:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="n">l1_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
<span class="n">l2_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">v</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">linf_norm</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L1 Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">l1_norm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L2 Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">l2_norm</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">L∞ Norm:</span><span class="sh">"</span><span class="p">,</span> <span class="n">linf_norm</span><span class="p">)</span>
</code></pre></div></div> <p>In NLP, we use norm-based regularization to keep our word embeddings generalizable. In computer vision, we regularize convolutional filters to avoid overfitting to noise in the training images. Measuring and controlling vector magnitude is key to making our models not just accurate, but robust.</p> <hr/> <h2 id="inner-and-outer-products">Inner and Outer Products</h2> <p>Let’s say you’re building a recommendation engine or clustering users based on their behavior. One of the first things you’ll need to do is measure how similar two data points are. But how do you quantify “similarity” in a vector space?</p> <p>That’s where the <strong>inner product</strong> comes in.</p> <p>Given two vectors \(\mathbf{u}\) and \(\mathbf{v}\), the inner product (or dot product) is:</p> \[\mathbf{u} \cdot \mathbf{v} = \sum_{i=1}^{n} u_i v_i\] <p>If the result is large, it means the vectors are pointing in similar directions—i.e., they are similar. In fact, this is the basis for cosine similarity, a widely used metric in NLP for comparing word vectors.</p> <p>On the other hand, the <strong>outer product</strong> builds a full matrix of interactions between two vectors. For \(\mathbf{u} \in \mathbb{R}^m\) and \(\mathbf{v} \in \mathbb{R}^n\), the outer product looks like this:</p> \[\mathbf{u} \otimes \mathbf{v} = \begin{bmatrix} u_1v_1 &amp; u_1v_2 &amp; \cdots &amp; u_1v_n \\ u_2v_1 &amp; u_2v_2 &amp; \cdots &amp; u_2v_n \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ u_mv_1 &amp; u_mv_2 &amp; \cdots &amp; u_mv_n \\ \end{bmatrix}\] <p>While the inner product gives us a single similarity score, the outer product creates a full interaction map—useful when we want to understand how features influence each other.</p> <p>Here’s how to compute both in Python:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="c1"># Inner product: similarity
</span><span class="n">inner_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Inner Product:</span><span class="sh">"</span><span class="p">,</span> <span class="n">inner_product</span><span class="p">)</span>

<span class="c1"># Outer product: interaction matrix
</span><span class="n">outer_product</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">outer</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Outer Product:</span><span class="se">\n</span><span class="sh">"</span><span class="p">,</span> <span class="n">outer_product</span><span class="p">)</span>
</code></pre></div></div> <p>In NLP, we use inner products to compare embeddings and cluster similar words or documents. Outer products come in handy when building attention mechanisms or covariance matrices that model relationships between features. In computer vision, outer products are used to analyze spatial patterns in images, enabling algorithms to detect textures, faces, and more.</p> <hr/> <p>From updating model parameters with gradient descent to compressing high-dimensional data, measuring vector magnitudes, and analyzing feature interactions—vectors and vector spaces are the hidden framework behind much of machine learning.</p> <p>These mathematical ideas may seem abstract at first, but they solve incredibly concrete problems. They power dimensionality reduction in PCA, make regularization work in regression models, and drive similarity searches in recommendation engines and NLP systems.</p> <p>If you’ve followed along this far, you’ve just walked through the foundational math that makes many machine learning techniques possible. And we’re just getting started—these concepts will show up again and again as we explore more advanced topics in the Math for ML series.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="math"/><category term="ml"/><category term="ai"/><category term="linear_algebra"/><category term="math"/><summary type="html"><![CDATA[Linear Algebra 1 - Mathematics for Machine Learning]]></summary></entry><entry><title type="html">k-means++ অ্যালগরিদম</title><link href="https://joyoshish.github.io/blog/2024/kmeansplusplus_beng/" rel="alternate" type="text/html" title="k-means++ অ্যালগরিদম"/><published>2024-05-05T10:00:00+00:00</published><updated>2024-05-05T10:00:00+00:00</updated><id>https://joyoshish.github.io/blog/2024/kmeansplusplus_beng</id><content type="html" xml:base="https://joyoshish.github.io/blog/2024/kmeansplusplus_beng/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp-480.webp 480w,/assets/img/kmpp-800.webp 800w,/assets/img/kmpp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Unlucky choices! (Source: http://shabal.in/visuals.html) </div> <p>k-means++ clustering ব্যাবহার করে আমরা বেশিরভাগ unsupervised learning এর ক্ষেত্রে k-means clustering এর তুলনায় বেশি দ্রুত এবং অধিকতর নিকট সমাধান পেতে পারি। যেহেতু unsupervised learning এর ক্ষেত্রে কোনও সুনির্দিষ্ট সমাধান থাকে না, তাই যেকোনো unsupervised learning এই আমরা চেষ্টা করি cost কমানোর যেটি k-means++ খুবই দক্ষতার সাথে করে।</p> <p>David Arthur এবং Sergei Vassilvitskii ২০০৭ সালে k-means++ অ্যালগরিদমটির প্রস্তাব দেন standard k-means থেকে clustering এর NP-hard সমাধান হিসাবে।</p> <h3 id="standard-k-means-algorithm-এর-অপূর্ণতা">Standard k-means algorithm এর অপূর্ণতা</h3> <p>k-means++ শেখার আগে আমাদের জেনে নেওয়া ভাল standard k-means algorithm এর ত্রুটি এবং এর অপূর্ণতাগুলি।</p> <p>১/ কিছু ক্ষেত্রে k-means বহুবার iterated হয় যা algorithm টাকে ধীর করে দেয়।</p> <p>২/ k-means কেমন কাজ করবে তা বেশিরভাগটাই নির্ভর করে আমরা প্রাথমিক ভাবে যে cluster center গুলো এলোমেলোভাবে নির্বাচন করেছি তার উপর। এটির কারণেই কখনো কখনো k-means একই cluster এ দুটি cluster center নির্বাচন করে ফেলে। এটি উপযুক্ত উদাহরণ দিয়ে বোঝা যাক।</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp1.webp-480.webp 480w,/assets/img/kmpp1.webp-800.webp 800w,/assets/img/kmpp1.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এক্ষেত্রে ‘a’ cluster এ বাস্তবে 2 টি cluster থাকলেও k-means তা নির্ধারণ করতে অক্ষম। একইসাথে এটি ‘c’ ও ‘d’ cluster কে আলাদা বললেও তা আসলে একটিই cluster। এভাবেই k-means কখনো কখনো local optimum এই সীমাবদ্ধ থেকে যায় এবং তা bad cluster সৃষ্টি করে।</p> <h3 id="k-means-algorithm">k-means++ algorithm</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp2.webp-480.webp 480w,/assets/img/kmpp2.webp-800.webp 800w,/assets/img/kmpp2.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এবার আসা যাক k-means++ algorithm এ এবং এটি কেমন কার্যকরী ভাবে clustering করে।</p> <p>কোনো data set থেকে আমরা নিম্নোক্ত উপায়ে clustering করতে পারি।</p> <p>১ক/ প্রথমেই আমরা randomly একটি cluster center নির্ধারণ করি data set থেকে। (uniform and random choice)</p> <p>১খ/ এখন data set এর থেকে পরবর্তী center টি নির্ধারণ করতে আমরা furthest point strategy ব্যাবহার করি।</p> <p>এই পদ্ধতিতে আমরা কোনো point কে তখনি একটি cluster center নির্বাচন করি যখন সেটি পূর্ব নির্ধারিত cluster center থেকে সবথেকে বেশি দুরে অবস্থান করে।</p> <p>এটি গাণিতিকভাবে নির্ণয় করতে আমরা একটি নতুন চল ধরব D(x) যা হল কোনো data point থেকে ইতিমধ্যে নির্বাচিত cluster center গুলোর দূরত্ব গুলোর মধ্যে নিম্নতমটি (min c∈C abs(x − c))। এখন পরবর্তী cluster center নির্ধারণ করার সময় আমরা data point গুলোর মধ্যে সেটিকে নির্বাচন করব যার ক্ষেত্রে D(x)²/ΣD(x)² এই সম্ভাবনা টা বেশি। (furthest point strategy)</p> <p>নীচে একটি উদাহারণ দিলে বিষয়টি আরও পরিষ্কার হয়ে যাবে। (ক্রমশ)</p> <p>২/ আমরা এই ভাবে cluster center নির্বাচন করব যতক্ষণ আমাদের কাছে k টি cluster center আসে।</p> <p>৩/ এভাবে cluster center initialisation এর পর আমরা প্রত্যেক data point কে তার নিকটতম cluster center এর সংশ্লিষ্ট cluster এ অন্তর্ভুক্ত করবো, যেমন ভাবে আমরা k-means algorithm এ করে থাকি।</p> <p>আমরা এই data points allocation ততক্ষণ করবো যতক্ষণ না cluster center গুলো অপরিবর্তিত থাকছে।</p> <h3 id="উদাহরণ">উদাহরণ</h3> <p>এবার একটি 1 dimensional data points নিয়ে k-means++ algorithm টি ঝালিয়ে নেওয়া যাক।</p> <p>ধরা যাক আমাদের কাছে data points আছে {0, 1, 2, 5, 6} এবং আমরা ২ টি cluster এই data points থেকে পেতে চাই। (k = 2)</p> <p>প্রথম cluster center C1 হল 0। C1 = 0 (প্রথম cluster center টি আমরা অনির্দিষ্ট ভাবে প্রথম data point টাকে নেব)</p> <p>এখন দ্বিতীয় cluster center নির্বাচনের ক্ষেত্রে আমরা D(x)²/ΣD(x)² এই সম্ভাবনা টা প্রতিটি data point এর জন্য নির্ণয় করবো।</p> <p>P(C2=1) = ((1–0)²)f = 1f নিকটতম cluster center, C1=0</p> <p>P(C2=2) = ((2–0)²)f = 4f নিকটতম cluster center, C1=0</p> <p>P(C2=5) = ((5–0)²)f = 25f নিকটতম cluster center, C1=0</p> <p>P(C2=6) = ((6–0)²)f = 36f নিকটতম cluster center, C1=0</p> <p>এখানে f = (1+4+25+36)</p> <p>দেখা যাচ্ছে P(C2=6) সবথেকে বেশি, তাই k-means++ 6 কে C2 হিসাবে নির্বাচন করবে। C2 = 6</p> <p>এরপর আমরা data point গুলোকে সবথেকে কাছের সংশ্লিষ্ট cluster center এর সাথে যুক্ত করলেই দুটি cluster পেয়ে যাব {(0, 1, 2)(5, 6)} ।</p> <p>অন্য একটি উদাহরণ নিয়ে দেখা যাক।</p> <p>ধরা যাক আমাদের কাছে data points আছে</p> <p>{0, 1, 2, 5, 6, 9, 10} এবং আমরা ৩ টি cluster এই data points থেকে পেতে চাই। (k = 3)</p> <p>প্রথম cluster center C1 হল 0। C1 = 0</p> <p>দ্বিতীয় cluster center নির্বাচনের ক্ষেত্রে প্রথম উদাহরণের মতই আমরা furthest point 10 কে C2 হিসাবে নির্বাচন করবো। C2 = 10</p> <p>এখন তৃতীয় cluster center এর জন্য আমাদেরকে সংশ্লিষ্ট সম্ভাবনা গুলো নির্ণয় করতে হবে প্রতিটি data point এর জন্য।</p> <p>P(C3=1) = ((1–0)²)f = 1f নিকটতম cluster center, C1=0</p> <p>P(C3=2) = ((2–0)²)f = 4f নিকটতম cluster center, C1=0</p> <p>P(C3=5) = ((5–0)²)f = 25f নিকটতম cluster center, C1=0</p> <p>P(C3=6) = ((6–10)²)f = 16f নিকটতম cluster center, C2=10</p> <p>P(C3=9) = ((9–10)²)f = 1f নিকটতম cluster center, C2=10</p> <p>এখানে f = (1+4+25+16+1)</p> <p>দেখা যাচ্ছে P(C3=5) সবথেকে বেশি, তাই k-means++ 5 কে C3 হিসাবে নির্বাচন করবে। C3 = 5</p> <p>এরপর আমরা data point গুলোকে সবথেকে কাছের সংশ্লিষ্ট cluster center এর সাথে সংযুক্ত করলেই তিনটি cluster পেয়ে যাব। {(0, 1, 2)(5, 6)(9, 10)}</p> <p>এভাবেই 2 dimensional বা উচ্চতর dimension এ আমরা k-means++ ব্যাবহার করে clustering করতে পারি। তথ্যগতভাবে বলা যেতে পারে k-means এবং k-means++ এর মধ্যে cluster center initialisation এরই শুধু পার্থক্য রয়েছে, বাকি প্রক্রিয়া দুক্ষেত্রেই এক।</p> <h3 id="k-means-algorithm-এর-কার্যকারিতা">k-means++ algorithm এর কার্যকারিতা</h3> <p>এখন k-means algorithm এর থেকে k-means++ কতটা কার্যকর তা দেখা যাক।</p> <p>এখানে একটি dataset এর scatter plot দেওয়া হল</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp3.webp-480.webp 480w,/assets/img/kmpp3.webp-800.webp 800w,/assets/img/kmpp3.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এবার k-means এবং k-means++ এর জন্য cluster center initialisation এর 2 dimensional histogram দেখা যাক</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/kmpp4.webp-480.webp 480w,/assets/img/kmpp4.webp-800.webp 800w,/assets/img/kmpp4.webp-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/kmpp4.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>এর থেকে পরিষ্কার ভাবে বোঝা যায় k-means++ cluster গুলোর মধ্যভাগে বেশি initialise করেছে যা বেশিরভাগ dataset এর জন্য দ্রুত হবে এবং অধিকতর সঠিক cluster বানাতে সক্ষম হবে।</p> <h3 id="k-means-এর-অপূর্ণতা">k-means++ এর অপূর্ণতা</h3> <p>K-means++ সম্পর্কে আলোচনা শেষ করার পূর্বে এটির কিছু ত্রুটি দেখে নেওয়া দরকার যা বাস্তব data set গুলোতে ব্যাবহারের ক্ষেত্রে দেখা দিতে পারে।</p> <p>১/ k-means++ বিচ্ছিন্ন data point এর ক্ষেত্রে খুবই সংবেদনশীল (sensitive to outliers)। কারণ k-means++ সবসময় furthest point strategy ব্যাবহার করে cluster center গুলো নির্বাচন করে, যা করতে গিয়ে কিছুক্ষেত্রে তা বিচ্ছিন্ন একটি point কে cluster center নির্বাচন করে ফেলে যা কোন cluster সৃষ্টি করতে তেমন কোন গুরুত্ব রাখে না।</p> <p>২/ ধরা যাক কোন Game Developer Company তাদের game খেলে, এমন যত gamer আছে, তাদের পছন্দের game genre(List of video game genres — Wikipedia) গুলো দিয়ে clustering করতে আগ্রহী। এখন game genre এর সংখ্যা অনেক বেশি হওয়ায় k-means++ cluster center initialisation এর জন্য ঠিক ততবারই data point গুলোর মধ্যে algorithm টিকে pass করাবে। সেক্ষেত্রে প্রক্রিয়াটি মন্থর হয়ে যাবে। k-means++ k সংখ্যক বার data point pass করার জন্য যেখানে k একটি অনেক বড় সংখ্যা তখন দ্রুত কাজ করে না। এটি অতিক্রম করতে আমরা oversampling এর সাহায্য নিই এবং এর থেকে k-means এর অন্য একটি ভিন্নতা — parallel k-means এর উৎপত্তি (<a href="http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf">http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf</a>)।</p> <p>এসব সত্ত্বেও বেশিরভাগ data set এর জন্য k-means++ তথাকথিত k-means algorithm এর থেকে বেশি সফল ভাবে clustering করতে সক্ষম।</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="ml"/><category term="ai"/><summary type="html"><![CDATA[k-means++ অ্যালগরিদম]]></summary></entry><entry><title type="html">k-means++ অ্যালগরিদম</title><link href="https://joyoshish.github.io/blog/2020/k-means/" rel="alternate" type="text/html" title="k-means++ অ্যালগরিদম"/><published>2020-08-07T15:27:12+00:00</published><updated>2020-08-07T15:27:12+00:00</updated><id>https://joyoshish.github.io/blog/2020/k-means-</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/k-means/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ৩ : Linear Regression ও Gradient Descent</title><link href="https://joyoshish.github.io/blog/2020/linear-regression-gradient-descent/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ৩ : Linear Regression ও Gradient Descent"/><published>2020-08-04T09:19:46+00:00</published><updated>2020-08-04T09:19:46+00:00</updated><id>https://joyoshish.github.io/blog/2020/-----linear-regression--gradient-descent</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/linear-regression-gradient-descent/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ২ : ক্ষয়ক্ষতির হিসেব</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ২ : ক্ষয়ক্ষতির হিসেব"/><published>2020-08-03T03:17:30+00:00</published><updated>2020-08-03T03:17:30+00:00</updated><id>https://joyoshish.github.io/blog/------</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং-এর কলকব্জা ১ : যন্ত্র কতভাবে শেখে</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং-এর কলকব্জা ১ : যন্ত্র কতভাবে শেখে"/><published>2020-08-03T03:15:42+00:00</published><updated>2020-08-03T03:15:42+00:00</updated><id>https://joyoshish.github.io/blog/-------</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">চলো যন্ত্রকে ফুল চেনাই</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="চলো যন্ত্রকে ফুল চেনাই"/><published>2020-08-03T03:14:01+00:00</published><updated>2020-08-03T03:14:01+00:00</updated><id>https://joyoshish.github.io/blog/---</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">মেশিন লার্নিং : উপক্রমণিকা</title><link href="https://joyoshish.github.io/blog/2020/" rel="alternate" type="text/html" title="মেশিন লার্নিং : উপক্রমণিকা"/><published>2020-08-02T04:52:54+00:00</published><updated>2020-08-02T04:52:54+00:00</updated><id>https://joyoshish.github.io/blog/--</id><content type="html" xml:base="https://joyoshish.github.io/blog/2020/"><![CDATA[]]></content><author><name></name></author></entry></feed>